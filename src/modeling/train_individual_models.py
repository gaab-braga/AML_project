"""
Fun√ß√µes de treinamento individuais para modelos AML.
Permite controle fino e refinamento espec√≠fico por modelo.
"""

import pandas as pd
import numpy as np
from typing import Dict, Any, Tuple, Optional
from pathlib import Path
import time
import logging
from datetime import datetime

# Configura√ß√£o centralizada de logging
def setup_logging():
    """Configura logging centralizado."""
    logging.getLogger().setLevel(logging.ERROR)
    for handler in logging.root.handlers[:]:
        logging.root.removeHandler(handler)

setup_logging()
logger = logging.getLogger(__name__)
logger.setLevel(logging.ERROR)

def train_xgboost_model(
    X: pd.DataFrame,
    y: pd.Series,
    config: Dict[str, Any],
    artifacts_dir: Path,
    force_retrain: bool = False,
    enable_gpu: bool = False
) -> Dict[str, Any]:
    """
    Treina modelo XGBoost com configura√ß√µes otimizadas para AML.

    XGBoost √© excelente para dados estruturados e desbalanceados,
    com suporte nativo a early stopping e GPU acceleration.

    Args:
        X: Features
        y: Target
        config: Configura√ß√£o experimental
        artifacts_dir: Diret√≥rio de artefatos
        force_retrain: For√ßar retreinamento
        enable_gpu: Habilitar acelera√ß√£o GPU

    Returns:
        Resultados do treinamento
    """
    print("üöÄ Treinando XGBoost - Otimizado para CPU/GPU")

    # Configura√ß√£o espec√≠fica XGBoost
    xgb_config = config.copy()
    xgb_config['models']['xgboost']['params'].update({
        'tree_method': 'gpu_hist' if enable_gpu else 'hist',
        'predictor': 'gpu_predictor' if enable_gpu else 'cpu_predictor',
        'n_jobs': -1,  # Paraleliza√ß√£o CPU
        'verbosity': 0
    })

    # Treinar usando fun√ß√£o base
    from .train_single_model import train_single_model
    try:
        result = train_single_model(X, y, 'xgboost', xgb_config, artifacts_dir, force_retrain)
        print("‚úÖ XGBoost treinado com sucesso")
        return {
            'results': {'xgboost': result},
            'successful_model_names': ['xgboost'],
            'model_names': ['xgboost']
        }
    except Exception as e:
        print(f"‚ùå Erro no XGBoost: {e}")
        return {
            'results': {'xgboost': {'error': str(e)}},
            'successful_model_names': [],
            'model_names': ['xgboost']
        }


def train_lightgbm_model(
    X: pd.DataFrame,
    y: pd.Series,
    config: Dict[str, Any],
    artifacts_dir: Path,
    force_retrain: bool = False
) -> Dict[str, Any]:
    """
    Treina modelo LightGBM com configura√ß√µes otimizadas para AML.

    LightGBM √© eficiente em mem√≥ria e r√°pido, mas requer cuidados
    espec√≠ficos com paraleliza√ß√£o e early stopping.

    Args:
        X: Features
        y: Target
        config: Configura√ß√£o experimental
        artifacts_dir: Diret√≥rio de artefatos
        force_retrain: For√ßar retreinamento

    Returns:
        Resultados do treinamento
    """
    print("üöÄ Treinando LightGBM - Otimizado para velocidade e mem√≥ria")

    # Configura√ß√£o espec√≠fica LightGBM
    lgb_config = config.copy()
    lgb_config['models']['lightgbm']['params'].update({
        'n_jobs': 1,  # LightGBM tem problemas com paraleliza√ß√£o
        'verbosity': -1,  # Silenciar completamente
        'boosting_type': 'gbdt',
        'objective': 'binary',
        'is_unbalance': True,
        'metric': 'auc'
    })

    # Garantir m√©tricas corretas
    lgb_config['metrics'] = ['roc_auc', 'average_precision', 'recall', 'precision', 'f1']

    # Treinar usando fun√ß√£o base
    from .train_single_model import train_single_model
    try:
        result = train_single_model(X, y, 'lightgbm', lgb_config, artifacts_dir, force_retrain)
        print("‚úÖ LightGBM treinado com sucesso")
        return {
            'results': {'lightgbm': result},
            'successful_model_names': ['lightgbm'],
            'model_names': ['lightgbm']
        }
    except Exception as e:
        print(f"‚ùå Erro no LightGBM: {e}")
        return {
            'results': {'lightgbm': {'error': str(e)}},
            'successful_model_names': [],
            'model_names': ['lightgbm']
        }


def train_random_forest_model(
    X: pd.DataFrame,
    y: pd.Series,
    config: Dict[str, Any],
    artifacts_dir: Path,
    force_retrain: bool = False
) -> Dict[str, Any]:
    """
    Treina modelo Random Forest com configura√ß√µes otimizadas para AML.

    Random Forest √© robusto e interpret√°vel, mas pode ser lento
    em datasets grandes. Foca em controle de overfitting.

    Args:
        X: Features
        y: Target
        config: Configura√ß√£o experimental
        artifacts_dir: Diret√≥rio de artefatos
        force_retrain: For√ßar retreinamento

    Returns:
        Resultados do treinamento
    """
    print("üöÄ Treinando Random Forest - Focado em robustez e interpretabilidade")

    # Configura√ß√£o espec√≠fica Random Forest
    rf_config = config.copy()
    rf_config['models']['random_forest']['params'].update({
        'n_jobs': -1,  # Paraleliza√ß√£o m√°xima
        'class_weight': 'balanced',
        'random_state': 42,
        'bootstrap': True,
        'oob_score': True  # Out-of-bag scoring
    })

    # Treinar usando fun√ß√£o base
    from .train_single_model import train_single_model
    try:
        result = train_single_model(X, y, 'random_forest', rf_config, artifacts_dir, force_retrain)
        print("‚úÖ Random Forest treinado com sucesso")
        return {
            'results': {'random_forest': result},
            'successful_model_names': ['random_forest'],
            'model_names': ['random_forest']
        }
    except Exception as e:
        print(f"‚ùå Erro no Random Forest: {e}")
        return {
            'results': {'random_forest': {'error': str(e)}},
            'successful_model_names': [],
            'model_names': ['random_forest']
        }
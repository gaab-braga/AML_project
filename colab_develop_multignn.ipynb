{
          "cells": [
                    {
                              "cell_type": "markdown",
                              "id": "34cd42ef",
                              "metadata": {},
                              "source": [
                                        "# üöÄ Multi-GNN AML Detection - Colab Production Ready\n",
                                        "\n",
                                        "Este notebook implementa **detec√ß√£o de lavagem de dinheiro usando Multi-GNN** seguindo o guia de otimiza√ß√£o completo.\n",
                                        "\n",
                                        "## üìã Checklist Pr√©-Execu√ß√£o\n",
                                        "\n",
                                        "- [ ] Conta Google (para acessar Colab)\n",
                                        "- [ ] Kaggle API key (`kaggle.json`) - [Como obter](https://www.kaggle.com/docs/api)\n",
                                        "- [ ] GPU habilitada no Colab (Runtime > Change runtime type > GPU)\n",
                                        "\n",
                                        "## üéØ Ordem de Execu√ß√£o (IMPORTANTE!)\n",
                                        "\n",
                                        "1. **Cell 1**: Verifica√ß√£o GPU\n",
                                        "2. **Cell 2**: Instala√ß√£o PyTorch Geometric\n",
                                        "3. **Cell 3**: Imports + Kaggle Setup\n",
                                        "4. **Cell 4**: üß™ Teste Configura√ß√£o Kaggle\n",
                                        "5. **Cell 5**: Download Dados\n",
                                        "6. **Cell 6**: Configura√ß√£o\n",
                                        "7. **Cell 7**: Load Data\n",
                                        "8. **Cell 8**: Feature Engineering\n",
                                        "9. **Cell 9**: Graph Construction\n",
                                        "10. **Cell 10**: Model Definition\n",
                                        "11. **Cell 11**: Training Setup\n",
                                        "12. **Cell 12**: TREINAMENTO\n",
                                        "13. **Cell 13**: Evaluation\n",
                                        "14. **Cell 14**: Export\n",
                                        "\n",
                                        "**‚ö†Ô∏è Execute as c√©lulas nesta ordem espec√≠fica!**"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": 1,
                              "id": "6f034fd5",
                              "metadata": {},
                              "outputs": [
                                        {
                                                  "name": "stdout",
                                                  "output_type": "stream",
                                                  "text": [
                                                            "üîç SYSTEM VERIFICATION\n",
                                                            "============================================================\n",
                                                            "üêç Python: 3.9.23 | packaged by conda-forge | (main, Jun  4 2025, 17:49:16) [MSC v.1929 64 bit (AMD64)]\n",
                                                            "‚ùå Not running on Google Colab\n",
                                                            "‚ùå No GPU detected - This will be very slow!\n",
                                                            "============================================================\n",
                                                            "‚ùå No GPU detected - This will be very slow!\n",
                                                            "============================================================\n"
                                                  ]
                                        }
                              ],
                              "source": [
                                        "# üîç SYSTEM VERIFICATION\n",
                                        "print(\"üîç SYSTEM VERIFICATION\")\n",
                                        "print(\"=\" * 60)\n",
                                        "\n",
                                        "# Verificar Python\n",
                                        "import sys\n",
                                        "print(f\"üêç Python: {sys.version}\")\n",
                                        "\n",
                                        "# Verificar se est√° no Colab\n",
                                        "try:\n",
                                        "    import google.colab\n",
                                        "    print(\"‚úÖ Running on Google Colab\")\n",
                                        "except ImportError:\n",
                                        "    print(\"‚ùå Not running on Google Colab\")\n",
                                        "\n",
                                        "# Verificar GPU\n",
                                        "import torch\n",
                                        "if torch.cuda.is_available():\n",
                                        "    print(\"‚úÖ GPU Detected:\")\n",
                                        "    print(f\"   Device: {torch.cuda.get_device_name(0)}\")\n",
                                        "    print(f\"   CUDA Version: {torch.version.cuda}\")\n",
                                        "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
                                        "else:\n",
                                        "    print(\"‚ùå No GPU detected - This will be very slow!\")\n",
                                        "\n",
                                        "print(\"=\" * 60)"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "ffaaa5d3",
                              "metadata": {},
                              "source": [
                                        "## üîß Instala√ß√£o PyTorch Geometric\n",
                                        "\n",
                                        "Esta c√©lula instala automaticamente o PyTorch Geometric compat√≠vel com a vers√£o do CUDA detectada."
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "391928b2",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# üöÄ INSTALA√á√ÉO PYTORCH GEOMETRIC\n",
                                        "print(\"üîß PYTORCH GEOMETRIC INSTALLATION\")\n",
                                        "print(\"=\" * 60)\n",
                                        "\n",
                                        "import subprocess\n",
                                        "import sys\n",
                                        "\n",
                                        "def install_pytorch_geometric():\n",
                                        "    \"\"\"Instala PyTorch Geometric compat√≠vel com CUDA.\"\"\"\n",
                                        "    try:\n",
                                        "        # Verificar CUDA\n",
                                        "        cuda_version = torch.version.cuda\n",
                                        "        if cuda_version:\n",
                                        "            cuda_short = cuda_version.replace(\".\", \"\")[:3]  # e.g., \"118\" for 11.8\n",
                                        "            print(f\"üì¶ PyTorch: {torch.__version__}+cu{cuda_short}\")\n",
                                        "            print(f\"üéÆ CUDA: {cuda_version}\")\n",
                                        "\n",
                                        "            # URL do wheel\n",
                                        "            wheel_url = f\"https://data.pyg.org/whl/torch-{torch.__version__}+cu{cuda_short}.html\"\n",
                                        "            print(f\"üåê Wheel URL: {wheel_url}\")\n",
                                        "\n",
                                        "            # Instalar depend√™ncias PyG\n",
                                        "            print(\"\\nüì• Installing PyG dependencies...\")\n",
                                        "\n",
                                        "            packages = [\n",
                                        "                \"torch-scatter\",\n",
                                        "                \"torch-sparse\",\n",
                                        "                \"torch-cluster\",\n",
                                        "                \"torch-spline-conv\",\n",
                                        "                \"torch-geometric\"\n",
                                        "            ]\n",
                                        "\n",
                                        "            for package in packages:\n",
                                        "                print(f\"   üì¶ Installing {package}...\")\n",
                                        "                cmd = f\"pip install {package} -f {wheel_url}\"\n",
                                        "                result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
                                        "\n",
                                        "                if result.returncode == 0:\n",
                                        "                    print(f\"   ‚úÖ {package} installed successfully\")\n",
                                        "                else:\n",
                                        "                    print(f\"   ‚ùå Failed to install {package}\")\n",
                                        "                    print(f\"      Error: {result.stderr}\")\n",
                                        "                    return False\n",
                                        "\n",
                                        "            print(\"\\n‚úÖ Installation complete!\")\n",
                                        "            return True\n",
                                        "\n",
                                        "        else:\n",
                                        "            print(\"‚ùå CUDA not available - installing CPU version\")\n",
                                        "            result = subprocess.run(\"pip install torch-geometric\", shell=True, capture_output=True, text=True)\n",
                                        "            return result.returncode == 0\n",
                                        "\n",
                                        "    except Exception as e:\n",
                                        "        print(f\"‚ùå Installation failed: {e}\")\n",
                                        "        return False\n",
                                        "\n",
                                        "# Executar instala√ß√£o\n",
                                        "success = install_pytorch_geometric()\n",
                                        "\n",
                                        "if success:\n",
                                        "    print(\"\\nüß™ Testing installation...\")\n",
                                        "    try:\n",
                                        "        import torch_geometric\n",
                                        "        print(f\"‚úÖ PyTorch Geometric: {torch_geometric.__version__}\")\n",
                                        "        print(\"‚úÖ CUDA available:\", torch.cuda.is_available())\n",
                                        "        print(\"‚úÖ GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\")\n",
                                        "    except ImportError as e:\n",
                                        "        print(f\"‚ùå Import failed: {e}\")\n",
                                        "        success = False\n",
                                        "\n",
                                        "if not success:\n",
                                        "    print(\"\\nüí° Troubleshooting:\")\n",
                                        "    print(\"   1. Restart runtime (Runtime > Restart runtime)\")\n",
                                        "    print(\"   2. Run this cell again\")\n",
                                        "    print(\"   3. Check CUDA version compatibility\")\n",
                                        "\n",
                                        "print(\"=\" * 60)"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "54a0938e",
                              "metadata": {},
                              "source": [
                                        "## üìö Imports + Kaggle Setup\n",
                                        "\n",
                                        "Esta c√©lula importa todas as bibliotecas necess√°rias e configura a API do Kaggle."
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "91f081c5",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# üìö IMPORTING LIBRARIES\n",
                                        "print(\"üìö IMPORTING LIBRARIES\")\n",
                                        "print(\"=\" * 60)\n",
                                        "\n",
                                        "import os\n",
                                        "import sys\n",
                                        "import subprocess\n",
                                        "import json\n",
                                        "import shutil\n",
                                        "import pandas as pd\n",
                                        "import numpy as np\n",
                                        "from pathlib import Path\n",
                                        "import torch\n",
                                        "import torch.nn.functional as F\n",
                                        "from torch_geometric.data import Data, DataLoader\n",
                                        "from torch_geometric.nn import GINConv, global_mean_pool\n",
                                        "import torch.nn as nn\n",
                                        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
                                        "from sklearn.model_selection import train_test_split\n",
                                        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n",
                                        "from datetime import datetime\n",
                                        "import requests\n",
                                        "from io import StringIO\n",
                                        "import zipfile\n",
                                        "from urllib.request import urlopen\n",
                                        "import warnings\n",
                                        "import networkx as nx\n",
                                        "import matplotlib.pyplot as plt\n",
                                        "import seaborn as sns\n",
                                        "warnings.filterwarnings('ignore')\n",
                                        "\n",
                                        "print(f\"‚úÖ PyTorch: {torch.__version__}\")\n",
                                        "print(f\"‚úÖ PyTorch Geometric: {torch_geometric.__version__}\")\n",
                                        "print(f\"‚úÖ CUDA available: {torch.cuda.is_available()}\")\n",
                                        "print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
                                        "print(f\"üéØ Using device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")\n",
                                        "\n",
                                        "# Configurar device\n",
                                        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                                        "\n",
                                        "print(\"=\" * 60)\n",
                                        "\n",
                                        "# üîë KAGGLE API SETUP\n",
                                        "print(\"üîë KAGGLE API SETUP\")\n",
                                        "print(\"=\" * 60)\n",
                                        "\n",
                                        "def setup_kaggle():\n",
                                        "    \"\"\"Configura a API do Kaggle.\"\"\"\n",
                                        "    try:\n",
                                        "        # Criar diret√≥rio .kaggle\n",
                                        "        kaggle_dir = Path(\"/root/.kaggle\")\n",
                                        "        kaggle_dir.mkdir(exist_ok=True)\n",
                                        "\n",
                                        "        # Verificar se kaggle.json j√° existe\n",
                                        "        kaggle_file = kaggle_dir / \"kaggle.json\"\n",
                                        "        if kaggle_file.exists():\n",
                                        "            print(\"‚úÖ kaggle.json already exists\")\n",
                                        "        else:\n",
                                        "            print(\"üì§ Please upload your kaggle.json file\")\n",
                                        "            print(\"   (Get it from: https://www.kaggle.com/settings/account)\")\n",
                                        "\n",
                                        "            from google.colab import files\n",
                                        "            uploaded = files.upload()\n",
                                        "\n",
                                        "            if 'kaggle.json' in uploaded:\n",
                                        "                # Mover arquivo\n",
                                        "                shutil.move('kaggle.json', str(kaggle_file))\n",
                                        "\n",
                                        "                # Definir permiss√µes\n",
                                        "                kaggle_file.chmod(0o600)\n",
                                        "                print(\"‚úÖ kaggle.json uploaded and configured\")\n",
                                        "            else:\n",
                                        "                print(\"‚ùå kaggle.json not found in upload\")\n",
                                        "                return False\n",
                                        "\n",
                                        "        # Testar API\n",
                                        "        result = subprocess.run([\"kaggle\", \"competitions\", \"list\", \"--csv\"],\n",
                                        "                              capture_output=True, text=True, timeout=30)\n",
                                        "\n",
                                        "        if result.returncode == 0:\n",
                                        "            print(\"‚úÖ Kaggle API configured!\")\n",
                                        "            print(\"‚úÖ Kaggle API import successful\")\n",
                                        "            return True\n",
                                        "        else:\n",
                                        "            print(\"‚ùå Kaggle API test failed\")\n",
                                        "            print(f\"   Error: {result.stderr}\")\n",
                                        "            return False\n",
                                        "\n",
                                        "    except Exception as e:\n",
                                        "        print(f\"‚ùå Kaggle setup failed: {e}\")\n",
                                        "        return False\n",
                                        "\n",
                                        "# Configurar Kaggle\n",
                                        "kaggle_success = setup_kaggle()\n",
                                        "\n",
                                        "if kaggle_success:\n",
                                        "    print(\"‚úÖ All imports successful!\")\n",
                                        "else:\n",
                                        "    print(\"‚ö†Ô∏è  Kaggle setup failed - you can still run with synthetic data\")\n",
                                        "\n",
                                        "print(\"=\" * 60)"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "1536fa90",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# üß™ TESTANDO CONFIGURA√á√ÉO DO KAGGLE\n",
                                        "print(\"üß™ TESTANDO CONFIGURA√á√ÉO DO KAGGLE\")\n",
                                        "print(\"=\" * 60)\n",
                                        "\n",
                                        "import os\n",
                                        "from pathlib import Path\n",
                                        "\n",
                                        "# Verificar se estamos no Colab\n",
                                        "try:\n",
                                        "    import google.colab\n",
                                        "    print(\"‚úÖ Executando no Google Colab\")\n",
                                        "except ImportError:\n",
                                        "    print(\"‚ùå N√ÉO est√° executando no Google Colab\")\n",
                                        "    print(\"   Este notebook foi projetado para o Google Colab\")\n",
                                        "\n",
                                        "# Verificar kaggle.json\n",
                                        "kaggle_dir = Path(\"/root/.kaggle\")\n",
                                        "kaggle_file = kaggle_dir / \"kaggle.json\"\n",
                                        "\n",
                                        "if kaggle_file.exists():\n",
                                        "    print(\"‚úÖ kaggle.json encontrado!\")\n",
                                        "    print(f\"   Local: {kaggle_file}\")\n",
                                        "    \n",
                                        "    # Verificar permiss√µes\n",
                                        "    permissions = oct(kaggle_file.stat().st_mode)[-3:]\n",
                                        "    print(f\"   Permiss√µes: {permissions}\")\n",
                                        "    \n",
                                        "    if permissions == \"600\":\n",
                                        "        print(\"‚úÖ Permiss√µes corretas (600)\")\n",
                                        "    else:\n",
                                        "        print(f\"‚ö†Ô∏è  Permiss√µes incorretas: {permissions} (deve ser 600)\")\n",
                                        "else:\n",
                                        "    print(\"‚ùå kaggle.json N√ÉO encontrado!\")\n",
                                        "    print(\"   Execute a c√©lula de Imports + Kaggle Setup primeiro!\")\n",
                                        "\n",
                                        "# Testar API do Kaggle\n",
                                        "print()\n",
                                        "print(\"üîç Testando API do Kaggle...\")\n",
                                        "try:\n",
                                        "    import subprocess\n",
                                        "    result = subprocess.run([\"kaggle\", \"competitions\", \"list\", \"--csv\"], \n",
                                        "                          capture_output=True, text=True, timeout=10)\n",
                                        "    if result.returncode == 0:\n",
                                        "        print(\"‚úÖ API do Kaggle funcionando!\")\n",
                                        "        print(\"‚úÖ Configura√ß√£o completa - pode prosseguir!\")\n",
                                        "    else:\n",
                                        "        print(\"‚ùå API do Kaggle falhou\")\n",
                                        "        print(f\"   Erro: {result.stderr.strip()}\")\n",
                                        "except Exception as e:\n",
                                        "    print(f\"‚ùå Erro ao testar API: {e}\")\n",
                                        "\n",
                                        "print(\"=\" * 60)\n",
                                        "print()\n",
                                        "print(\"üéØ Se tudo estiver ‚úÖ, pode prosseguir para a pr√≥xima c√©lula!\")"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "c4cead72",
                              "metadata": {},
                              "source": [
                                        "## üì• Download Dados\n",
                                        "\n",
                                        "Esta c√©lula baixa o dataset AML do Kaggle ou gera dados sint√©ticos se o Kaggle falhar."
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "61ffd783",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# üì• DOWNLOADING IBM AML DATASET\n",
                                        "print(\"üì• DOWNLOADING IBM AML DATASET\")\n",
                                        "print(\"=\" * 60)\n",
                                        "\n",
                                        "# Configurar caminhos\n",
                                        "data_dir = Path(\"/content/aml_data\")\n",
                                        "raw_dir = data_dir / \"raw\"\n",
                                        "processed_dir = data_dir / \"processed\"\n",
                                        "\n",
                                        "for dir_path in [data_dir, raw_dir, processed_dir]:\n",
                                        "    dir_path.mkdir(exist_ok=True)\n",
                                        "\n",
                                        "print(f\"üìÅ Data directory: {data_dir}\")\n",
                                        "print(f\"üìÅ Raw data: {raw_dir}\")\n",
                                        "print(f\"üìÅ Processed data: {processed_dir}\")\n",
                                        "\n",
                                        "def download_kaggle_dataset():\n",
                                        "    \"\"\"Download do dataset via Kaggle API.\"\"\"\n",
                                        "    try:\n",
                                        "        print(\"üì• Downloading from Kaggle: ealtman2019/ibm-transactions-for-anti-money-laundering-aml\")\n",
                                        "        print(\"   This may take a few minutes...\")\n",
                                        "\n",
                                        "        # Comando kaggle\n",
                                        "        cmd = \"kaggle datasets download ealtman2019/ibm-transactions-for-anti-money-laundering-aml -p /content/aml_data/raw --unzip\"\n",
                                        "        result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=600)\n",
                                        "\n",
                                        "        if result.returncode == 0:\n",
                                        "            print(\"‚úÖ Download complete!\")\n",
                                        "\n",
                                        "            # Listar arquivos baixados\n",
                                        "            downloaded_files = list(raw_dir.glob(\"*.csv\"))\n",
                                        "            print(\"üìã Downloaded files:\")\n",
                                        "            for file in downloaded_files:\n",
                                        "                size_mb = file.stat().st_size / (1024 * 1024)\n",
                                        "                print(f\"   {file.name}: {size_mb:.1f} MB\")\n",
                                        "\n",
                                        "            return True\n",
                                        "        else:\n",
                                        "            print(\"‚ùå Download failed\")\n",
                                        "            print(f\"   Error: {result.stderr}\")\n",
                                        "            return False\n",
                                        "\n",
                                        "    except subprocess.TimeoutExpired:\n",
                                        "        print(\"‚è∞ Download timeout\")\n",
                                        "        return False\n",
                                        "    except Exception as e:\n",
                                        "        print(f\"‚ùå Download error: {e}\")\n",
                                        "        return False\n",
                                        "\n",
                                        "def generate_synthetic_data(sample_size=None):\n",
                                        "    \"\"\"Gera dados sint√©ticos realistas para AML.\"\"\"\n",
                                        "    print(\"‚ö†Ô∏è  Using synthetic data (Kaggle download failed)\")\n",
                                        "    print(\"   Generating realistic AML transaction data...\")\n",
                                        "\n",
                                        "    np.random.seed(42)\n",
                                        "\n",
                                        "    n_transactions = sample_size or 50000\n",
                                        "    n_accounts = 2000\n",
                                        "\n",
                                        "    # Gerar dados transacionais realistas\n",
                                        "    data = {\n",
                                        "        'Timestamp': pd.date_range('2020-01-01', periods=n_transactions, freq='1min'),\n",
                                        "        'From Bank': np.random.randint(1, 11, n_transactions),\n",
                                        "        'From Account': np.random.randint(100000, 999999, n_transactions),\n",
                                        "        'To Bank': np.random.randint(1, 11, n_transactions),\n",
                                        "        'To Account': np.random.randint(100000, 999999, n_transactions),\n",
                                        "        'Amount Received': np.random.exponential(1000, n_transactions),\n",
                                        "        'Receiving Currency': np.random.choice(['USD', 'EUR', 'GBP', 'JPY'], n_transactions),\n",
                                        "        'Amount Paid': np.random.exponential(1000, n_transactions),\n",
                                        "        'Payment Currency': np.random.choice(['USD', 'EUR', 'GBP', 'JPY'], n_transactions),\n",
                                        "        'Payment Format': np.random.choice(['ACH', 'Wire', 'Check', 'Cash'], n_transactions),\n",
                                        "        'Is Laundering': np.random.choice([0, 1], n_transactions, p=[0.95, 0.05])\n",
                                        "    }\n",
                                        "\n",
                                        "    df = pd.DataFrame(data)\n",
                                        "\n",
                                        "    # Salvar dados\n",
                                        "    data_file = raw_dir / \"HI-Small_Trans.csv\"\n",
                                        "    df.to_csv(data_file, index=False)\n",
                                        "\n",
                                        "    print(f\"‚úÖ Synthetic data generated: {len(df)} transactions\")\n",
                                        "    print(f\"   File saved: {data_file}\")\n",
                                        "    print(f\"   Laundering transactions: {df['Is Laundering'].sum()}\")\n",
                                        "\n",
                                        "    return True\n",
                                        "\n",
                                        "# Tentar download do Kaggle primeiro\n",
                                        "if kaggle_success:\n",
                                        "    download_success = download_kaggle_dataset()\n",
                                        "else:\n",
                                        "    download_success = False\n",
                                        "\n",
                                        "# Fallback para dados sint√©ticos\n",
                                        "if not download_success:\n",
                                        "    generate_synthetic_data(sample_size=50000)\n",
                                        "\n",
                                        "print(\"=\" * 60)"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "001ad76d",
                              "metadata": {},
                              "source": [
                                        "## ‚öôÔ∏è Configura√ß√£o\n",
                                        "\n",
                                        "Esta c√©lula define todos os hiperpar√¢metros e configura√ß√µes do experimento."
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "0ba97740",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# ‚öôÔ∏è CONFIGURATION\n",
                                        "print(\"‚öôÔ∏è CONFIGURATION\")\n",
                                        "print(\"=\" * 60)\n",
                                        "\n",
                                        "class Config:\n",
                                        "    \"\"\"Configura√ß√µes do experimento Multi-GNN AML.\"\"\"\n",
                                        "\n",
                                        "    # PARA TESTE R√ÅPIDO (5-10 min total):\n",
                                        "    SAMPLE_SIZE = 10000  # Apenas 10k transa√ß√µes\n",
                                        "    EPOCHS = 20          # Menos √©pocas\n",
                                        "\n",
                                        "    # PARA PRODU√á√ÉO (30-60 min total):\n",
                                        "    # SAMPLE_SIZE = None   # Dataset completo\n",
                                        "    # EPOCHS = 100         # Treinamento completo\n",
                                        "\n",
                                        "    # Arquitetura (escolha uma):\n",
                                        "    GNN_TYPE = 'GIN'         # ‚≠ê RECOMENDADO - melhor performance\n",
                                        "    # GNN_TYPE = 'GAT'       # Interpretabilidade\n",
                                        "    # GNN_TYPE = 'GraphSAGE' # Escalabilidade\n",
                                        "    # GNN_TYPE = 'GCN'       # Baseline r√°pido\n",
                                        "\n",
                                        "    # Hiperpar√¢metros da rede\n",
                                        "    HIDDEN_CHANNELS = 128\n",
                                        "    NUM_LAYERS = 3\n",
                                        "    DROPOUT = 0.3\n",
                                        "\n",
                                        "    # Treinamento\n",
                                        "    LEARNING_RATE = 0.001\n",
                                        "    WEIGHT_DECAY = 1e-4\n",
                                        "    BATCH_SIZE = 32\n",
                                        "\n",
                                        "    # Early stopping\n",
                                        "    PATIENCE = 15\n",
                                        "    MIN_DELTA = 0.001\n",
                                        "\n",
                                        "    # Dados\n",
                                        "    TEST_SIZE = 0.2\n",
                                        "    VAL_SIZE = 0.1\n",
                                        "    RANDOM_STATE = 42\n",
                                        "\n",
                                        "# Instanciar configura√ß√£o\n",
                                        "config = Config()\n",
                                        "\n",
                                        "print(\"üìã Configuration:\")\n",
                                        "print(\"-\" * 40)\n",
                                        "print(f\"   SAMPLE_SIZE: {config.SAMPLE_SIZE}\")\n",
                                        "print(f\"   TEST_SIZE: {config.TEST_SIZE}\")\n",
                                        "print(f\"   VAL_SIZE: {config.VAL_SIZE}\")\n",
                                        "print(f\"   GNN_TYPE: {config.GNN_TYPE}\")\n",
                                        "print(f\"   HIDDEN_CHANNELS: {config.HIDDEN_CHANNELS}\")\n",
                                        "print(f\"   NUM_LAYERS: {config.NUM_LAYERS}\")\n",
                                        "print(f\"   DROPOUT: {config.DROPOUT}\")\n",
                                        "print(f\"   EPOCHS: {config.EPOCHS}\")\n",
                                        "print(f\"   LEARNING_RATE: {config.LEARNING_RATE}\")\n",
                                        "print(f\"   BATCH_SIZE: {config.BATCH_SIZE}\")\n",
                                        "print(f\"   PATIENCE: {config.PATIENCE}\")\n",
                                        "\n",
                                        "print(\"=\" * 60)"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "f40f2256",
                              "metadata": {},
                              "source": [
                                        "## üìä Load Data\n",
                                        "\n",
                                        "Esta c√©lula carrega e faz uma limpeza b√°sica dos dados transacionais."
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "a6fbf5a9",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# üìä DATA LOADING & BASIC CLEANING\n",
                                        "print(\"üìä DATA LOADING & BASIC CLEANING\")\n",
                                        "print(\"=\" * 60)\n",
                                        "\n",
                                        "# Carregar dados\n",
                                        "data_file = raw_dir / \"HI-Small_Trans.csv\"\n",
                                        "\n",
                                        "if not data_file.exists():\n",
                                        "    print(f\"‚ùå Data file not found: {data_file}\")\n",
                                        "    print(\"   Please run the download cell first!\")\n",
                                        "else:\n",
                                        "    # Carregar dados\n",
                                        "    df = pd.read_csv(data_file)\n",
                                        "\n",
                                        "    print(f\"üìÑ Loading: {data_file.name}\")\n",
                                        "    print(f\"‚úÖ Loaded {len(df):,} transactions\")\n",
                                        "\n",
                                        "    # Mostrar colunas\n",
                                        "    print(\"üìã Columns:\")\n",
                                        "    for i, col in enumerate(df.columns, 1):\n",
                                        "        print(f\"   {i:2d}. {col}\")\n",
                                        "\n",
                                        "    print(f\"üìä Data shape: {df.shape}\")\n",
                                        "\n",
                                        "    # Amostrar dados se necess√°rio\n",
                                        "    if config.SAMPLE_SIZE and len(df) > config.SAMPLE_SIZE:\n",
                                        "        print(f\"üìä Sampling {config.SAMPLE_SIZE:,} transactions...\")\n",
                                        "        df = df.sample(n=config.SAMPLE_SIZE, random_state=config.RANDOM_STATE)\n",
                                        "        print(f\"‚úÖ Sampled to {len(df):,} transactions\")\n",
                                        "\n",
                                        "    # Limpeza b√°sica\n",
                                        "    print(\"üßπ Basic data cleaning...\")\n",
                                        "    original_size = len(df)\n",
                                        "\n",
                                        "    # Remover valores nulos\n",
                                        "    df = df.dropna()\n",
                                        "    print(f\"   Removed {original_size - len(df)} rows with null values\")\n",
                                        "\n",
                                        "    # Remover duplicatas\n",
                                        "    original_size = len(df)\n",
                                        "    df = df.drop_duplicates()\n",
                                        "    print(f\"   Removed {original_size - len(df)} duplicate rows\")\n",
                                        "\n",
                                        "    # Verificar coluna target\n",
                                        "    target_col = 'Is Laundering'\n",
                                        "    if target_col in df.columns:\n",
                                        "        print(f\"üéØ Target column: '{target_col}'\")\n",
                                        "\n",
                                        "        # Distribui√ß√£o da classe\n",
                                        "        class_counts = df[target_col].value_counts().sort_index()\n",
                                        "        print(\"üìä Class distribution:\")\n",
                                        "        for class_val, count in class_counts.items():\n",
                                        "            percentage = count / len(df) * 100\n",
                                        "            print(\".1f\")\n",
                                        "\n",
                                        "        # Calcular pos_weight para loss function\n",
                                        "        pos_weight = (len(df) - df[target_col].sum()) / df[target_col].sum()\n",
                                        "        print(\".2f\")\n",
                                        "\n",
                                        "        # Salvar pos_weight para uso posterior\n",
                                        "        config.POS_WEIGHT = pos_weight\n",
                                        "\n",
                                        "    else:\n",
                                        "        print(f\"‚ùå Target column '{target_col}' not found!\")\n",
                                        "        print(f\"   Available columns: {list(df.columns)}\")\n",
                                        "\n",
                                        "    # Salvar dados limpos\n",
                                        "    clean_file = processed_dir / \"transactions_clean.csv\"\n",
                                        "    df.to_csv(clean_file, index=False)\n",
                                        "\n",
                                        "    print(f\"üíæ Saved cleaned data to: {clean_file}\")\n",
                                        "\n",
                                        "    # Mostrar primeiras linhas\n",
                                        "    print(\"\\nüîç First few rows:\")\n",
                                        "    print(df.head())\n",
                                        "\n",
                                        "    # Estat√≠sticas b√°sicas\n",
                                        "    print(f\"\\n‚úÖ Final dataset: {len(df):,} transactions\")\n",
                                        "    print(\"=\" * 60)\n",
                                        "\n",
                                        "    # Salvar DataFrame global\n",
                                        "    global df_clean\n",
                                        "    df_clean = df"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "26b07718",
                              "metadata": {},
                              "source": [
                                        "## üîß Feature Engineering\n",
                                        "\n",
                                        "Esta c√©lula cria features avan√ßadas para detec√ß√£o de AML, incluindo features de rede usando NetworkX."
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "6247d533",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# üîß FEATURE ENGINEERING\n",
                                        "print(\"üîß FEATURE ENGINEERING\")\n",
                                        "print(\"=\" * 60)\n",
                                        "\n",
                                        "try:\n",
                                        "    # Verificar se dados foram carregados\n",
                                        "    if 'df_clean' not in globals():\n",
                                        "        print(\"‚ùå Clean data not found! Run the Load Data cell first.\")\n",
                                        "    else:\n",
                                        "        df = df_clean.copy()\n",
                                        "        print(f\"Starting with {len(df):,} transactions\")\n",
                                        "\n",
                                        "        # 1. Processamento de timestamps\n",
                                        "        print(\"1Ô∏è‚É£ Processing timestamps...\")\n",
                                        "        df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
                                        "        df['timestamp_seconds'] = (df['Timestamp'] - df['Timestamp'].min()).dt.total_seconds()\n",
                                        "        print(\"   ‚úÖ Processed timestamp: Timestamp\")\n",
                                        "\n",
                                        "        # 2. Features temporais\n",
                                        "        print(\"2Ô∏è‚É£ Creating temporal features...\")\n",
                                        "        df['hour'] = df['Timestamp'].dt.hour\n",
                                        "        df['day_of_week'] = df['Timestamp'].dt.dayofweek\n",
                                        "        df['month'] = df['Timestamp'].dt.month\n",
                                        "        print(\"   ‚úÖ Created temporal features: hour, day_of_week, month\")\n",
                                        "\n",
                                        "        # 3. Features de transa√ß√£o\n",
                                        "        print(\"3Ô∏è‚É£ Creating transaction features...\")\n",
                                        "        df['amount_ratio'] = df['Amount Received'] / (df['Amount Paid'] + 1e-6)\n",
                                        "        df['amount_diff'] = abs(df['Amount Received'] - df['Amount Paid'])\n",
                                        "        df['amount_log'] = np.log1p(df['Amount Paid'])\n",
                                        "        print(\"   ‚úÖ Created transaction features: amount_ratio, amount_diff, amount_log\")\n",
                                        "\n",
                                        "        # 4. Encoding categ√≥rico\n",
                                        "        print(\"4Ô∏è‚É£ Encoding categorical variables...\")\n",
                                        "        categorical_cols = ['Receiving Currency', 'Payment Currency', 'Payment Format']\n",
                                        "        label_encoders = {}\n",
                                        "\n",
                                        "        for col in categorical_cols:\n",
                                        "            if col in df.columns:\n",
                                        "                le = LabelEncoder()\n",
                                        "                df[f'{col}_encoded'] = le.fit_transform(df[col])\n",
                                        "                label_encoders[col] = le\n",
                                        "                print(f\"   ‚úÖ Encoded {col} -> {col}_encoded\")\n",
                                        "\n",
                                        "        # 5. Features de frequ√™ncia por conta\n",
                                        "        print(\"5Ô∏è‚É£ Creating account frequency features...\")\n",
                                        "        # Frequ√™ncia hor√°ria por conta de origem\n",
                                        "        df['freq_hour'] = df.groupby(['From Account', 'hour']).cumcount()\n",
                                        "\n",
                                        "        # Frequ√™ncia di√°ria por conta de origem\n",
                                        "        df['freq_day'] = df.groupby(['From Account', df['Timestamp'].dt.date]).cumcount()\n",
                                        "\n",
                                        "        # N√∫mero total de transa√ß√µes por conta\n",
                                        "        from_freq = df['From Account'].value_counts()\n",
                                        "        to_freq = df['To Account'].value_counts()\n",
                                        "\n",
                                        "        df['from_account_degree'] = df['From Account'].map(from_freq)\n",
                                        "        df['to_account_degree'] = df['To Account'].map(to_freq)\n",
                                        "        print(\"   ‚úÖ Created account frequency features\")\n",
                                        "\n",
                                        "        # 6. Features de tempo\n",
                                        "        print(\"6Ô∏è‚É£ Creating temporal sequence features...\")\n",
                                        "        # Diferen√ßa de tempo entre transa√ß√µes consecutivas por conta\n",
                                        "        df = df.sort_values(['From Account', 'Timestamp'])\n",
                                        "        df['time_diff'] = df.groupby('From Account')['timestamp_seconds'].diff().fillna(0)\n",
                                        "        df['time_diff_log'] = np.log1p(df['time_diff'])\n",
                                        "        print(\"   ‚úÖ Created temporal sequence features\")\n",
                                        "\n",
                                        "        # 7. Features de comportamento (m√©dias m√≥veis)\n",
                                        "        print(\"7Ô∏è‚É£ Creating behavioral features...\")\n",
                                        "        # M√©dia m√≥vel de valores por conta\n",
                                        "        df['rolling_mean_amount'] = df.groupby('From Account')['Amount Paid'].rolling(5, min_periods=1).mean().reset_index(0, drop=True)\n",
                                        "\n",
                                        "        # Desvio padr√£o m√≥vel\n",
                                        "        df['rolling_std_amount'] = df.groupby('From Account')['Amount Paid'].rolling(5, min_periods=1).std().reset_index(0, drop=True).fillna(0)\n",
                                        "\n",
                                        "        # M√©dia m√≥vel de frequ√™ncia\n",
                                        "        df['rolling_mean_freq'] = df.groupby('From Account')['freq_hour'].rolling(5, min_periods=1).mean().reset_index(0, drop=True)\n",
                                        "        print(\"   ‚úÖ Created behavioral features\")\n",
                                        "\n",
                                        "        # 8. Features de rede usando NetworkX\n",
                                        "        print(\"8Ô∏è‚É£ Creating network features with NetworkX...\")\n",
                                        "        try:\n",
                                        "            # Criar grafo direcionado\n",
                                        "            G = nx.DiGraph()\n",
                                        "\n",
                                        "            # Adicionar n√≥s (contas √∫nicas)\n",
                                        "            all_accounts = set(df['From Account'].unique()) | set(df['To Account'].unique())\n",
                                        "            G.add_nodes_from(all_accounts)\n",
                                        "\n",
                                        "            # Adicionar arestas (transa√ß√µes)\n",
                                        "            edges = list(zip(df['From Account'], df['To Account']))\n",
                                        "            G.add_edges_from(edges)\n",
                                        "\n",
                                        "            print(f\"   Network: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
                                        "\n",
                                        "            # Calcular PageRank\n",
                                        "            print(\"   Computing PageRank...\")\n",
                                        "            pagerank = nx.pagerank(G, alpha=0.85)\n",
                                        "            df['pagerank_from'] = df['From Account'].map(pagerank).fillna(0)\n",
                                        "            df['pagerank_to'] = df['To Account'].map(pagerank).fillna(0)\n",
                                        "\n",
                                        "            # Calcular Betweenness Centrality (amostra para performance)\n",
                                        "            print(\"   Computing Betweenness Centrality...\")\n",
                                        "            if G.number_of_nodes() > 1000:\n",
                                        "                # Para grafos grandes, calcular apenas para uma amostra\n",
                                        "                sample_nodes = list(G.nodes())[:1000]\n",
                                        "                betweenness = nx.betweenness_centrality_subset(G, sources=sample_nodes, targets=sample_nodes)\n",
                                        "            else:\n",
                                        "                betweenness = nx.betweenness_centrality(G)\n",
                                        "\n",
                                        "            df['betweenness_from'] = df['From Account'].map(betweenness).fillna(0)\n",
                                        "            df['betweenness_to'] = df['To Account'].map(betweenness).fillna(0)\n",
                                        "\n",
                                        "            # Calcular Clustering Coefficient\n",
                                        "            print(\"   Computing Clustering Coefficient...\")\n",
                                        "            clustering = nx.clustering(G.to_undirected())\n",
                                        "            df['clustering_from'] = df['From Account'].map(clustering).fillna(0)\n",
                                        "            df['clustering_to'] = df['To Account'].map(clustering).fillna(0)\n",
                                        "\n",
                                        "            # Calcular Degree Centrality\n",
                                        "            print(\"   Computing Degree Centrality...\")\n",
                                        "            degree_centrality = nx.degree_centrality(G)\n",
                                        "            df['degree_centrality_from'] = df['From Account'].map(degree_centrality).fillna(0)\n",
                                        "            df['degree_centrality_to'] = df['To Account'].map(degree_centrality).fillna(0)\n",
                                        "\n",
                                        "            print(\"   ‚úÖ Network features computed successfully\")\n",
                                        "\n",
                                        "        except Exception as e:\n",
                                        "            print(f\"   ‚ö†Ô∏è  NetworkX features failed: {e}\")\n",
                                        "            print(\"   Continuing without network features...\")\n",
                                        "            # Adicionar features b√°sicas de rede como fallback\n",
                                        "            df['pagerank_from'] = 0.0\n",
                                        "            df['pagerank_to'] = 0.0\n",
                                        "            df['betweenness_from'] = 0.0\n",
                                        "            df['betweenness_to'] = 0.0\n",
                                        "            df['clustering_from'] = 0.0\n",
                                        "            df['clustering_to'] = 0.0\n",
                                        "            df['degree_centrality_from'] = df['from_account_degree'] / df['from_account_degree'].max()\n",
                                        "            df['degree_centrality_to'] = df['to_account_degree'] / df['to_account_degree'].max()\n",
                                        "\n",
                                        "        # 9. Normaliza√ß√£o\n",
                                        "        print(\"9Ô∏è‚É£ Normalizing numerical features...\")\n",
                                        "        numeric_cols = [\n",
                                        "            'Amount Received', 'Amount Paid', 'timestamp_seconds', 'amount_ratio',\n",
                                        "            'amount_diff', 'amount_log', 'time_diff', 'time_diff_log', 'freq_hour', 'freq_day',\n",
                                        "            'from_account_degree', 'to_account_degree', 'rolling_mean_amount', 'rolling_std_amount',\n",
                                        "            'rolling_mean_freq', 'pagerank_from', 'pagerank_to', 'betweenness_from', 'betweenness_to',\n",
                                        "            'clustering_from', 'clustering_to', 'degree_centrality_from', 'degree_centrality_to'\n",
                                        "        ]\n",
                                        "\n",
                                        "        # Filtrar colunas que existem\n",
                                        "        numeric_cols = [col for col in numeric_cols if col in df.columns]\n",
                                        "\n",
                                        "        scaler = StandardScaler()\n",
                                        "        df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
                                        "        print(f\"   ‚úÖ Normalized {len(numeric_cols)} numerical features\")\n",
                                        "\n",
                                        "        # Salvar dados processados\n",
                                        "        processed_file = processed_dir / \"transactions_processed.csv\"\n",
                                        "        df.to_csv(processed_file, index=False)\n",
                                        "\n",
                                        "        print(f\"‚úÖ Feature engineering complete!\")\n",
                                        "        print(f\"   Total features: {len(df.columns)}\")\n",
                                        "        print(f\"   üíæ Saved processed data to: {processed_file}\")\n",
                                        "\n",
                                        "        # Estat√≠sticas finais\n",
                                        "        print(f\"\\nüìä Final dataset: {len(df):,} transactions, {len(df.columns)} features\")\n",
                                        "\n",
                                        "        # Salvar objetos globais\n",
                                        "        global df_processed, feature_scaler, categorical_encoders\n",
                                        "        df_processed = df\n",
                                        "        feature_scaler = scaler\n",
                                        "        categorical_encoders = label_encoders\n",
                                        "\n",
                                        "        print(\"=\" * 60)\n",
                                        "\n",
                                        "except Exception as e:\n",
                                        "    print(f\"‚ùå Feature engineering failed: {e}\")\n",
                                        "    print(\"=\" * 60)"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "1ba8ea76",
                              "metadata": {},
                              "source": [
                                        "## üèóÔ∏è Graph Construction\n",
                                        "\n",
                                        "Esta c√©lula constr√≥i o grafo para o Multi-GNN usando PyTorch Geometric."
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "388817a2",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# üèóÔ∏è GRAPH CONSTRUCTION\n",
                                        "print(\"üèóÔ∏è GRAPH CONSTRUCTION\")\n",
                                        "print(\"=\" * 60)\n",
                                        "\n",
                                        "try:\n",
                                        "    if 'df_processed' not in globals():\n",
                                        "        print(\"‚ùå Processed data not found! Run Feature Engineering cell first.\")\n",
                                        "    else:\n",
                                        "        df = df_processed\n",
                                        "        print(f\"Building graph from {len(df):,} transactions\")\n",
                                        "\n",
                                        "        # 1. Criar mapeamento de n√≥s (contas)\n",
                                        "        print(\"1Ô∏è‚É£ Creating account mapping...\")\n",
                                        "        all_accounts = pd.concat([df['From Account'], df['To Account']]).unique()\n",
                                        "        account_to_node = {acc: i for i, acc in enumerate(all_accounts)}\n",
                                        "        print(f\"   ‚úÖ Mapped {len(account_to_node):,} unique accounts to nodes\")\n",
                                        "\n",
                                        "        # 2. Criar arestas direcionadas\n",
                                        "        print(\"2Ô∏è‚É£ Building directed edges...\")\n",
                                        "        edges_from = df['From Account'].map(account_to_node).values\n",
                                        "        edges_to = df['To Account'].map(account_to_node).values\n",
                                        "        edge_index = torch.tensor([edges_from, edges_to], dtype=torch.long)\n",
                                        "        print(f\"   ‚úÖ Created {len(edge_index[0]):,} directed edges\")\n",
                                        "\n",
                                        "        # 3. Features dos n√≥s (contas)\n",
                                        "        print(\"3Ô∏è‚É£ Creating node features...\")\n",
                                        "        node_features = []\n",
                                        "\n",
                                        "        for account in all_accounts:\n",
                                        "            # Agregar features por conta\n",
                                        "            account_data = df[df['From Account'] == account]\n",
                                        "            if len(account_data) == 0:\n",
                                        "                account_data = df[df['To Account'] == account]\n",
                                        "\n",
                                        "            if len(account_data) > 0:\n",
                                        "                # Agregar estat√≠sticas da conta\n",
                                        "                features = [\n",
                                        "                    account_data['Amount Paid'].mean(),  # Volume m√©dio\n",
                                        "                    account_data['Amount Received'].mean(),  # Recebimento m√©dio\n",
                                        "                    len(account_data),  # N√∫mero de transa√ß√µes\n",
                                        "                    account_data['Is Laundering'].mean(),  # Risco m√©dio\n",
                                        "                    account_data['time_diff'].mean(),  # Tempo m√©dio entre transa√ß√µes\n",
                                        "                    account_data['pagerank_from'].iloc[0] if len(account_data) > 0 else 0,  # PageRank\n",
                                        "                    account_data['betweenness_from'].iloc[0] if len(account_data) > 0 else 0,  # Betweenness\n",
                                        "                    account_data['clustering_from'].iloc[0] if len(account_data) > 0 else 0,  # Clustering\n",
                                        "                    account_data['degree_centrality_from'].iloc[0] if len(account_data) > 0 else 0,  # Degree centrality\n",
                                        "                ]\n",
                                        "            else:\n",
                                        "                features = [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                                        "\n",
                                        "            node_features.append(features)\n",
                                        "\n",
                                        "        x = torch.tensor(node_features, dtype=torch.float)\n",
                                        "        print(f\"   ‚úÖ Node features shape: {x.shape}\")\n",
                                        "\n",
                                        "        # 4. Labels das arestas (transa√ß√µes)\n",
                                        "        y = torch.tensor(df['Is Laundering'].values, dtype=torch.long)\n",
                                        "        print(f\"   ‚úÖ Edge labels shape: {y.shape}\")\n",
                                        "        print(f\"   Positive edges: {y.sum().item():,} ({y.sum().item()/len(y)*100:.2f}%)\")\n",
                                        "\n",
                                        "        # 5. Features das arestas (transa√ß√µes)\n",
                                        "        print(\"5Ô∏è‚É£ Creating edge features...\")\n",
                                        "        edge_features = []\n",
                                        "\n",
                                        "        for _, row in df.iterrows():\n",
                                        "            edge_feat = [\n",
                                        "                row['Amount Paid'],\n",
                                        "                row['Amount Received'],\n",
                                        "                row['amount_ratio'],\n",
                                        "                row['amount_diff'],\n",
                                        "                row['amount_log'],\n",
                                        "                row['time_diff'],\n",
                                        "                row['time_diff_log'],\n",
                                        "                row['freq_hour'],\n",
                                        "                row['freq_day'],\n",
                                        "                row['from_account_degree'],\n",
                                        "                row['to_account_degree'],\n",
                                        "                row['rolling_mean_amount'],\n",
                                        "                row['rolling_std_amount'],\n",
                                        "                row['rolling_mean_freq'],\n",
                                        "                row['pagerank_from'],\n",
                                        "                row['pagerank_to'],\n",
                                        "                row['betweenness_from'],\n",
                                        "                row['betweenness_to'],\n",
                                        "                row['clustering_from'],\n",
                                        "                row['clustering_to'],\n",
                                        "                row['degree_centrality_from'],\n",
                                        "                row['degree_centrality_to'],\n",
                                        "                # Features codificadas\n",
                                        "                row.get('Receiving Currency_encoded', 0),\n",
                                        "                row.get('Payment Currency_encoded', 0),\n",
                                        "                row.get('Payment Format_encoded', 0),\n",
                                        "                # Features temporais\n",
                                        "                row['hour'] / 23.0,  # Normalizar\n",
                                        "                row['day_of_week'] / 6.0,  # Normalizar\n",
                                        "                row['month'] / 12.0,  # Normalizar\n",
                                        "            ]\n",
                                        "            edge_features.append(edge_feat)\n",
                                        "\n",
                                        "        edge_attr = torch.tensor(edge_features, dtype=torch.float)\n",
                                        "        print(f\"   ‚úÖ Edge features shape: {edge_attr.shape}\")\n",
                                        "\n",
                                        "        # 6. Criar objeto Data do PyTorch Geometric\n",
                                        "        print(\"6Ô∏è‚É£ Building PyG Data object...\")\n",
                                        "        graph_data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)\n",
                                        "\n",
                                        "        print(\"\\nüìä Graph Statistics:\")\n",
                                        "        print(f\"   Nodes: {graph_data.num_nodes:,}\")\n",
                                        "        print(f\"   Edges: {graph_data.num_edges:,}\")\n",
                                        "        print(f\"   Node features: {graph_data.x.shape[1]}\")\n",
                                        "        print(f\"   Edge features: {graph_data.edge_attr.shape[1]}\")\n",
                                        "        print(f\"   Positive edges: {y.sum().item():,} ({y.sum().item()/len(y)*100:.2f}%)\")\n",
                                        "\n",
                                        "        # Salvar grafo\n",
                                        "        models_dir = Path(\"/content/models\")\n",
                                        "        models_dir.mkdir(exist_ok=True)\n",
                                        "        graph_file = models_dir / \"graph_data.pt\"\n",
                                        "        torch.save(graph_data, graph_file)\n",
                                        "        print(f\"üíæ Saved graph to: {graph_file}\")\n",
                                        "\n",
                                        "        # Salvar mapeamento de contas\n",
                                        "        account_mapping_file = models_dir / \"account_mapping.json\"\n",
                                        "        with open(account_mapping_file, 'w') as f:\n",
                                        "            # Converter chaves para string (contas podem ser n√∫meros grandes)\n",
                                        "            json.dump({str(k): v for k, v in account_to_node.items()}, f)\n",
                                        "        print(f\"üíæ Saved account mapping to: {account_mapping_file}\")\n",
                                        "\n",
                                        "        print(\"‚úÖ Graph construction complete!\")\n",
                                        "        print(\"=\" * 60)\n",
                                        "\n",
                                        "        # Salvar objetos globais\n",
                                        "        global pyg_graph_data, node_mapping\n",
                                        "        pyg_graph_data = graph_data\n",
                                        "        node_mapping = account_to_node\n",
                                        "\n",
                                        "except Exception as e:\n",
                                        "    print(f\"‚ùå Graph construction failed: {e}\")\n",
                                        "    print(\"=\" * 60)"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "bb096525",
                              "metadata": {},
                              "source": [
                                        "## ü§ñ Model Definition\n",
                                        "\n",
                                        "Esta c√©lula define a arquitetura do modelo GNN para classifica√ß√£o de arestas."
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "15085c2d",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# ü§ñ MODEL ARCHITECTURE\n",
                                        "print(\"ü§ñ MODEL ARCHITECTURE\")\n",
                                        "print(\"=\" * 60)\n",
                                        "\n",
                                        "class EdgeGINModel(torch.nn.Module):\n",
                                        "    \"\"\"Modelo GIN para classifica√ß√£o de arestas (transa√ß√µes) em AML.\"\"\"\n",
                                        "\n",
                                        "    def __init__(self, num_node_features, num_edge_features, hidden_channels=128, num_classes=2):\n",
                                        "        super(EdgeGINModel, self).__init__()\n",
                                        "\n",
                                        "        self.num_node_features = num_node_features\n",
                                        "        self.num_edge_features = num_edge_features\n",
                                        "        self.hidden_channels = hidden_channels\n",
                                        "\n",
                                        "        # Encoder de arestas\n",
                                        "        self.edge_encoder = torch.nn.Sequential(\n",
                                        "            torch.nn.Linear(num_edge_features, hidden_channels),\n",
                                        "            torch.nn.ReLU(),\n",
                                        "            torch.nn.Dropout(config.DROPOUT),\n",
                                        "            torch.nn.Linear(hidden_channels, hidden_channels),\n",
                                        "            torch.nn.ReLU(),\n",
                                        "            torch.nn.Dropout(config.DROPOUT)\n",
                                        "        )\n",
                                        "\n",
                                        "        # Camadas GIN para n√≥s\n",
                                        "        self.conv1 = GINConv(\n",
                                        "            torch.nn.Sequential(\n",
                                        "                torch.nn.Linear(num_node_features, hidden_channels),\n",
                                        "                torch.nn.ReLU(),\n",
                                        "                torch.nn.Linear(hidden_channels, hidden_channels)\n",
                                        "            )\n",
                                        "        )\n",
                                        "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
                                        "\n",
                                        "        self.conv2 = GINConv(\n",
                                        "            torch.nn.Sequential(\n",
                                        "                torch.nn.Linear(hidden_channels, hidden_channels),\n",
                                        "                torch.nn.ReLU(),\n",
                                        "                torch.nn.Linear(hidden_channels, hidden_channels)\n",
                                        "            )\n",
                                        "        )\n",
                                        "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
                                        "\n",
                                        "        if config.NUM_LAYERS >= 3:\n",
                                        "            self.conv3 = GINConv(\n",
                                        "                torch.nn.Sequential(\n",
                                        "                    torch.nn.Linear(hidden_channels, hidden_channels),\n",
                                        "                    torch.nn.ReLU(),\n",
                                        "                    torch.nn.Linear(hidden_channels, hidden_channels)\n",
                                        "                )\n",
                                        "            )\n",
                                        "            self.bn3 = torch.nn.BatchNorm1d(hidden_channels)\n",
                                        "\n",
                                        "        # Classificador de arestas\n",
                                        "        self.edge_classifier = torch.nn.Sequential(\n",
                                        "            torch.nn.Linear(hidden_channels * 2 + hidden_channels, hidden_channels),\n",
                                        "            torch.nn.ReLU(),\n",
                                        "            torch.nn.Dropout(config.DROPOUT),\n",
                                        "            torch.nn.Linear(hidden_channels, hidden_channels // 2),\n",
                                        "            torch.nn.ReLU(),\n",
                                        "            torch.nn.Dropout(config.DROPOUT),\n",
                                        "            torch.nn.Linear(hidden_channels // 2, num_classes)\n",
                                        "        )\n",
                                        "\n",
                                        "    def forward(self, x, edge_index, edge_attr):\n",
                                        "        # Codificar features das arestas\n",
                                        "        edge_features = self.edge_encoder(edge_attr)\n",
                                        "\n",
                                        "        # Camadas GIN\n",
                                        "        x = self.conv1(x, edge_index)\n",
                                        "        x = self.bn1(x)\n",
                                        "        x = F.relu(x)\n",
                                        "        x = F.dropout(x, p=config.DROPOUT, training=self.training)\n",
                                        "\n",
                                        "        x = self.conv2(x, edge_index)\n",
                                        "        x = self.bn2(x)\n",
                                        "        x = F.relu(x)\n",
                                        "        x = F.dropout(x, p=config.DROPOUT, training=self.training)\n",
                                        "\n",
                                        "        if config.NUM_LAYERS >= 3:\n",
                                        "            x = self.conv3(x, edge_index)\n",
                                        "            x = self.bn3(x)\n",
                                        "            x = F.relu(x)\n",
                                        "            x = F.dropout(x, p=config.DROPOUT, training=self.training)\n",
                                        "\n",
                                        "        # Para cada aresta, concatenar features dos n√≥s de origem/destino + features da aresta\n",
                                        "        row, col = edge_index\n",
                                        "        edge_embeddings = torch.cat([x[row], x[col], edge_features], dim=1)\n",
                                        "\n",
                                        "        # Classificar arestas\n",
                                        "        out = self.edge_classifier(edge_embeddings)\n",
                                        "        return out\n",
                                        "\n",
                                        "# Instanciar modelo\n",
                                        "try:\n",
                                        "    if 'pyg_graph_data' not in globals():\n",
                                        "        print(\"‚ùå Graph data not found! Run Graph Construction cell first.\")\n",
                                        "    else:\n",
                                        "        model = EdgeGINModel(\n",
                                        "            num_node_features=pyg_graph_data.x.shape[1],\n",
                                        "            num_edge_features=pyg_graph_data.edge_attr.shape[1],\n",
                                        "            hidden_channels=config.HIDDEN_CHANNELS,\n",
                                        "            num_classes=2\n",
                                        "        ).to(device)\n",
                                        "\n",
                                        "        # Contar par√¢metros\n",
                                        "        total_params = sum(p.numel() for p in model.parameters())\n",
                                        "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
                                        "\n",
                                        "        print(\"‚úÖ Model created successfully!\")\n",
                                        "        print(f\"   Architecture: {config.GNN_TYPE}\")\n",
                                        "        print(f\"   Parameters: {total_params:,} (trainable: {trainable_params:,})\")\n",
                                        "        print(f\"   Node features: {pyg_graph_data.x.shape[1]}\")\n",
                                        "        print(f\"   Edge features: {pyg_graph_data.edge_attr.shape[1]}\")\n",
                                        "        print(f\"   Hidden channels: {config.HIDDEN_CHANNELS}\")\n",
                                        "        print(f\"   Layers: {config.NUM_LAYERS}\")\n",
                                        "        print(f\"   Dropout: {config.DROPOUT}\")\n",
                                        "\n",
                                        "        print(\"\\nüìã Model Architecture:\")\n",
                                        "        print(model)\n",
                                        "\n",
                                        "        # Salvar modelo global\n",
                                        "        global gnn_model\n",
                                        "        gnn_model = model\n",
                                        "\n",
                                        "        print(\"=\" * 60)\n",
                                        "\n",
                                        "except Exception as e:\n",
                                        "    print(f\"‚ùå Model creation failed: {e}\")\n",
                                        "    print(\"=\" * 60)"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "99b884b5",
                              "metadata": {},
                              "source": [
                                        "## ‚öôÔ∏è Training Setup\n",
                                        "\n",
                                        "Esta c√©lula configura otimizador, loss function, scheduler e early stopping."
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "1b47ee25",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# ‚öôÔ∏è TRAINING SETUP\n",
                                        "print(\"‚öôÔ∏è TRAINING SETUP\")\n",
                                        "print(\"=\" * 60)\n",
                                        "\n",
                                        "try:\n",
                                        "    if 'gnn_model' not in globals():\n",
                                        "        print(\"‚ùå Model not found! Run Model Definition cell first.\")\n",
                                        "    else:\n",
                                        "        # Dividir dados em treino/val/test\n",
                                        "        print(\"üìä Data split...\")\n",
                                        "\n",
                                        "        # Usar train_test_split estratificado\n",
                                        "        from sklearn.model_selection import train_test_split\n",
                                        "\n",
                                        "        # √çndices das arestas\n",
                                        "        edge_indices = np.arange(len(pyg_graph_data.y))\n",
                                        "\n",
                                        "        # Split estratificado\n",
                                        "        train_val_idx, test_idx = train_test_split(\n",
                                        "            edge_indices,\n",
                                        "            test_size=config.TEST_SIZE,\n",
                                        "            stratify=pyg_graph_data.y.numpy(),\n",
                                        "            random_state=config.RANDOM_STATE\n",
                                        "        )\n",
                                        "\n",
                                        "        train_idx, val_idx = train_test_split(\n",
                                        "            train_val_idx,\n",
                                        "            test_size=config.VAL_SIZE / (1 - config.TEST_SIZE),\n",
                                        "            stratify=pyg_graph_data.y.numpy()[train_val_idx],\n",
                                        "            random_state=config.RANDOM_STATE\n",
                                        "        )\n",
                                        "\n",
                                        "        print(f\"   Train: {len(train_idx):,} edges ({len(train_idx)/len(edge_indices)*100:.1f}%)\")\n",
                                        "        print(f\"   Val:   {len(val_idx):,} edges ({len(val_idx)/len(edge_indices)*100:.1f}%)\")\n",
                                        "        print(f\"   Test:  {len(test_idx):,} edges ({len(test_idx)/len(edge_indices)*100:.1f}%)\")\n",
                                        "\n",
                                        "        # Otimizador\n",
                                        "        optimizer = torch.optim.AdamW(\n",
                                        "            gnn_model.parameters(),\n",
                                        "            lr=config.LEARNING_RATE,\n",
                                        "            weight_decay=config.WEIGHT_DECAY\n",
                                        "        )\n",
                                        "        print(f\"‚úÖ Optimizer: AdamW (lr={config.LEARNING_RATE}, wd={config.WEIGHT_DECAY})\")\n",
                                        "\n",
                                        "        # Loss function com pesos para classe desbalanceada\n",
                                        "        pos_weight = torch.tensor([config.POS_WEIGHT], dtype=torch.float).to(device)\n",
                                        "        criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
                                        "        print(f\"‚úÖ Loss: BCEWithLogitsLoss (pos_weight={config.POS_WEIGHT:.2f})\")\n",
                                        "\n",
                                        "        # Scheduler\n",
                                        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
                                        "            optimizer, mode='max', factor=0.5, patience=10,\n",
                                        "            min_lr=1e-6, verbose=True\n",
                                        "        )\n",
                                        "        print(\"‚úÖ Scheduler: ReduceLROnPlateau\")\n",
                                        "\n",
                                        "        # Early stopping\n",
                                        "        class EarlyStopping:\n",
                                        "            def __init__(self, patience=15, min_delta=0.001):\n",
                                        "                self.patience = patience\n",
                                        "                self.min_delta = min_delta\n",
                                        "                self.counter = 0\n",
                                        "                self.best_score = None\n",
                                        "                self.early_stop = False\n",
                                        "\n",
                                        "            def __call__(self, val_score):\n",
                                        "                if self.best_score is None:\n",
                                        "                    self.best_score = val_score\n",
                                        "                elif val_score < self.best_score + self.min_delta:\n",
                                        "                    self.counter += 1\n",
                                        "                    if self.counter >= self.patience:\n",
                                        "                        self.early_stop = True\n",
                                        "                else:\n",
                                        "                    self.best_score = val_score\n",
                                        "                    self.counter = 0\n",
                                        "\n",
                                        "        early_stopping = EarlyStopping(patience=config.PATIENCE, min_delta=config.MIN_DELTA)\n",
                                        "        print(f\"‚úÖ Early stopping: patience={config.PATIENCE}, min_delta={config.MIN_DELTA}\")\n",
                                        "\n",
                                        "        # Salvar objetos globais\n",
                                        "        global train_indices, val_indices, test_indices, gnn_optimizer, gnn_criterion, gnn_scheduler, gnn_early_stopping\n",
                                        "        train_indices = train_idx\n",
                                        "        val_indices = val_idx\n",
                                        "        test_indices = test_idx\n",
                                        "        gnn_optimizer = optimizer\n",
                                        "        gnn_criterion = criterion\n",
                                        "        gnn_scheduler = scheduler\n",
                                        "        gnn_early_stopping = early_stopping\n",
                                        "\n",
                                        "        print(\"‚úÖ Training setup complete!\")\n",
                                        "        print(\"=\" * 60)\n",
                                        "\n",
                                        "except Exception as e:\n",
                                        "    print(f\"‚ùå Training setup failed: {e}\")\n",
                                        "    print(\"=\" * 60)"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "20fc9760",
                              "metadata": {},
                              "source": [
                                        "## üöÄ TREINAMENTO\n",
                                        "\n",
                                        "Esta c√©lula executa o treinamento completo do Multi-GNN com monitoramento de m√©tricas."
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "8b35f858",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# üöÄ TRAINING\n",
                                        "print(\"üöÄ TRAINING\")\n",
                                        "print(\"=\" * 60)\n",
                                        "\n",
                                        "def train_epoch(model, optimizer, criterion, data, train_idx):\n",
                                        "    \"\"\"Treina por uma √©poca.\"\"\"\n",
                                        "    model.train()\n",
                                        "    total_loss = 0\n",
                                        "\n",
                                        "    # Forward pass apenas para arestas de treino\n",
                                        "    out = model(data.x, data.edge_index, data.edge_attr)\n",
                                        "\n",
                                        "    # Loss apenas para arestas de treino\n",
                                        "    loss = criterion(out[train_idx], data.y[train_idx].float().unsqueeze(1))\n",
                                        "    loss.backward()\n",
                                        "    optimizer.step()\n",
                                        "    optimizer.zero_grad()\n",
                                        "\n",
                                        "    return loss.item()\n",
                                        "\n",
                                        "def evaluate(model, data, idx):\n",
                                        "    \"\"\"Avalia o modelo.\"\"\"\n",
                                        "    model.eval()\n",
                                        "    with torch.no_grad():\n",
                                        "        out = model(data.x, data.edge_index, data.edge_attr)\n",
                                        "        pred = torch.sigmoid(out[idx]).squeeze()\n",
                                        "        pred_binary = (pred > 0.5).float()\n",
                                        "\n",
                                        "        labels = data.y[idx].float()\n",
                                        "\n",
                                        "        # M√©tricas\n",
                                        "        accuracy = (pred_binary == labels).float().mean().item()\n",
                                        "\n",
                                        "        # F1 Score\n",
                                        "        tp = ((pred_binary == 1) & (labels == 1)).sum().item()\n",
                                        "        fp = ((pred_binary == 1) & (labels == 0)).sum().item()\n",
                                        "        fn = ((pred_binary == 0) & (labels == 1)).sum().item()\n",
                                        "\n",
                                        "        precision = tp / (tp + fp + 1e-6)\n",
                                        "        recall = tp / (tp + fn + 1e-6)\n",
                                        "        f1 = 2 * (precision * recall) / (precision + recall + 1e-6)\n",
                                        "\n",
                                        "        # AUC\n",
                                        "        try:\n",
                                        "            from sklearn.metrics import roc_auc_score\n",
                                        "            auc = roc_auc_score(labels.cpu().numpy(), pred.cpu().numpy())\n",
                                        "        except:\n",
                                        "            auc = 0.5\n",
                                        "\n",
                                        "    return accuracy, f1, auc\n",
                                        "\n",
                                        "try:\n",
                                        "    if 'gnn_model' not in globals():\n",
                                        "        print(\"‚ùå Model not found! Run Training Setup cell first.\")\n",
                                        "    else:\n",
                                        "        print(f\"üéØ Training {config.GNN_TYPE} for {config.EPOCHS} epochs...\")\n",
                                        "\n",
                                        "        # Hist√≥rico de treinamento\n",
                                        "        history = {\n",
                                        "            'epoch': [], 'train_loss': [], 'train_acc': [], 'train_f1': [], 'train_auc': [],\n",
                                        "            'val_acc': [], 'val_f1': [], 'val_auc': [], 'lr': []\n",
                                        "        }\n",
                                        "\n",
                                        "        best_val_f1 = 0\n",
                                        "        best_model_state = None\n",
                                        "\n",
                                        "        print(\"Epoch | Train Loss | Train Acc | Train F1 | Val Acc | Val F1 | Val AUC | LR\")\n",
                                        "        print(\"-\" * 80)\n",
                                        "\n",
                                        "        for epoch in range(config.EPOCHS):\n",
                                        "            # Treinar\n",
                                        "            train_loss = train_epoch(gnn_model, gnn_optimizer, gnn_criterion, pyg_graph_data, train_indices)\n",
                                        "\n",
                                        "            # Avaliar treino\n",
                                        "            train_acc, train_f1, train_auc = evaluate(gnn_model, pyg_graph_data, train_indices)\n",
                                        "\n",
                                        "            # Avaliar valida√ß√£o\n",
                                        "            val_acc, val_f1, val_auc = evaluate(gnn_model, pyg_graph_data, val_indices)\n",
                                        "\n",
                                        "            # Learning rate atual\n",
                                        "            current_lr = gnn_optimizer.param_groups[0]['lr']\n",
                                        "\n",
                                        "            # Salvar melhores pesos\n",
                                        "            if val_f1 > best_val_f1:\n",
                                        "                best_val_f1 = val_f1\n",
                                        "                best_model_state = gnn_model.state_dict().copy()\n",
                                        "\n",
                                        "            # Scheduler step\n",
                                        "            gnn_scheduler.step(val_f1)\n",
                                        "\n",
                                        "            # Early stopping\n",
                                        "            gnn_early_stopping(val_f1)\n",
                                        "\n",
                                        "            # Logging\n",
                                        "            print(f\"{epoch+1:5d} | {train_loss:10.4f} | {train_acc:9.3f} | {train_f1:8.3f} | {val_acc:7.3f} | {val_f1:6.3f} | {val_auc:7.3f} | {current_lr:.2e}\")\n",
                                        "\n",
                                        "            # Salvar hist√≥rico\n",
                                        "            history['epoch'].append(epoch+1)\n",
                                        "            history['train_loss'].append(train_loss)\n",
                                        "            history['train_acc'].append(train_acc)\n",
                                        "            history['train_f1'].append(train_f1)\n",
                                        "            history['train_auc'].append(train_auc)\n",
                                        "            history['val_acc'].append(val_acc)\n",
                                        "            history['val_f1'].append(val_f1)\n",
                                        "            history['val_auc'].append(val_auc)\n",
                                        "            history['lr'].append(current_lr)\n",
                                        "\n",
                                        "            # Early stopping\n",
                                        "            if gnn_early_stopping.early_stop:\n",
                                        "                print(f\"\\n‚ö†Ô∏è  Early stopping triggered at epoch {epoch+1}\")\n",
                                        "                print(f\"   Best epoch: {np.argmax(history['val_f1'])+1} (Val F1: {best_val_f1:.4f})\")\n",
                                        "                break\n",
                                        "\n",
                                        "        # Carregar melhores pesos\n",
                                        "        if best_model_state:\n",
                                        "            gnn_model.load_state_dict(best_model_state)\n",
                                        "\n",
                                        "        # Salvar modelo\n",
                                        "        models_dir = Path(\"/content/models\")\n",
                                        "        model_file = models_dir / f\"{config.GNN_TYPE}_best_model.pth\"\n",
                                        "        torch.save({\n",
                                        "            'model_state_dict': gnn_model.state_dict(),\n",
                                        "            'config': config.__dict__,\n",
                                        "            'history': history,\n",
                                        "            'best_val_f1': best_val_f1\n",
                                        "        }, model_file)\n",
                                        "\n",
                                        "        print(f\"\\n‚úÖ Training complete!\")\n",
                                        "        print(f\"   Best validation F1: {best_val_f1:.4f}\")\n",
                                        "        print(f\"   Model saved to: {model_file}\")\n",
                                        "\n",
                                        "        # Salvar hist√≥rico global\n",
                                        "        global training_history\n",
                                        "        training_history = history\n",
                                        "\n",
                                        "        print(\"=\" * 60)\n",
                                        "\n",
                                        "except Exception as e:\n",
                                        "    print(f\"‚ùå Training failed: {e}\")\n",
                                        "    print(\"=\" * 60)"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "269502ea",
                              "metadata": {},
                              "source": [
                                        "## üìä Evaluation\n",
                                        "\n",
                                        "Esta c√©lula avalia o modelo no conjunto de teste e gera m√©tricas finais."
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "356efd13",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# üìä EVALUATION\n",
                                        "print(\"üìä EVALUATION\")\n",
                                        "print(\"=\" * 60)\n",
                                        "\n",
                                        "try:\n",
                                        "    if 'gnn_model' not in globals():\n",
                                        "        print(\"‚ùå Model not found! Run training first.\")\n",
                                        "    else:\n",
                                        "        # Avaliar no conjunto de teste\n",
                                        "        test_acc, test_f1, test_auc = evaluate(gnn_model, pyg_graph_data, test_indices)\n",
                                        "\n",
                                        "        print(\"‚úÖ Test Set Results:\")\n",
                                        "        print(f\"   Loss: N/A (computed during training)\")\n",
                                        "        print(f\"   Accuracy: {test_acc:.4f}\")\n",
                                        "        print(f\"   F1 Score: {test_f1:.4f}\")\n",
                                        "        print(f\"   ROC-AUC: {test_auc:.4f}\")\n",
                                        "\n",
                                        "        # Classification report detalhado\n",
                                        "        gnn_model.eval()\n",
                                        "        with torch.no_grad():\n",
                                        "            out = gnn_model(pyg_graph_data.x, pyg_graph_data.edge_index, pyg_graph_data.edge_attr)\n",
                                        "            test_pred = torch.sigmoid(out[test_indices]).squeeze()\n",
                                        "            test_pred_binary = (test_pred > 0.5).cpu().numpy()\n",
                                        "            test_labels = pyg_graph_data.y[test_indices].cpu().numpy()\n",
                                        "\n",
                                        "        print(\"üìã Classification Report:\")\n",
                                        "        print(classification_report(test_labels, test_pred_binary, target_names=['Legitimate', 'Laundering']))\n",
                                        "\n",
                                        "        # Confusion matrix\n",
                                        "        cm = confusion_matrix(test_labels, test_pred_binary)\n",
                                        "        print(\"üìä Confusion Matrix:\")\n",
                                        "        print(f\"   {cm}\")\n",
                                        "\n",
                                        "        # Salvar resultados\n",
                                        "        results_dir = Path(\"/content/results\")\n",
                                        "        results_dir.mkdir(exist_ok=True)\n",
                                        "\n",
                                        "        # Salvar m√©tricas\n",
                                        "        metrics = {\n",
                                        "            'model': config.GNN_TYPE,\n",
                                        "            'dataset': 'IBM AML (Kaggle or Synthetic)',\n",
                                        "            'num_transactions': len(pyg_graph_data.y),\n",
                                        "            'num_nodes': pyg_graph_data.num_nodes,\n",
                                        "            'num_edges': pyg_graph_data.num_edges,\n",
                                        "            'test_accuracy': test_acc,\n",
                                        "            'test_f1': test_f1,\n",
                                        "            'test_auc': test_auc,\n",
                                        "            'best_val_f1': best_val_f1 if 'best_val_f1' in globals() else 0,\n",
                                        "            'training_epochs': len(training_history['epoch']) if 'training_history' in globals() else 0,\n",
                                        "            'parameters': sum(p.numel() for p in gnn_model.parameters()),\n",
                                        "            'timestamp': datetime.now().isoformat()\n",
                                        "        }\n",
                                        "\n",
                                        "        metrics_file = results_dir / f\"evaluation_results_{config.GNN_TYPE}.json\"\n",
                                        "        with open(metrics_file, 'w') as f:\n",
                                        "            json.dump(metrics, f, indent=2)\n",
                                        "\n",
                                        "        print(f\"\\nüíæ Saved evaluation results to: {metrics_file}\")\n",
                                        "        print(\"=\" * 60)\n",
                                        "\n",
                                        "except Exception as e:\n",
                                        "    print(f\"‚ùå Evaluation failed: {e}\")\n",
                                        "    print(\"=\" * 60)"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "d2854dd1",
                              "metadata": {},
                              "source": [
                                        "## üì§ Export\n",
                                        "\n",
                                        "Esta c√©lula exporta as predi√ß√µes para benchmark contra XGBoost."
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "44003311",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "# üì§ EXPORTING PREDICTIONS\n",
                                        "print(\"üì§ EXPORTING PREDICTIONS\")\n",
                                        "print(\"=\" * 60)\n",
                                        "\n",
                                        "try:\n",
                                        "    if 'gnn_model' not in globals():\n",
                                        "        print(\"‚ùå Model not found! Run training first.\")\n",
                                        "    else:\n",
                                        "        # Gerar predi√ß√µes para todas as transa√ß√µes\n",
                                        "        gnn_model.eval()\n",
                                        "        with torch.no_grad():\n",
                                        "            out = gnn_model(pyg_graph_data.x, pyg_graph_data.edge_index, pyg_graph_data.edge_attr)\n",
                                        "            all_pred_probs = torch.sigmoid(out).squeeze().cpu().numpy()\n",
                                        "            all_labels = pyg_graph_data.y.cpu().numpy()\n",
                                        "\n",
                                        "        # Criar DataFrame de resultados\n",
                                        "        results_df = pd.DataFrame({\n",
                                        "            'prediction_prob': all_pred_probs,\n",
                                        "            'ground_truth': all_labels\n",
                                        "        })\n",
                                        "\n",
                                        "        # Adicionar coluna de predi√ß√µes bin√°rias\n",
                                        "        results_df['prediction'] = (results_df['prediction_prob'] > 0.5).astype(int)\n",
                                        "\n",
                                        "        # Salvar predi√ß√µes\n",
                                        "        output_file = \"/content/multi_gnn_predictions.csv\"\n",
                                        "        results_df.to_csv(output_file, index=False)\n",
                                        "\n",
                                        "        print(f\"‚úÖ Exported predictions to: {output_file}\")\n",
                                        "        print(f\"   Total predictions: {len(results_df):,}\")\n",
                                        "\n",
                                        "        # Estat√≠sticas das predi√ß√µes\n",
                                        "        pred_dist = results_df['prediction'].value_counts().sort_index()\n",
                                        "        print(\"üìä Prediction Distribution:\")\n",
                                        "        for class_val, count in pred_dist.items():\n",
                                        "            class_name = \"Laundering\" if class_val == 1 else \"Legitimate\"\n",
                                        "            percentage = count / len(results_df) * 100\n",
                                        "            print(\".1f\")\n",
                                        "\n",
                                        "        # Estat√≠sticas por conjunto\n",
                                        "        splits = {\n",
                                        "            'train': train_indices,\n",
                                        "            'val': val_indices,\n",
                                        "            'test': test_indices\n",
                                        "        }\n",
                                        "\n",
                                        "        print(\"\\nüìà Summary by Split:\")\n",
                                        "        print(\"       Count  Avg Prob  Positive Cases\")\n",
                                        "        print(\"split                                 \")\n",
                                        "        print(\"-\" * 40)\n",
                                        "\n",
                                        "        for split_name, indices in splits.items():\n",
                                        "            split_preds = results_df.iloc[indices]\n",
                                        "            count = len(split_preds)\n",
                                        "            avg_prob = split_preds['prediction_prob'].mean()\n",
                                        "            pos_cases = split_preds['prediction'].sum()\n",
                                        "            print(f\"{split_name:6} {count:8,} {avg_prob:8.3f} {pos_cases:13,}\")\n",
                                        "\n",
                                        "        # Salvar hist√≥rico de treinamento\n",
                                        "        if 'training_history' in globals():\n",
                                        "            history_file = results_dir / f\"training_history_{config.GNN_TYPE}.json\"\n",
                                        "            with open(history_file, 'w') as f:\n",
                                        "                json.dump(training_history, f)\n",
                                        "            print(f\"\\nüíæ Saved training history to: {history_file}\")\n",
                                        "\n",
                                        "        # Benchmark summary\n",
                                        "        benchmark_summary = {\n",
                                        "            'model': config.GNN_TYPE,\n",
                                        "            'dataset': 'IBM AML',\n",
                                        "            'num_transactions': len(results_df),\n",
                                        "            'num_nodes': pyg_graph_data.num_nodes,\n",
                                        "            'num_edges': pyg_graph_data.num_edges,\n",
                                        "            'test_accuracy': test_acc if 'test_acc' in globals() else 0,\n",
                                        "            'test_f1': test_f1 if 'test_f1' in globals() else 0,\n",
                                        "            'test_auc': test_auc if 'test_auc' in globals() else 0,\n",
                                        "            'best_val_f1': best_val_f1 if 'best_val_f1' in globals() else 0,\n",
                                        "            'training_epochs': len(training_history['epoch']) if 'training_history' in globals() else 0,\n",
                                        "            'parameters': sum(p.numel() for p in gnn_model.parameters()),\n",
                                        "            'timestamp': datetime.now().isoformat()\n",
                                        "        }\n",
                                        "\n",
                                        "        summary_file = results_dir / f\"benchmark_summary_{config.GNN_TYPE}.json\"\n",
                                        "        with open(summary_file, 'w') as f:\n",
                                        "            json.dump(benchmark_summary, f, indent=2)\n",
                                        "\n",
                                        "        print(f\"üíæ Saved benchmark summary to: {summary_file}\")\n",
                                        "\n",
                                        "        print(\"\\nüéâ PIPELINE COMPLETE!\")\n",
                                        "        print(\"=\" * 60)\n",
                                        "        print(\"\\nüìÅ Output Files:\")\n",
                                        "        print(f\"   1. Predictions: {output_file}\")\n",
                                        "        print(f\"   2. Model: /content/models/{config.GNN_TYPE}_best_model.pth\")\n",
                                        "        print(f\"   3. Summary: {summary_file}\")\n",
                                        "        print(f\"   4. History: {history_file}\")\n",
                                        "        print(\"\\n‚úÖ Ready for benchmark comparison with XGBoost!\")\n",
                                        "\n",
                                        "except Exception as e:\n",
                                        "    print(f\"‚ùå Export failed: {e}\")\n",
                                        "    print(\"=\" * 60)"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "08cefd4a",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "## üì• Download dos Resultados\n",
                                        "\n",
                                        "# Ap√≥s a conclus√£o bem-sucedida, baixe o arquivo de predi√ß√µes para usar no benchmark contra XGBoost.\n",
                                        "\n",
                                        "### **Op√ß√£o 1: Download Manual (recomendado)**\n",
                                        "\n",
                                        "# python\n",
                                        "# Execute esta c√©lula para baixar arquivos importantes\n",
                                        "from google.colab import files\n",
                                        "\n",
                                        "# Download predi√ß√µes (para benchmark)\n",
                                        "files.download('/content/multi_gnn_predictions.csv')\n",
                                        "\n",
                                        "# Download m√©tricas\n",
                                        "files.download('/content/results/evaluation_results_GIN.json')\n",
                                        "\n",
                                        "# Download modelo (opcional - arquivo grande)\n",
                                        "# files.download('/content/models/GIN_best_model.pth')\n",
                                        "#\n",
                                        "\n",
                                        "### **Op√ß√£o 2: Salvar no Google Drive**\n",
                                        "\n",
                                        "# python\n",
                                        "# Monte o Google Drive\n",
                                        "from google.colab import drive\n",
                                        "drive.mount('/content/drive')\n",
                                        "\n",
                                        "# Copie resultados\n",
                                        "import shutil\n",
                                        "output_dir = '/content/drive/MyDrive/AML_GNN_Results'\n",
                                        "!mkdir -p \"{output_dir}\"\n",
                                        "\n",
                                        "!cp /content/multi_gnn_predictions.csv \"{output_dir}/\"\n",
                                        "!cp /content/results/evaluation_results_GIN.json \"{output_dir}/\"\n",
                                        "\n",
                                        "print(f\"‚úÖ Resultados salvos em: {output_dir}\")\n"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "fb3413c9",
                              "metadata": {},
                              "source": [
                                        "##  Download dos Resultados\n",
                                        "\n",
                                        "ApÔøΩs a conclusÔøΩo bem-sucedida, baixe o arquivo de prediÔøΩÔøΩes para usar no benchmark contra XGBoost.\n",
                                        "\n",
                                        "### **OpÔøΩÔøΩo 1: Download Manual (recomendado)**\n",
                                        "\n",
                                        "```python\n",
                                        "# Execute esta cÔøΩlula para baixar arquivos importantes\n",
                                        "from google.colab import files\n",
                                        "\n",
                                        "# Download prediÔøΩÔøΩes (para benchmark)\n",
                                        "files.download('/content/multi_gnn_predictions.csv')\n",
                                        "\n",
                                        "# Download mÔøΩtricas\n",
                                        "files.download('/content/results/evaluation_results_GIN.json')\n",
                                        "\n",
                                        "# Download modelo (opcional - arquivo grande)\n",
                                        "# files.download('/content/models/GIN_best_model.pth')\n",
                                        "```\n",
                                        "\n",
                                        "### **OpÔøΩÔøΩo 2: Salvar no Google Drive**\n",
                                        "\n",
                                        "```python\n",
                                        "# Monte o Google Drive\n",
                                        "from google.colab import drive\n",
                                        "drive.mount('/content/drive')\n",
                                        "\n",
                                        "# Copie resultados\n",
                                        "import shutil\n",
                                        "output_dir = '/content/drive/MyDrive/AML_GNN_Results'\n",
                                        "!mkdir -p \"{output_dir}\"\n",
                                        "\n",
                                        "!cp /content/multi_gnn_predictions.csv \"{output_dir}/\"\n",
                                        "!cp /content/results/evaluation_results_GIN.json \"{output_dir}/\"\n",
                                        "\n",
                                        "print(f\" Resultados salvos em: {output_dir}\")\n",
                                        "```"
                              ]
                    }
          ],
          "metadata": {
                    "kernelspec": {
                              "display_name": "multignn",
                              "language": "python",
                              "name": "python3"
                    },
                    "language_info": {
                              "codemirror_mode": {
                                        "name": "ipython",
                                        "version": 3
                              },
                              "file_extension": ".py",
                              "mimetype": "text/x-python",
                              "name": "python",
                              "nbconvert_exporter": "python",
                              "pygments_lexer": "ipython3",
                              "version": "3.9.23"
                    }
          },
          "nbformat": 4,
          "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a33d45d2",
   "metadata": {},
   "source": [
    "# 02 · Feature Engineering e Pipelines\n",
    "## Detecção de Lavagem de Dinheiro (AML)\n",
    "\n",
    "**Objetivo:** Construir features robustas para modelagem, com pipelines reprodutíveis e foco em prevenção de data leakage.\n",
    "\n",
    "### Introdução\n",
    "\n",
    "Em detecção de AML, features temporais e de rede são cruciais porque transações suspeitas frequentemente envolvem padrões sequenciais e conexões entre entidades. Escolhi implementar agregações por entidade porque dados financeiros são intrinsecamente temporais - um cliente que movimenta grandes valores em janelas curtas pode indicar comportamento de lavagem. Para evitar data leakage, todas as features são calculadas apenas com dados históricos disponíveis no momento da transação.\n",
    "\n",
    "### Pipeline de Transformação\n",
    "\n",
    "```\n",
    "Raw Data → Cleaning → Feature Engineering → Validation → Pipeline\n",
    "    ↓         ↓             ↓                ↓          ↓\n",
    "- Load     - Remove     - Temporal       - Correlation - Sklearn\n",
    "- Parse    - Duplicates - Aggregations   - Analysis   - Pipeline\n",
    "- Anonymize - Missing   - Network        - Leakage    - Reproducible\n",
    "            - Values    - Features       - Check      - Training\n",
    "```\n",
    "\n",
    "### Estratégia de Features\n",
    "\n",
    "1. **Features Temporais**: Agregações por entidade em janelas móveis (7d, 30d)\n",
    "2. **Features de Rede**: Métricas de conectividade entre contas\n",
    "3. **Features Categóricas**: Encoding seguro sem leakage\n",
    "4. **Features Derivadas**: Razões e taxas calculadas eticamente\n",
    "\n",
    "> **Decisão de Design:** Priorizei features interpretáveis sobre complexas para garantir que o modelo possa ser explicado para compliance regulatória."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1568e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup e Importações\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Adicionar src ao path\n",
    "sys.path.append(str(Path('..').resolve()))\n",
    "\n",
    "# Imports core\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Configuração visual\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b533a4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funções locais implementadas com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Configurar sys.path para importações\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Adicionar diretório src ao path\n",
    "sys.path.append(str(Path('..').resolve()))\n",
    "\n",
    "# Importações das funções refatoradas\n",
    "from src.features.aml_features import (\n",
    "    load_raw_transactions,\n",
    "    validate_data_compliance,\n",
    "    clean_transactions,\n",
    "    impute_and_encode,\n",
    "    aggregate_by_entity,\n",
    "    compute_network_features,\n",
    "    create_temporal_features,\n",
    "    create_network_features,\n",
    "    encode_categorical_features,\n",
    "    AMLFeaturePipeline\n",
    ")\n",
    "\n",
    "# Importar Pattern Feature Engineer\n",
    "from src.features.pattern_engineering import PatternFeatureEngineer\n",
    "\n",
    "# Importar função de cálculo de IV\n",
    "from src.features.iv_calculator import calculate_iv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf18b4e",
   "metadata": {},
   "source": [
    "## ▸ Carregamento e Preparação dos Dados\n",
    "\n",
    "Carregamos os dados brutos e aplicamos limpeza inicial, garantindo compliance e anonimização."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd5ee81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 5,078,345 transações | Fraude: 0.102%\n"
     ]
    }
   ],
   "source": [
    "# Carregar dados processados\n",
    "df_raw = pd.read_csv('../data/processed/transactions_enriched_final.csv')\n",
    "\n",
    "# Garantir tipos corretos\n",
    "df_raw['Timestamp'] = pd.to_datetime(df_raw['Timestamp'])\n",
    "df_raw['is_fraud'] = df_raw['is_fraud'].astype(int)\n",
    "\n",
    "print(f\"Dataset: {len(df_raw):,} transações | Fraude: {df_raw['is_fraud'].mean():.3%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e401d0",
   "metadata": {},
   "source": [
    "## ▸ Limpeza e Pré-processamento\n",
    "\n",
    "Aplicamos limpeza para remover duplicatas, valores inválidos e garantir integridade dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df6b6772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset limpo: 5,078,336 transações\n"
     ]
    }
   ],
   "source": [
    "# Limpeza básica dos dados\n",
    "df_clean = df_raw.drop_duplicates()\n",
    "df_clean = df_clean.dropna(subset=['Amount Paid', 'Timestamp'])\n",
    "df_clean = df_clean[df_clean['Amount Paid'] > 0]\n",
    "\n",
    "print(f\"Dataset limpo: {len(df_clean):,} transações\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43791bd2",
   "metadata": {},
   "source": [
    "## ▸ Features Temporais\n",
    "\n",
    "Criamos agregações por entidade em janelas temporais para capturar padrões comportamentais. Esta é a feature mais importante porque transações de lavagem frequentemente ocorrem em bursts temporais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fd755ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-19 13:17:28,137 - INFO - Creating temporal features with pandas...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[temporal] Starting temporal feature creation with pandas...\n",
      "[temporal] Processing window=7 days...\n",
      "[temporal] Finished window=7d in 1511.23s\n",
      "[temporal] Processing window=30 days...\n",
      "[temporal] Finished window=30d in 1210.69s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-19 14:02:52,949 - INFO - Created 12 temporal features using pandas\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[temporal] Done. Total time: 2724.81s\n",
      "Features temporais criadas: 42 colunas\n"
     ]
    }
   ],
   "source": [
    "# Garantir ordenação temporal antes de criar features\n",
    "df_clean_sorted = df_clean.sort_values('Timestamp').reset_index(drop=True)\n",
    "\n",
    "# Renomear colunas para formato esperado pela função\n",
    "df_clean_sorted = df_clean_sorted.rename(columns={\n",
    "    'Timestamp': 'timestamp',\n",
    "    'From Account': 'source',\n",
    "    'Amount Paid': 'amount'\n",
    "})\n",
    "\n",
    "temporal_features_full_df = create_temporal_features(df_clean_sorted, windows=[7, 30])\n",
    "\n",
    "print(f\"Features temporais criadas: {temporal_features_full_df.shape[1]} colunas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "949dbc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping Timestamp - too many unique values (14127)\n",
      "Warning: Skipping From Bank ID - too many unique values (5321)\n",
      "Warning: Skipping From Account - too many unique values (64297)\n",
      "Warning: Skipping To Bank ID - too many unique values (2655)\n",
      "Warning: Skipping To Account - too many unique values (79250)\n",
      "Warning: Skipping Amount Received - too many unique values (81825)\n",
      "Warning: Skipping Amount Paid - too many unique values (82001)\n",
      "Warning: Skipping Bank Name - too many unique values (3773)\n",
      "Warning: Skipping Bank ID - too many unique values (5321)\n",
      "Warning: Skipping Account Number - too many unique values (64297)\n",
      "Warning: Skipping Entity ID - too many unique values (48368)\n",
      "Warning: Skipping Entity Name - too many unique values (48368)\n",
      "Warning: Skipping Bank Name_to - too many unique values (2044)\n",
      "Warning: Skipping Bank ID_to - too many unique values (2655)\n",
      "Warning: Skipping Account Number_to - too many unique values (79250)\n",
      "Warning: Skipping Entity ID_to - too many unique values (63719)\n",
      "Warning: Skipping Entity Name_to - too many unique values (63719)\n",
      "Warning: Skipping is_night_transaction - constant variable\n",
      "Warning: Skipping timestamp - too many unique values (14098)\n",
      "Warning: Skipping source - too many unique values (64253)\n",
      "Warning: Skipping amount - too many unique values (82062)\n",
      "Warning: Skipping source_amount_sum_7d - too many unique values (98913)\n",
      "Warning: Skipping source_amount_mean_7d - too many unique values (98888)\n",
      "Warning: Skipping source_amount_std_7d - too many unique values (87857)\n",
      "Warning: Skipping source_transaction_count_7d - too many unique values (8049)\n",
      "Warning: Skipping source_amount_sum_30d - too many unique values (99006)\n",
      "Warning: Skipping source_amount_mean_30d - too many unique values (98974)\n",
      "Warning: Skipping source_amount_std_30d - too many unique values (88703)\n",
      "Warning: Skipping source_transaction_count_30d - too many unique values (8472)\n",
      "Features temporais preditivas: 3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Combinar features temporais com dados limpos\n",
    "df_with_temporal_full = pd.concat([df_clean.reset_index(drop=True), temporal_features_full_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Remover duplicatas se existirem\n",
    "df_with_temporal_full = df_with_temporal_full.loc[:, ~df_with_temporal_full.columns.duplicated()]\n",
    "\n",
    "# Amostra para análise IV\n",
    "df_temporal_iv_sample = df_with_temporal_full.sample(n=min(100000, len(df_with_temporal_full)), random_state=42)\n",
    "\n",
    "# Calcular IV\n",
    "iv_results_temporal = calculate_iv(df_temporal_iv_sample, target_col='is_fraud')\n",
    "\n",
    "temporal_cols = [col for col in temporal_features_full_df.columns if 'source_amount' in col or 'hour' in col or 'day_of_week' in col or 'is_business' in col or 'is_weekend' in col]\n",
    "temporal_iv_results = iv_results_temporal[iv_results_temporal['variable'].isin(temporal_cols)].copy()\n",
    "\n",
    "temporal_iv_results.to_csv('../artifacts/temporal_features_iv_report_corrected.csv', index=False)\n",
    "\n",
    "predictive_count = len(temporal_iv_results[temporal_iv_results['IV'] >= 0.02])\n",
    "print(f\"Features temporais preditivas: {predictive_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066293e7",
   "metadata": {},
   "source": [
    "## ▸ Features de Rede\n",
    "\n",
    "Analisamos a conectividade entre contas para identificar padrões de lavagem estruturada. Contas fraudulentas frequentemente formam clusters densos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14604cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-19 14:16:14,510 - INFO - Creating network features...\n",
      "2025-10-19 14:16:36,653 - INFO - Created network features for 515080 nodes\n",
      "2025-10-19 14:16:36,653 - INFO - Created network features for 515080 nodes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features de rede calculadas: 515080 nós\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Features de Rede\n",
    "# Renomear colunas para formato esperado pela função\n",
    "df_clean_network = df_clean.rename(columns={\n",
    "    'From Account': 'source',\n",
    "    'To Account': 'target'\n",
    "})\n",
    "\n",
    "network_features_full_df = create_network_features(df_clean_network)\n",
    "\n",
    "print(f\"Features de rede calculadas: {len(network_features_full_df)} nós\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc47735a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinar features de rede com dados transacionais\n",
    "df_with_network = df_clean.copy()\n",
    "\n",
    "# Renomear colunas para merge\n",
    "df_with_network = df_with_network.rename(columns={\n",
    "    'From Account': 'source',\n",
    "    'To Account': 'target'\n",
    "})\n",
    "\n",
    "# Merge features para source accounts\n",
    "df_with_network = df_with_network.merge(\n",
    "    network_features_full_df[['node', 'degree', 'in_degree', 'out_degree',\n",
    "                        'degree_centrality', 'in_degree_centrality', 'out_degree_centrality']],\n",
    "    left_on='source', right_on='node', how='left'\n",
    ").rename(columns={\n",
    "    'degree': 'source_degree', 'in_degree': 'source_in_degree', 'out_degree': 'source_out_degree',\n",
    "    'degree_centrality': 'source_degree_centrality', 'in_degree_centrality': 'source_in_degree_centrality',\n",
    "    'out_degree_centrality': 'source_out_degree_centrality'\n",
    "}).drop('node', axis=1, errors='ignore')\n",
    "\n",
    "# Merge features para target accounts\n",
    "df_with_network = df_with_network.merge(\n",
    "    network_features_full_df[['node', 'degree', 'in_degree', 'out_degree',\n",
    "                        'degree_centrality', 'in_degree_centrality', 'out_degree_centrality']],\n",
    "    left_on='target', right_on='node', how='left'\n",
    ").rename(columns={\n",
    "    'degree': 'target_degree', 'in_degree': 'target_in_degree', 'out_degree': 'target_out_degree',\n",
    "    'degree_centrality': 'target_degree_centrality', 'in_degree_centrality': 'target_in_degree_centrality',\n",
    "    'out_degree_centrality': 'target_out_degree_centrality'\n",
    "}).drop('node', axis=1, errors='ignore')\n",
    "\n",
    "# Preencher valores ausentes\n",
    "network_cols = [col for col in df_with_network.columns if col.startswith(('source_', 'target_')) and\n",
    "                ('degree' in col or 'centrality' in col)]\n",
    "df_with_network[network_cols] = df_with_network[network_cols].fillna(0)\n",
    "\n",
    "# Amostra para análise IV\n",
    "df_network_sample = df_with_network.sample(n=min(100000, len(df_with_network)), random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "344ae273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping Timestamp - too many unique values (14127)\n",
      "Warning: Skipping From Bank ID - too many unique values (5321)\n",
      "\n",
      "Warning: Skipping From Bank ID - too many unique values (5321)\n",
      "Warning: Skipping source - too many unique values (64297)\n",
      "Warning: Skipping To Bank ID - too many unique values (2655)\n",
      "Warning: Skipping source - too many unique values (64297)\n",
      "Warning: Skipping To Bank ID - too many unique values (2655)\n",
      "Warning: Skipping target - too many unique values (79250)\n",
      "Warning: Skipping Amount Received - too many unique values (81825)\n",
      "Warning: Skipping target - too many unique values (79250)\n",
      "Warning: Skipping Amount Received - too many unique values (81825)\n",
      "Warning: Skipping Amount Paid - too many unique values (82001)\n",
      "Warning: Skipping Amount Paid - too many unique values (82001)\n",
      "Warning: Skipping Bank Name - too many unique values (3773)\n",
      "Warning: Skipping Bank ID - too many unique values (5321)\n",
      "Warning: Skipping Bank Name - too many unique values (3773)\n",
      "Warning: Skipping Bank ID - too many unique values (5321)\n",
      "Warning: Skipping Account Number - too many unique values (64297)\n",
      "Warning: Skipping Account Number - too many unique values (64297)\n",
      "Warning: Skipping Entity ID - too many unique values (48368)\n",
      "Warning: Skipping Entity ID - too many unique values (48368)\n",
      "Warning: Skipping Entity Name - too many unique values (48368)\n",
      "Warning: Skipping Entity Name - too many unique values (48368)\n",
      "Warning: Skipping Bank Name_to - too many unique values (2044)\n",
      "Warning: Skipping Bank ID_to - too many unique values (2655)\n",
      "Warning: Skipping Bank Name_to - too many unique values (2044)\n",
      "Warning: Skipping Bank ID_to - too many unique values (2655)\n",
      "Warning: Skipping Account Number_to - too many unique values (79250)\n",
      "Warning: Skipping Account Number_to - too many unique values (79250)\n",
      "Warning: Skipping Entity ID_to - too many unique values (63719)\n",
      "Warning: Skipping Entity ID_to - too many unique values (63719)\n",
      "Warning: Skipping Entity Name_to - too many unique values (63719)\n",
      "Warning: Skipping Entity Name_to - too many unique values (63719)\n",
      "Warning: Skipping is_night_transaction - constant variable\n",
      "Warning: Skipping is_night_transaction - constant variable\n"
     ]
    }
   ],
   "source": [
    "# Calcular Information Value para features de rede\n",
    "numeric_network_cols = [col for col in network_cols if col in df_network_sample.select_dtypes(include=[np.number]).columns]\n",
    "\n",
    "iv_results = calculate_iv(df_network_sample, target_col='is_fraud', bins=10, max_iv=10.0)\n",
    "\n",
    "network_iv_results = iv_results[iv_results['variable'].isin(numeric_network_cols)].copy()\n",
    "\n",
    "network_iv_results.to_csv('../artifacts/network_features_iv_analysis.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4d66573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features temporais: 3 | Features de rede: 12\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-19 14:20:38,904 - INFO - Creating temporal features with pandas...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[temporal] Starting temporal feature creation with pandas...\n",
      "\n",
      "[temporal] Processing window=7 days...\n",
      "[temporal] Processing window=7 days...\n",
      "[temporal] Finished window=7d in 1457.67s\n",
      "[temporal] Processing window=30 days...\n",
      "[temporal] Finished window=7d in 1457.67s\n",
      "[temporal] Processing window=30 days...\n",
      "[temporal] Finished window=30d in 2178.65s\n",
      "[temporal] Finished window=30d in 2178.65s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-19 15:21:18,246 - INFO - Created 12 temporal features using pandas\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[temporal] Done. Total time: 3639.28s\n",
      "\n",
      "Dataset final: 5,078,336 linhas × 47 colunas\n",
      "Dataset final: 5,078,336 linhas × 47 colunas\n"
     ]
    }
   ],
   "source": [
    "# Decisão Final: Features baseadas em IV\n",
    "temporal_predictive = len(temporal_iv_results[temporal_iv_results['IV'] >= 0.02])\n",
    "network_predictive = len(network_iv_results[network_iv_results['IV'] >= 0.02])\n",
    "\n",
    "print(f\"Features temporais: {temporal_predictive} | Features de rede: {network_predictive}\")\n",
    "\n",
    "# Criar DataFrame final com features selecionadas\n",
    "df_final_features = df_clean.copy()\n",
    "\n",
    "if temporal_predictive > 0:\n",
    "    # Renomear colunas para formato esperado pela função\n",
    "    df_clean_renamed = df_clean.rename(columns={\n",
    "        'Timestamp': 'timestamp',\n",
    "        'From Account': 'source',\n",
    "        'Amount Paid': 'amount'\n",
    "    })\n",
    "    temporal_features_full = create_temporal_features(df_clean_renamed, windows=[7, 30])\n",
    "    temporal_cols = [col for col in temporal_features_full.columns if col.startswith('source_amount_') or col.startswith('hour') or col.startswith('day_of_week') or col.startswith('is_business_hours') or col.startswith('is_weekend')]\n",
    "    df_final_features = pd.merge(df_final_features, temporal_features_full[temporal_cols], left_index=True, right_index=True, how='left')\n",
    "\n",
    "if network_predictive > 0:\n",
    "    # Renomear coluna para merge\n",
    "    df_final_features_renamed = df_final_features.rename(columns={'From Account': 'source'})\n",
    "    df_final_features = df_final_features_renamed.merge(\n",
    "        network_features_full_df[['node', 'degree', 'in_degree', 'out_degree', 'degree_centrality', 'in_degree_centrality', 'out_degree_centrality']],\n",
    "        left_on='source', right_on='node', how='left'\n",
    "    ).rename(columns={\n",
    "        'degree': 'source_degree', 'in_degree': 'source_in_degree', 'out_degree': 'source_out_degree',\n",
    "        'degree_centrality': 'source_degree_centrality', 'in_degree_centrality': 'source_in_degree_centrality', 'out_degree_centrality': 'source_out_degree_centrality'\n",
    "    }).drop('node', axis=1, errors='ignore')\n",
    "\n",
    "print(f\"Dataset final: {df_final_features.shape[0]:,} linhas × {df_final_features.shape[1]} colunas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823bc127",
   "metadata": {},
   "source": [
    "## ▸ Features Categóricas\n",
    "\n",
    "Aplicamos encoding seguro para variáveis categóricas, garantindo que não haja data leakage através de informações futuras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1457b203",
   "metadata": {},
   "source": [
    "### Auditoria de Encoding Categórico e Prevenção de Data Leakage\n",
    "\n",
    "**Análise Sênior:** A função `encode_categorical_features` pode implementar *target encoding*, que usa a variável alvo (`is_fraud`) para criar as features. Aplicar este método em todo o dataset antes da separação treino/teste resulta em **data leakage**, inflando artificialmente a performance do modelo.\n",
    "\n",
    "**Decisão Estratégica:** Para garantir a robustez do modelo, o encoding de variáveis categóricas será integrado diretamente ao **pipeline de modelagem (notebook 03)**. Lá, o encoder será \"treinado\" (`.fit()`) apenas com os dados de treino e depois aplicado (`.transform()`) nos dados de treino e validação/teste separadamente. Esta célula será desativada ou simplificada para refletir essa decisão."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5174052",
   "metadata": {},
   "source": [
    "## ▸ Validação de Features\n",
    "\n",
    "Analisamos correlações, distribuição e possíveis problemas de leakage antes de criar o pipeline final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91336e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3209 pattern transactions from 370 laundering attempts\n",
      "\n",
      "Top 10 features por correlação:\n",
      "  is_fraud: 1.0000\n",
      "  is_weekend: 0.0162\n",
      "  same_bank_transfer: 0.0108\n",
      "  amount_cv_3: 0.0099\n",
      "  is_business_hours: 0.0092\n",
      "  fan_in_degree: 0.0088\n",
      "Top 10 features por correlação:\n",
      "  is_fraud: 1.0000\n",
      "  is_weekend: 0.0162\n",
      "  same_bank_transfer: 0.0108\n",
      "  amount_cv_3: 0.0099\n",
      "  is_business_hours: 0.0092\n",
      "  fan_in_degree: 0.0088\n",
      "  account_in_degree: 0.0086\n",
      "  Bank ID_to: 0.0057\n",
      "  to_bank: 0.0057\n",
      "  hour_x: 0.0055\n",
      "  account_in_degree: 0.0086\n",
      "  Bank ID_to: 0.0057\n",
      "  to_bank: 0.0057\n",
      "  hour_x: 0.0055\n",
      "Dataset salvo: 5,078,336 linhas × 75 colunas\n",
      "Dataset salvo: 5,078,336 linhas × 75 colunas\n"
     ]
    }
   ],
   "source": [
    "# Combinar features e criar patterns\n",
    "df_final = df_final_features.copy()\n",
    "\n",
    "# Mapear colunas para formato esperado\n",
    "column_mapping = {\n",
    "    'From Bank ID': 'from_bank',\n",
    "    'To Bank ID': 'to_bank',\n",
    "    'Amount Received': 'amount_received',\n",
    "    'Receiving Currency': 'receiving_currency',\n",
    "    'Payment Currency': 'payment_currency',\n",
    "    'Amount Paid': 'amount',\n",
    "    'From Account': 'from_account',\n",
    "    'To Account': 'to_account',\n",
    "    'Timestamp': 'timestamp'\n",
    "}\n",
    "df_final = df_final.rename(columns=column_mapping)\n",
    "\n",
    "# Features baseadas em patterns\n",
    "pattern_engineer = PatternFeatureEngineer()\n",
    "df_with_patterns = pattern_engineer.create_pattern_similarity_features(df_final.copy())\n",
    "\n",
    "# Top 10 features por correlação\n",
    "numeric_cols = df_with_patterns.select_dtypes(include=[np.number]).columns\n",
    "correlations = df_with_patterns[numeric_cols].corr()['is_fraud'].abs().sort_values(ascending=False)\n",
    "top_10 = correlations.head(10)\n",
    "\n",
    "print(\"Top 10 features por correlação:\")\n",
    "for feature, corr in top_10.items():\n",
    "    print(f\"  {feature}: {corr:.4f}\")\n",
    "\n",
    "# Salvar dataset\n",
    "output_dir = Path('../data/processed/')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "df_with_patterns.to_pickle(output_dir / 'features_with_patterns.pkl')\n",
    "\n",
    "print(f\"Dataset salvo: {df_with_patterns.shape[0]:,} linhas × {df_with_patterns.shape[1]} colunas\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

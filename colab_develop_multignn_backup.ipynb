{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34cd42ef",
   "metadata": {},
   "source": [
    "# üöÄ Multi-GNN AML Detection - Colab Production Ready\n",
    "\n",
    "Este notebook implementa **detec√ß√£o de lavagem de dinheiro usando Multi-GNN** seguindo o guia de otimiza√ß√£o completo.\n",
    "\n",
    "## üìã Checklist Pr√©-Execu√ß√£o\n",
    "\n",
    "- [ ] Conta Google (para acessar Colab)\n",
    "- [ ] Kaggle API key (`kaggle.json`) - [Como obter](https://www.kaggle.com/docs/api)\n",
    "- [ ] GPU habilitada no Colab (Runtime > Change runtime type > GPU)\n",
    "\n",
    "## üéØ Ordem de Execu√ß√£o (IMPORTANTE!)\n",
    "\n",
    "1. **Cell 1**: Verifica√ß√£o GPU\n",
    "2. **Cell 2**: Instala√ß√£o PyTorch Geometric\n",
    "3. **Cell 3**: Imports + Kaggle Setup\n",
    "4. **Cell 4**: Download Dados\n",
    "5. **Cell 5**: Configura√ß√£o\n",
    "6. **Cell 6**: Load Data\n",
    "7. **Cell 7**: Feature Engineering\n",
    "8. **Cell 8**: Graph Construction\n",
    "9. **Cell 9**: Model Definition\n",
    "10. **Cell 10**: Training Setup\n",
    "11. **Cell 11**: TREINAMENTO\n",
    "12. **Cell 12**: Evaluation\n",
    "13. **Cell 13**: Export\n",
    "\n",
    "**‚ö†Ô∏è Execute as c√©lulas nesta ordem espec√≠fica!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f034fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç SYSTEM VERIFICATION\n",
      "============================================================\n",
      "üêç Python: 3.9.23 | packaged by conda-forge | (main, Jun  4 2025, 17:49:16) [MSC v.1929 64 bit (AMD64)]\n",
      "‚ùå Not running on Google Colab\n",
      "‚ùå No GPU detected - This will be very slow!\n",
      "============================================================\n",
      "‚ùå No GPU detected - This will be very slow!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# üîç SYSTEM VERIFICATION\n",
    "\n",
    "# Verificar Python\n",
    "import sys\n",
    "\n",
    "# Verificar se est√° no Colab\n",
    "try:\n",
    "    import google.colab\n",
    "except ImportError:\n",
    "\n",
    "# Verificar GPU\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "else:\n",
    "\n",
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaaa5d3",
   "metadata": {},
   "source": [
    "## üîß Instala√ß√£o PyTorch Geometric\n",
    "\n",
    "Esta c√©lula instala automaticamente o PyTorch Geometric compat√≠vel com a vers√£o do CUDA detectada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391928b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ INSTALA√á√ÉO PYTORCH GEOMETRIC\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_pytorch_geometric():\n",
    "    \"\"\"Instala PyTorch Geometric compat√≠vel com CUDA.\"\"\"\n",
    "    try:\n",
    "        # Verificar CUDA\n",
    "        cuda_version = torch.version.cuda\n",
    "        if cuda_version:\n",
    "            cuda_short = cuda_version.replace(\".\", \"\")[:3]  # e.g., \"118\" for 11.8\n",
    "\n",
    "            # URL do wheel\n",
    "            wheel_url = f\"https://data.pyg.org/whl/torch-{torch.__version__}+cu{cuda_short}.html\"\n",
    "\n",
    "            # Instalar depend√™ncias PyG\n",
    "\n",
    "            packages = [\n",
    "                \"torch-scatter\",\n",
    "                \"torch-sparse\",\n",
    "                \"torch-cluster\",\n",
    "                \"torch-spline-conv\",\n",
    "                \"torch-geometric\"\n",
    "            ]\n",
    "\n",
    "            for package in packages:\n",
    "                cmd = f\"pip install {package} -f {wheel_url}\"\n",
    "                result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "\n",
    "                if result.returncode == 0:\n",
    "                else:\n",
    "                    return False\n",
    "\n",
    "            return True\n",
    "\n",
    "        else:\n",
    "            result = subprocess.run(\"pip install torch-geometric\", shell=True, capture_output=True, text=True)\n",
    "            return result.returncode == 0\n",
    "\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "# Executar instala√ß√£o\n",
    "success = install_pytorch_geometric()\n",
    "\n",
    "if success:\n",
    "    try:\n",
    "        import torch_geometric\n",
    "    except ImportError as e:\n",
    "        success = False\n",
    "\n",
    "if not success:\n",
    "\n",
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a0938e",
   "metadata": {},
   "source": [
    "## üìö Imports + Kaggle Setup\n",
    "\n",
    "Esta c√©lula importa todas as bibliotecas necess√°rias e configura a API do Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f081c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìö IMPORTING LIBRARIES\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GINConv, global_mean_pool\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from io import StringIO\n",
    "import zipfile\n",
    "from urllib.request import urlopen\n",
    "import warnings\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Configurar device\n",
    "\n",
    "\n",
    "# üîë KAGGLE API SETUP\n",
    "\n",
    "def setup_kaggle():\n",
    "    \"\"\"Configura a API do Kaggle.\"\"\"\n",
    "    try:\n",
    "        # Criar diret√≥rio .kaggle\n",
    "        kaggle_dir = Path(\"/root/.kaggle\")\n",
    "\n",
    "        # Verificar se kaggle.json j√° existe\n",
    "        kaggle_file = kaggle_dir / \"kaggle.json\"\n",
    "        if kaggle_file.exists():\n",
    "        else:\n",
    "\n",
    "            from google.colab import files\n",
    "            uploaded = files.upload()\n",
    "\n",
    "            if 'kaggle.json' in uploaded:\n",
    "                # Mover arquivo\n",
    "\n",
    "                # Definir permiss√µes\n",
    "                kaggle_file.chmod(0o600)\n",
    "            else:\n",
    "                return False\n",
    "\n",
    "        # Testar API\n",
    "        result = subprocess.run([\"kaggle\", \"competitions\", \"list\", \"--csv\"],\n",
    "                              capture_output=True, text=True, timeout=30)\n",
    "\n",
    "        if result.returncode == 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "# Configurar Kaggle\n",
    "kaggle_success = setup_kaggle()\n",
    "\n",
    "if kaggle_success:\n",
    "else:\n",
    "\n",
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cead72",
   "metadata": {},
   "source": [
    "## üì• Download Dados\n",
    "\n",
    "Esta c√©lula baixa o dataset AML do Kaggle ou gera dados sint√©ticos se o Kaggle falhar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ffd783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì• DOWNLOADING IBM AML DATASET\n",
    "\n",
    "# Configurar caminhos\n",
    "data_dir = Path(\"/content/aml_data\")\n",
    "raw_dir = data_dir / \"raw\"\n",
    "processed_dir = data_dir / \"processed\"\n",
    "\n",
    "for dir_path in [data_dir, raw_dir, processed_dir]:\n",
    "\n",
    "\n",
    "def download_kaggle_dataset():\n",
    "    \"\"\"Download do dataset via Kaggle API.\"\"\"\n",
    "    try:\n",
    "\n",
    "        # Comando kaggle\n",
    "        cmd = \"kaggle datasets download ealtman2019/ibm-transactions-for-anti-money-laundering-aml -p /content/aml_data/raw --unzip\"\n",
    "        result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=600)\n",
    "\n",
    "        if result.returncode == 0:\n",
    "\n",
    "            # Listar arquivos baixados\n",
    "            downloaded_files = list(raw_dir.glob(\"*.csv\"))\n",
    "            for file in downloaded_files:\n",
    "                size_mb = file.stat().st_size / (1024 * 1024)\n",
    "\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "def generate_synthetic_data(sample_size=None):\n",
    "    \"\"\"Gera dados sint√©ticos realistas para AML.\"\"\"\n",
    "\n",
    "    np.random.seed(42)\n",
    "\n",
    "    n_transactions = sample_size or 50000\n",
    "    n_accounts = 2000\n",
    "\n",
    "    # Gerar dados transacionais realistas\n",
    "    data = {\n",
    "        'Timestamp': pd.date_range('2020-01-01', periods=n_transactions, freq='1min'),\n",
    "        'From Bank': np.random.randint(1, 11, n_transactions),\n",
    "        'From Account': np.random.randint(100000, 999999, n_transactions),\n",
    "        'To Bank': np.random.randint(1, 11, n_transactions),\n",
    "        'To Account': np.random.randint(100000, 999999, n_transactions),\n",
    "        'Amount Received': np.random.exponential(1000, n_transactions),\n",
    "        'Receiving Currency': np.random.choice(['USD', 'EUR', 'GBP', 'JPY'], n_transactions),\n",
    "        'Amount Paid': np.random.exponential(1000, n_transactions),\n",
    "        'Payment Currency': np.random.choice(['USD', 'EUR', 'GBP', 'JPY'], n_transactions),\n",
    "        'Payment Format': np.random.choice(['ACH', 'Wire', 'Check', 'Cash'], n_transactions),\n",
    "        'Is Laundering': np.random.choice([0, 1], n_transactions, p=[0.95, 0.05])\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Salvar dados\n",
    "    data_file = raw_dir / \"HI-Small_Trans.csv\"\n",
    "    df.to_csv(data_file, index=False)\n",
    "\n",
    "\n",
    "    return True\n",
    "\n",
    "# Tentar download do Kaggle primeiro\n",
    "if kaggle_success:\n",
    "    download_success = download_kaggle_dataset()\n",
    "else:\n",
    "    download_success = False\n",
    "\n",
    "# Fallback para dados sint√©ticos\n",
    "if not download_success:\n",
    "    generate_synthetic_data(sample_size=50000)\n",
    "\n",
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001ad76d",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configura√ß√£o\n",
    "\n",
    "Esta c√©lula define todos os hiperpar√¢metros e configura√ß√µes do experimento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba97740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è CONFIGURATION\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Configura√ß√µes do experimento Multi-GNN AML.\"\"\"\n",
    "\n",
    "    # PARA TESTE R√ÅPIDO (5-10 min total):\n",
    "    SAMPLE_SIZE = 10000  # Apenas 10k transa√ß√µes\n",
    "    EPOCHS = 20          # Menos √©pocas\n",
    "\n",
    "    # PARA PRODU√á√ÉO (30-60 min total):\n",
    "    # SAMPLE_SIZE = None   # Dataset completo\n",
    "    # EPOCHS = 100         # Treinamento completo\n",
    "\n",
    "    # Arquitetura (escolha uma):\n",
    "    GNN_TYPE = 'GIN'         # ‚≠ê RECOMENDADO - melhor performance\n",
    "    # GNN_TYPE = 'GAT'       # Interpretabilidade\n",
    "    # GNN_TYPE = 'GraphSAGE' # Escalabilidade\n",
    "    # GNN_TYPE = 'GCN'       # Baseline r√°pido\n",
    "\n",
    "    # Hiperpar√¢metros da rede\n",
    "    HIDDEN_CHANNELS = 128\n",
    "    NUM_LAYERS = 3\n",
    "    DROPOUT = 0.3\n",
    "\n",
    "    # Treinamento\n",
    "    LEARNING_RATE = 0.001\n",
    "    WEIGHT_DECAY = 1e-4\n",
    "    BATCH_SIZE = 32\n",
    "\n",
    "    # Early stopping\n",
    "    PATIENCE = 15\n",
    "    MIN_DELTA = 0.001\n",
    "\n",
    "    # Dados\n",
    "    TEST_SIZE = 0.2\n",
    "    VAL_SIZE = 0.1\n",
    "    RANDOM_STATE = 42\n",
    "\n",
    "# Instanciar configura√ß√£o\n",
    "config = Config()\n",
    "\n",
    "\n",
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40f2256",
   "metadata": {},
   "source": [
    "## üìä Load Data\n",
    "\n",
    "Esta c√©lula carrega e faz uma limpeza b√°sica dos dados transacionais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fbf5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä DATA LOADING & BASIC CLEANING\n",
    "\n",
    "# Carregar dados\n",
    "data_file = raw_dir / \"HI-Small_Trans.csv\"\n",
    "\n",
    "if not data_file.exists():\n",
    "else:\n",
    "    # Carregar dados\n",
    "    df = pd.read_csv(data_file)\n",
    "\n",
    "\n",
    "    # Mostrar colunas\n",
    "    for i, col in enumerate(df.columns, 1):\n",
    "\n",
    "\n",
    "    # Amostrar dados se necess√°rio\n",
    "    if config.SAMPLE_SIZE and len(df) > config.SAMPLE_SIZE:\n",
    "        df = df.sample(n=config.SAMPLE_SIZE, random_state=config.RANDOM_STATE)\n",
    "\n",
    "    # Limpeza b√°sica\n",
    "    original_size = len(df)\n",
    "\n",
    "    # Remover valores nulos\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Remover duplicatas\n",
    "    original_size = len(df)\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    # Verificar coluna target\n",
    "    target_col = 'Is Laundering'\n",
    "    if target_col in df.columns:\n",
    "\n",
    "        # Distribui√ß√£o da classe\n",
    "        class_counts = df[target_col].value_counts().sort_index()\n",
    "        for class_val, count in class_counts.items():\n",
    "            percentage = count / len(df) * 100\n",
    "\n",
    "        # Calcular pos_weight para loss function\n",
    "        pos_weight = (len(df) - df[target_col].sum()) / df[target_col].sum()\n",
    "\n",
    "        # Salvar pos_weight para uso posterior\n",
    "        config.POS_WEIGHT = pos_weight\n",
    "\n",
    "    else:\n",
    "\n",
    "    # Salvar dados limpos\n",
    "    clean_file = processed_dir / \"transactions_clean.csv\"\n",
    "    df.to_csv(clean_file, index=False)\n",
    "\n",
    "\n",
    "    # Mostrar primeiras linhas\n",
    "\n",
    "    # Estat√≠sticas b√°sicas\n",
    "\n",
    "    # Salvar DataFrame global\n",
    "    global df_clean\n",
    "    df_clean = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b07718",
   "metadata": {},
   "source": [
    "## üîß Feature Engineering\n",
    "\n",
    "Esta c√©lula cria features avan√ßadas para detec√ß√£o de AML, incluindo features de rede usando NetworkX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6247d533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß FEATURE ENGINEERING\n",
    "\n",
    "try:\n",
    "    # Verificar se dados foram carregados\n",
    "    if 'df_clean' not in globals():\n",
    "    else:\n",
    "        df = df_clean.copy()\n",
    "\n",
    "        # 1. Processamento de timestamps\n",
    "        df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "        df['timestamp_seconds'] = (df['Timestamp'] - df['Timestamp'].min()).dt.total_seconds()\n",
    "\n",
    "        # 2. Features temporais\n",
    "        df['hour'] = df['Timestamp'].dt.hour\n",
    "        df['day_of_week'] = df['Timestamp'].dt.dayofweek\n",
    "        df['month'] = df['Timestamp'].dt.month\n",
    "\n",
    "        # 3. Features de transa√ß√£o\n",
    "        df['amount_ratio'] = df['Amount Received'] / (df['Amount Paid'] + 1e-6)\n",
    "        df['amount_diff'] = abs(df['Amount Received'] - df['Amount Paid'])\n",
    "        df['amount_log'] = np.log1p(df['Amount Paid'])\n",
    "\n",
    "        # 4. Encoding categ√≥rico\n",
    "        categorical_cols = ['Receiving Currency', 'Payment Currency', 'Payment Format']\n",
    "        label_encoders = {}\n",
    "\n",
    "        for col in categorical_cols:\n",
    "            if col in df.columns:\n",
    "                le = LabelEncoder()\n",
    "                df[f'{col}_encoded'] = le.fit_transform(df[col])\n",
    "                label_encoders[col] = le\n",
    "\n",
    "        # 5. Features de frequ√™ncia por conta\n",
    "        # Frequ√™ncia hor√°ria por conta de origem\n",
    "        df['freq_hour'] = df.groupby(['From Account', 'hour']).cumcount()\n",
    "\n",
    "        # Frequ√™ncia di√°ria por conta de origem\n",
    "        df['freq_day'] = df.groupby(['From Account', df['Timestamp'].dt.date]).cumcount()\n",
    "\n",
    "        # N√∫mero total de transa√ß√µes por conta\n",
    "        from_freq = df['From Account'].value_counts()\n",
    "        to_freq = df['To Account'].value_counts()\n",
    "\n",
    "        df['from_account_degree'] = df['From Account'].map(from_freq)\n",
    "        df['to_account_degree'] = df['To Account'].map(to_freq)\n",
    "\n",
    "        # 6. Features de tempo\n",
    "        # Diferen√ßa de tempo entre transa√ß√µes consecutivas por conta\n",
    "        df = df.sort_values(['From Account', 'Timestamp'])\n",
    "        df['time_diff'] = df.groupby('From Account')['timestamp_seconds'].diff().fillna(0)\n",
    "        df['time_diff_log'] = np.log1p(df['time_diff'])\n",
    "\n",
    "        # 7. Features de comportamento (m√©dias m√≥veis)\n",
    "        # M√©dia m√≥vel de valores por conta\n",
    "        df['rolling_mean_amount'] = df.groupby('From Account')['Amount Paid'].rolling(5, min_periods=1).mean().reset_index(0, drop=True)\n",
    "\n",
    "        # Desvio padr√£o m√≥vel\n",
    "        df['rolling_std_amount'] = df.groupby('From Account')['Amount Paid'].rolling(5, min_periods=1).std().reset_index(0, drop=True).fillna(0)\n",
    "\n",
    "        # M√©dia m√≥vel de frequ√™ncia\n",
    "        df['rolling_mean_freq'] = df.groupby('From Account')['freq_hour'].rolling(5, min_periods=1).mean().reset_index(0, drop=True)\n",
    "\n",
    "        # 8. Features de rede usando NetworkX\n",
    "        try:\n",
    "            # Criar grafo direcionado\n",
    "            G = nx.DiGraph()\n",
    "\n",
    "            # Adicionar n√≥s (contas √∫nicas)\n",
    "            all_accounts = set(df['From Account'].unique()) | set(df['To Account'].unique())\n",
    "            G.add_nodes_from(all_accounts)\n",
    "\n",
    "            # Adicionar arestas (transa√ß√µes)\n",
    "            edges = list(zip(df['From Account'], df['To Account']))\n",
    "            G.add_edges_from(edges)\n",
    "\n",
    "\n",
    "            # Calcular PageRank\n",
    "            pagerank = nx.pagerank(G, alpha=0.85)\n",
    "            df['pagerank_from'] = df['From Account'].map(pagerank).fillna(0)\n",
    "            df['pagerank_to'] = df['To Account'].map(pagerank).fillna(0)\n",
    "\n",
    "            # Calcular Betweenness Centrality (amostra para performance)\n",
    "            if G.number_of_nodes() > 1000:\n",
    "                # Para grafos grandes, calcular apenas para uma amostra\n",
    "                sample_nodes = list(G.nodes())[:1000]\n",
    "                betweenness = nx.betweenness_centrality_subset(G, sources=sample_nodes, targets=sample_nodes)\n",
    "            else:\n",
    "                betweenness = nx.betweenness_centrality(G)\n",
    "\n",
    "            df['betweenness_from'] = df['From Account'].map(betweenness).fillna(0)\n",
    "            df['betweenness_to'] = df['To Account'].map(betweenness).fillna(0)\n",
    "\n",
    "            # Calcular Clustering Coefficient\n",
    "            clustering = nx.clustering(G.to_undirected())\n",
    "            df['clustering_from'] = df['From Account'].map(clustering).fillna(0)\n",
    "            df['clustering_to'] = df['To Account'].map(clustering).fillna(0)\n",
    "\n",
    "            # Calcular Degree Centrality\n",
    "            degree_centrality = nx.degree_centrality(G)\n",
    "            df['degree_centrality_from'] = df['From Account'].map(degree_centrality).fillna(0)\n",
    "            df['degree_centrality_to'] = df['To Account'].map(degree_centrality).fillna(0)\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            # Adicionar features b√°sicas de rede como fallback\n",
    "            df['pagerank_from'] = 0.0\n",
    "            df['pagerank_to'] = 0.0\n",
    "            df['betweenness_from'] = 0.0\n",
    "            df['betweenness_to'] = 0.0\n",
    "            df['clustering_from'] = 0.0\n",
    "            df['clustering_to'] = 0.0\n",
    "            df['degree_centrality_from'] = df['from_account_degree'] / df['from_account_degree'].max()\n",
    "            df['degree_centrality_to'] = df['to_account_degree'] / df['to_account_degree'].max()\n",
    "\n",
    "        # 9. Normaliza√ß√£o\n",
    "        numeric_cols = [\n",
    "            'Amount Received', 'Amount Paid', 'timestamp_seconds', 'amount_ratio',\n",
    "            'amount_diff', 'amount_log', 'time_diff', 'time_diff_log', 'freq_hour', 'freq_day',\n",
    "            'from_account_degree', 'to_account_degree', 'rolling_mean_amount', 'rolling_std_amount',\n",
    "            'rolling_mean_freq', 'pagerank_from', 'pagerank_to', 'betweenness_from', 'betweenness_to',\n",
    "            'clustering_from', 'clustering_to', 'degree_centrality_from', 'degree_centrality_to'\n",
    "        ]\n",
    "\n",
    "        # Filtrar colunas que existem\n",
    "        numeric_cols = [col for col in numeric_cols if col in df.columns]\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "\n",
    "        # Salvar dados processados\n",
    "        processed_file = processed_dir / \"transactions_processed.csv\"\n",
    "        df.to_csv(processed_file, index=False)\n",
    "\n",
    "\n",
    "        # Estat√≠sticas finais\n",
    "\n",
    "        # Salvar objetos globais\n",
    "        global df_processed, feature_scaler, categorical_encoders\n",
    "        df_processed = df\n",
    "        feature_scaler = scaler\n",
    "        categorical_encoders = label_encoders\n",
    "\n",
    "\n",
    "except Exception as e:\n",
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba8ea76",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Graph Construction\n",
    "\n",
    "Esta c√©lula constr√≥i o grafo para o Multi-GNN usando PyTorch Geometric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388817a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üèóÔ∏è GRAPH CONSTRUCTION\n",
    "\n",
    "try:\n",
    "    if 'df_processed' not in globals():\n",
    "    else:\n",
    "        df = df_processed\n",
    "\n",
    "        # 1. Criar mapeamento de n√≥s (contas)\n",
    "        all_accounts = pd.concat([df['From Account'], df['To Account']]).unique()\n",
    "        account_to_node = {acc: i for i, acc in enumerate(all_accounts)}\n",
    "\n",
    "        # 2. Criar arestas direcionadas\n",
    "        edges_from = df['From Account'].map(account_to_node).values\n",
    "        edges_to = df['To Account'].map(account_to_node).values\n",
    "        edge_index = torch.tensor([edges_from, edges_to], dtype=torch.long)\n",
    "\n",
    "        # 3. Features dos n√≥s (contas)\n",
    "        node_features = []\n",
    "\n",
    "        for account in all_accounts:\n",
    "            # Agregar features por conta\n",
    "            account_data = df[df['From Account'] == account]\n",
    "            if len(account_data) == 0:\n",
    "                account_data = df[df['To Account'] == account]\n",
    "\n",
    "            if len(account_data) > 0:\n",
    "                # Agregar estat√≠sticas da conta\n",
    "                features = [\n",
    "                    account_data['Amount Paid'].mean(),  # Volume m√©dio\n",
    "                    account_data['Amount Received'].mean(),  # Recebimento m√©dio\n",
    "                    len(account_data),  # N√∫mero de transa√ß√µes\n",
    "                    account_data['Is Laundering'].mean(),  # Risco m√©dio\n",
    "                    account_data['time_diff'].mean(),  # Tempo m√©dio entre transa√ß√µes\n",
    "                    account_data['pagerank_from'].iloc[0] if len(account_data) > 0 else 0,  # PageRank\n",
    "                    account_data['betweenness_from'].iloc[0] if len(account_data) > 0 else 0,  # Betweenness\n",
    "                    account_data['clustering_from'].iloc[0] if len(account_data) > 0 else 0,  # Clustering\n",
    "                    account_data['degree_centrality_from'].iloc[0] if len(account_data) > 0 else 0,  # Degree centrality\n",
    "                ]\n",
    "            else:\n",
    "                features = [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "            node_features.append(features)\n",
    "\n",
    "        x = torch.tensor(node_features, dtype=torch.float)\n",
    "\n",
    "        # 4. Labels das arestas (transa√ß√µes)\n",
    "        y = torch.tensor(df['Is Laundering'].values, dtype=torch.long)\n",
    "\n",
    "        # 5. Features das arestas (transa√ß√µes)\n",
    "        edge_features = []\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            edge_feat = [\n",
    "                row['Amount Paid'],\n",
    "                row['Amount Received'],\n",
    "                row['amount_ratio'],\n",
    "                row['amount_diff'],\n",
    "                row['amount_log'],\n",
    "                row['time_diff'],\n",
    "                row['time_diff_log'],\n",
    "                row['freq_hour'],\n",
    "                row['freq_day'],\n",
    "                row['from_account_degree'],\n",
    "                row['to_account_degree'],\n",
    "                row['rolling_mean_amount'],\n",
    "                row['rolling_std_amount'],\n",
    "                row['rolling_mean_freq'],\n",
    "                row['pagerank_from'],\n",
    "                row['pagerank_to'],\n",
    "                row['betweenness_from'],\n",
    "                row['betweenness_to'],\n",
    "                row['clustering_from'],\n",
    "                row['clustering_to'],\n",
    "                row['degree_centrality_from'],\n",
    "                row['degree_centrality_to'],\n",
    "                # Features codificadas\n",
    "                row.get('Receiving Currency_encoded', 0),\n",
    "                row.get('Payment Currency_encoded', 0),\n",
    "                row.get('Payment Format_encoded', 0),\n",
    "                # Features temporais\n",
    "                row['hour'] / 23.0,  # Normalizar\n",
    "                row['day_of_week'] / 6.0,  # Normalizar\n",
    "                row['month'] / 12.0,  # Normalizar\n",
    "            ]\n",
    "            edge_features.append(edge_feat)\n",
    "\n",
    "        edge_attr = torch.tensor(edge_features, dtype=torch.float)\n",
    "\n",
    "        # 6. Criar objeto Data do PyTorch Geometric\n",
    "        graph_data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)\n",
    "\n",
    "\n",
    "        # Salvar grafo\n",
    "        models_dir = Path(\"/content/models\")\n",
    "        graph_file = models_dir / \"graph_data.pt\"\n",
    "        torch.save(graph_data, graph_file)\n",
    "\n",
    "        # Salvar mapeamento de contas\n",
    "        account_mapping_file = models_dir / \"account_mapping.json\"\n",
    "        with open(account_mapping_file, 'w') as f:\n",
    "            # Converter chaves para string (contas podem ser n√∫meros grandes)\n",
    "            json.dump({str(k): v for k, v in account_to_node.items()}, f)\n",
    "\n",
    "\n",
    "        # Salvar objetos globais\n",
    "        global pyg_graph_data, node_mapping\n",
    "        pyg_graph_data = graph_data\n",
    "        node_mapping = account_to_node\n",
    "\n",
    "except Exception as e:\n",
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb096525",
   "metadata": {},
   "source": [
    "## ü§ñ Model Definition\n",
    "\n",
    "Esta c√©lula define a arquitetura do modelo GNN para classifica√ß√£o de arestas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15085c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ü§ñ MODEL ARCHITECTURE\n",
    "\n",
    "class EdgeGINModel(torch.nn.Module):\n",
    "    \"\"\"Modelo GIN para classifica√ß√£o de arestas (transa√ß√µes) em AML.\"\"\"\n",
    "\n",
    "    def __init__(self, num_node_features, num_edge_features, hidden_channels=128, num_classes=2):\n",
    "        super(EdgeGINModel, self).__init__()\n",
    "\n",
    "        self.num_node_features = num_node_features\n",
    "        self.num_edge_features = num_edge_features\n",
    "        self.hidden_channels = hidden_channels\n",
    "\n",
    "        # Encoder de arestas\n",
    "        self.edge_encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(num_edge_features, hidden_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(config.DROPOUT),\n",
    "            torch.nn.Linear(hidden_channels, hidden_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(config.DROPOUT)\n",
    "        )\n",
    "\n",
    "        # Camadas GIN para n√≥s\n",
    "        self.conv1 = GINConv(\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.Linear(num_node_features, hidden_channels),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(hidden_channels, hidden_channels)\n",
    "            )\n",
    "        )\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "\n",
    "        self.conv2 = GINConv(\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.Linear(hidden_channels, hidden_channels),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(hidden_channels, hidden_channels)\n",
    "            )\n",
    "        )\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "\n",
    "        if config.NUM_LAYERS >= 3:\n",
    "            self.conv3 = GINConv(\n",
    "                torch.nn.Sequential(\n",
    "                    torch.nn.Linear(hidden_channels, hidden_channels),\n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.Linear(hidden_channels, hidden_channels)\n",
    "                )\n",
    "            )\n",
    "            self.bn3 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "\n",
    "        # Classificador de arestas\n",
    "        self.edge_classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_channels * 2 + hidden_channels, hidden_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(config.DROPOUT),\n",
    "            torch.nn.Linear(hidden_channels, hidden_channels // 2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(config.DROPOUT),\n",
    "            torch.nn.Linear(hidden_channels // 2, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # Codificar features das arestas\n",
    "        edge_features = self.edge_encoder(edge_attr)\n",
    "\n",
    "        # Camadas GIN\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=config.DROPOUT, training=self.training)\n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=config.DROPOUT, training=self.training)\n",
    "\n",
    "        if config.NUM_LAYERS >= 3:\n",
    "            x = self.conv3(x, edge_index)\n",
    "            x = self.bn3(x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=config.DROPOUT, training=self.training)\n",
    "\n",
    "        # Para cada aresta, concatenar features dos n√≥s de origem/destino + features da aresta\n",
    "        row, col = edge_index\n",
    "        edge_embeddings = torch.cat([x[row], x[col], edge_features], dim=1)\n",
    "\n",
    "        # Classificar arestas\n",
    "        out = self.edge_classifier(edge_embeddings)\n",
    "        return out\n",
    "\n",
    "# Instanciar modelo\n",
    "try:\n",
    "    if 'pyg_graph_data' not in globals():\n",
    "    else:\n",
    "        model = EdgeGINModel(\n",
    "            num_node_features=pyg_graph_data.x.shape[1],\n",
    "            num_edge_features=pyg_graph_data.edge_attr.shape[1],\n",
    "            hidden_channels=config.HIDDEN_CHANNELS,\n",
    "            num_classes=2\n",
    "        ).to(device)\n",
    "\n",
    "        # Contar par√¢metros\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "\n",
    "        # Salvar modelo global\n",
    "        global gnn_model\n",
    "        gnn_model = model\n",
    "\n",
    "\n",
    "except Exception as e:\n",
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b884b5",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Training Setup\n",
    "\n",
    "Esta c√©lula configura otimizador, loss function, scheduler e early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b47ee25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è TRAINING SETUP\n",
    "\n",
    "try:\n",
    "    if 'gnn_model' not in globals():\n",
    "    else:\n",
    "        # Dividir dados em treino/val/test\n",
    "\n",
    "        # Usar train_test_split estratificado\n",
    "        from sklearn.model_selection import train_test_split\n",
    "\n",
    "        # √çndices das arestas\n",
    "        edge_indices = np.arange(len(pyg_graph_data.y))\n",
    "\n",
    "        # Split estratificado\n",
    "        train_val_idx, test_idx = train_test_split(\n",
    "            edge_indices,\n",
    "            test_size=config.TEST_SIZE,\n",
    "            stratify=pyg_graph_data.y.numpy(),\n",
    "            random_state=config.RANDOM_STATE\n",
    "        )\n",
    "\n",
    "        train_idx, val_idx = train_test_split(\n",
    "            train_val_idx,\n",
    "            test_size=config.VAL_SIZE / (1 - config.TEST_SIZE),\n",
    "            stratify=pyg_graph_data.y.numpy()[train_val_idx],\n",
    "            random_state=config.RANDOM_STATE\n",
    "        )\n",
    "\n",
    "\n",
    "        # Otimizador\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            gnn_model.parameters(),\n",
    "            lr=config.LEARNING_RATE,\n",
    "            weight_decay=config.WEIGHT_DECAY\n",
    "        )\n",
    "\n",
    "        # Loss function com pesos para classe desbalanceada\n",
    "        pos_weight = torch.tensor([config.POS_WEIGHT], dtype=torch.float).to(device)\n",
    "        criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "        # Scheduler\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='max', factor=0.5, patience=10,\n",
    "            min_lr=1e-6, verbose=True\n",
    "        )\n",
    "\n",
    "        # Early stopping\n",
    "        class EarlyStopping:\n",
    "            def __init__(self, patience=15, min_delta=0.001):\n",
    "                self.patience = patience\n",
    "                self.min_delta = min_delta\n",
    "                self.counter = 0\n",
    "                self.best_score = None\n",
    "                self.early_stop = False\n",
    "\n",
    "            def __call__(self, val_score):\n",
    "                if self.best_score is None:\n",
    "                    self.best_score = val_score\n",
    "                elif val_score < self.best_score + self.min_delta:\n",
    "                    self.counter += 1\n",
    "                    if self.counter >= self.patience:\n",
    "                        self.early_stop = True\n",
    "                else:\n",
    "                    self.best_score = val_score\n",
    "                    self.counter = 0\n",
    "\n",
    "        early_stopping = EarlyStopping(patience=config.PATIENCE, min_delta=config.MIN_DELTA)\n",
    "\n",
    "        # Salvar objetos globais\n",
    "        global train_indices, val_indices, test_indices, gnn_optimizer, gnn_criterion, gnn_scheduler, gnn_early_stopping\n",
    "        train_indices = train_idx\n",
    "        val_indices = val_idx\n",
    "        test_indices = test_idx\n",
    "        gnn_optimizer = optimizer\n",
    "        gnn_criterion = criterion\n",
    "        gnn_scheduler = scheduler\n",
    "        gnn_early_stopping = early_stopping\n",
    "\n",
    "\n",
    "except Exception as e:\n",
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fc9760",
   "metadata": {},
   "source": [
    "## üöÄ TREINAMENTO\n",
    "\n",
    "Esta c√©lula executa o treinamento completo do Multi-GNN com monitoramento de m√©tricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b35f858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ TRAINING\n",
    "\n",
    "def train_epoch(model, optimizer, criterion, data, train_idx):\n",
    "    \"\"\"Treina por uma √©poca.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    # Forward pass apenas para arestas de treino\n",
    "    out = model(data.x, data.edge_index, data.edge_attr)\n",
    "\n",
    "    # Loss apenas para arestas de treino\n",
    "    loss = criterion(out[train_idx], data.y[train_idx].float().unsqueeze(1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "def evaluate(model, data, idx):\n",
    "    \"\"\"Avalia o modelo.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.edge_index, data.edge_attr)\n",
    "        pred = torch.sigmoid(out[idx]).squeeze()\n",
    "        pred_binary = (pred > 0.5).float()\n",
    "\n",
    "        labels = data.y[idx].float()\n",
    "\n",
    "        # M√©tricas\n",
    "        accuracy = (pred_binary == labels).float().mean().item()\n",
    "\n",
    "        # F1 Score\n",
    "        tp = ((pred_binary == 1) & (labels == 1)).sum().item()\n",
    "        fp = ((pred_binary == 1) & (labels == 0)).sum().item()\n",
    "        fn = ((pred_binary == 0) & (labels == 1)).sum().item()\n",
    "\n",
    "        precision = tp / (tp + fp + 1e-6)\n",
    "        recall = tp / (tp + fn + 1e-6)\n",
    "        f1 = 2 * (precision * recall) / (precision + recall + 1e-6)\n",
    "\n",
    "        # AUC\n",
    "        try:\n",
    "            from sklearn.metrics import roc_auc_score\n",
    "        except:\n",
    "            auc = 0.5\n",
    "\n",
    "    return accuracy, f1, auc\n",
    "\n",
    "try:\n",
    "    if 'gnn_model' not in globals():\n",
    "    else:\n",
    "\n",
    "        # Hist√≥rico de treinamento\n",
    "        history = {\n",
    "            'epoch': [], 'train_loss': [], 'train_acc': [], 'train_f1': [], 'train_auc': [],\n",
    "            'val_acc': [], 'val_f1': [], 'val_auc': [], 'lr': []\n",
    "        }\n",
    "\n",
    "        best_val_f1 = 0\n",
    "        best_model_state = None\n",
    "\n",
    "\n",
    "        for epoch in range(config.EPOCHS):\n",
    "            # Treinar\n",
    "            train_loss = train_epoch(gnn_model, gnn_optimizer, gnn_criterion, pyg_graph_data, train_indices)\n",
    "\n",
    "            # Avaliar treino\n",
    "            train_acc, train_f1, train_auc = evaluate(gnn_model, pyg_graph_data, train_indices)\n",
    "\n",
    "            # Avaliar valida√ß√£o\n",
    "            val_acc, val_f1, val_auc = evaluate(gnn_model, pyg_graph_data, val_indices)\n",
    "\n",
    "            # Learning rate atual\n",
    "            current_lr = gnn_optimizer.param_groups[0]['lr']\n",
    "\n",
    "            # Salvar melhores pesos\n",
    "            if val_f1 > best_val_f1:\n",
    "                best_val_f1 = val_f1\n",
    "                best_model_state = gnn_model.state_dict().copy()\n",
    "\n",
    "            # Scheduler step\n",
    "            gnn_scheduler.step(val_f1)\n",
    "\n",
    "            # Early stopping\n",
    "            gnn_early_stopping(val_f1)\n",
    "\n",
    "            # Logging\n",
    "\n",
    "            # Salvar hist√≥rico\n",
    "            history['epoch'].append(epoch+1)\n",
    "            history['train_loss'].append(train_loss)\n",
    "            history['train_acc'].append(train_acc)\n",
    "            history['train_f1'].append(train_f1)\n",
    "            history['train_auc'].append(train_auc)\n",
    "            history['val_acc'].append(val_acc)\n",
    "            history['val_f1'].append(val_f1)\n",
    "            history['val_auc'].append(val_auc)\n",
    "            history['lr'].append(current_lr)\n",
    "\n",
    "            # Early stopping\n",
    "            if gnn_early_stopping.early_stop:\n",
    "                break\n",
    "\n",
    "        # Carregar melhores pesos\n",
    "        if best_model_state:\n",
    "            gnn_model.load_state_dict(best_model_state)\n",
    "\n",
    "        # Salvar modelo\n",
    "        models_dir = Path(\"/content/models\")\n",
    "        model_file = models_dir / f\"{config.GNN_TYPE}_best_model.pth\"\n",
    "        torch.save({\n",
    "            'model_state_dict': gnn_model.state_dict(),\n",
    "            'config': config.__dict__,\n",
    "            'history': history,\n",
    "            'best_val_f1': best_val_f1\n",
    "        }, model_file)\n",
    "\n",
    "\n",
    "        # Salvar hist√≥rico global\n",
    "        global training_history\n",
    "        training_history = history\n",
    "\n",
    "\n",
    "except Exception as e:\n",
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269502ea",
   "metadata": {},
   "source": [
    "## üìä Evaluation\n",
    "\n",
    "Esta c√©lula avalia o modelo no conjunto de teste e gera m√©tricas finais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356efd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä EVALUATION\n",
    "\n",
    "try:\n",
    "    if 'gnn_model' not in globals():\n",
    "    else:\n",
    "        # Avaliar no conjunto de teste\n",
    "        test_acc, test_f1, test_auc = evaluate(gnn_model, pyg_graph_data, test_indices)\n",
    "\n",
    "\n",
    "        # Classification report detalhado\n",
    "        gnn_model.eval()\n",
    "        with torch.no_grad():\n",
    "            out = gnn_model(pyg_graph_data.x, pyg_graph_data.edge_index, pyg_graph_data.edge_attr)\n",
    "            test_pred = torch.sigmoid(out[test_indices]).squeeze()\n",
    "\n",
    "üìã Classification Report:\")\n",
    "\n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(test_labels, test_pred_binary)\n",
    "\n",
    "        # Salvar resultados\n",
    "        results_dir = Path(\"/content/results\")\n",
    "\n",
    "        # Salvar m√©tricas\n",
    "        metrics = {\n",
    "            'model': config.GNN_TYPE,\n",
    "            'dataset': 'IBM AML (Kaggle or Synthetic)',\n",
    "            'num_transactions': len(pyg_graph_data.y),\n",
    "            'num_nodes': pyg_graph_data.num_nodes,\n",
    "            'num_edges': pyg_graph_data.num_edges,\n",
    "            'test_accuracy': test_acc,\n",
    "            'test_f1': test_f1,\n",
    "            'test_auc': test_auc,\n",
    "            'best_val_f1': best_val_f1 if 'best_val_f1' in globals() else 0,\n",
    "            'training_epochs': len(training_history['epoch']) if 'training_history' in globals() else 0,\n",
    "            'parameters': sum(p.numel() for p in gnn_model.parameters()),\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "        metrics_file = results_dir / f\"evaluation_results_{config.GNN_TYPE}.json\"\n",
    "        with open(metrics_file, 'w') as f:\n",
    "            json.dump(metrics, f, indent=2)\n",
    "\n",
    "\n",
    "except Exception as e:\n",
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2854dd1",
   "metadata": {},
   "source": [
    "## üì§ Export\n",
    "\n",
    "Esta c√©lula exporta as predi√ß√µes para benchmark contra XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44003311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì§ EXPORTING PREDICTIONS\n",
    "\n",
    "try:\n",
    "    if 'gnn_model' not in globals():\n",
    "    else:\n",
    "        # Gerar predi√ß√µes para todas as transa√ß√µes\n",
    "        gnn_model.eval()\n",
    "        with torch.no_grad():\n",
    "            out = gnn_model(pyg_graph_data.x, pyg_graph_data.edge_index, pyg_graph_data.edge_attr)\n",
    "\n",
    "        # Criar DataFrame de resultados\n",
    "        results_df = pd.DataFrame({\n",
    "            'prediction_prob': all_pred_probs,\n",
    "            'ground_truth': all_labels\n",
    "        })\n",
    "\n",
    "        # Adicionar coluna de predi√ß√µes bin√°rias\n",
    "        results_df['prediction'] = (results_df['prediction_prob'] > 0.5).astype(int)\n",
    "\n",
    "        # Salvar predi√ß√µes\n",
    "        output_file = \"/content/multi_gnn_predictions.csv\"\n",
    "        results_df.to_csv(output_file, index=False)\n",
    "\n",
    "\n",
    "        # Estat√≠sticas das predi√ß√µes\n",
    "        pred_dist = results_df['prediction'].value_counts().sort_index()\n",
    "        for class_val, count in pred_dist.items():\n",
    "            class_name = \"Laundering\" if class_val == 1 else \"Legitimate\"\n",
    "            percentage = count / len(results_df) * 100\n",
    "\n",
    "        # Estat√≠sticas por conjunto\n",
    "        splits = {\n",
    "            'train': train_indices,\n",
    "            'val': val_indices,\n",
    "            'test': test_indices\n",
    "        }\n",
    "\n",
    "\n",
    "        for split_name, indices in splits.items():\n",
    "            split_preds = results_df.iloc[indices]\n",
    "            count = len(split_preds)\n",
    "            avg_prob = split_preds['prediction_prob'].mean()\n",
    "            pos_cases = split_preds['prediction'].sum()\n",
    "\n",
    "        # Salvar hist√≥rico de treinamento\n",
    "        if 'training_history' in globals():\n",
    "            history_file = results_dir / f\"training_history_{config.GNN_TYPE}.json\"\n",
    "            with open(history_file, 'w') as f:\n",
    "                json.dump(training_history, f)\n",
    "\n",
    "        # Benchmark summary\n",
    "        benchmark_summary = {\n",
    "            'model': config.GNN_TYPE,\n",
    "            'dataset': 'IBM AML',\n",
    "            'num_transactions': len(results_df),\n",
    "            'num_nodes': pyg_graph_data.num_nodes,\n",
    "            'num_edges': pyg_graph_data.num_edges,\n",
    "            'test_accuracy': test_acc if 'test_acc' in globals() else 0,\n",
    "            'test_f1': test_f1 if 'test_f1' in globals() else 0,\n",
    "            'test_auc': test_auc if 'test_auc' in globals() else 0,\n",
    "            'best_val_f1': best_val_f1 if 'best_val_f1' in globals() else 0,\n",
    "            'training_epochs': len(training_history['epoch']) if 'training_history' in globals() else 0,\n",
    "            'parameters': sum(p.numel() for p in gnn_model.parameters()),\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "        summary_file = results_dir / f\"benchmark_summary_{config.GNN_TYPE}.json\"\n",
    "        with open(summary_file, 'w') as f:\n",
    "            json.dump(benchmark_summary, f, indent=2)\n",
    "\n",
    "\n",
    "\n",
    "except Exception as e:\n",
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multignn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
# #     D o w n l o a d   d o s   R e s u l t a d o s 
 
 A p Û s   a   c o n c l u s „ o   b e m - s u c e d i d a ,   b a i x e   o   a r q u i v o   d e   p r e d i Á ı e s   p a r a   u s a r   n o   b e n c h m a r k   c o n t r a   X G B o o s t . 
 
 # # #   * * O p Á „ o   1 :   D o w n l o a d   M a n u a l   ( r e c o m e n d a d o ) * * 
 
 ` ` ` p y t h o n 
 #   E x e c u t e   e s t a   c È l u l a   p a r a   b a i x a r   a r q u i v o s   i m p o r t a n t e s 
 f r o m   g o o g l e . c o l a b   i m p o r t   f i l e s 
 
 #   D o w n l o a d   p r e d i Á ı e s   ( p a r a   b e n c h m a r k ) 
 f i l e s . d o w n l o a d ( " / c o n t e n t / m u l t i _ g n n _ p r e d i c t i o n s . c s v " ) 
 
 #   D o w n l o a d   m È t r i c a s 
 f i l e s . d o w n l o a d ( " / c o n t e n t / r e s u l t s / e v a l u a t i o n _ r e s u l t s _ G I N . j s o n " ) 
 
 #   D o w n l o a d   m o d e l o   ( o p c i o n a l   -   a r q u i v o   g r a n d e ) 
 #   f i l e s . d o w n l o a d ( " / c o n t e n t / m o d e l s / G I N _ b e s t _ m o d e l . p t h " ) 
 ` ` ` 
 
 # # #   * * O p Á „ o   2 :   S a l v a r   n o   G o o g l e   D r i v e * * 
 
 ` ` ` p y t h o n 
 #   M o n t e   o   G o o g l e   D r i v e 
 f r o m   g o o g l e . c o l a b   i m p o r t   d r i v e 
 d r i v e . m o u n t ( " / c o n t e n t / d r i v e " ) 
 
 #   C o p i e   r e s u l t a d o s 
 i m p o r t   s h u t i l 
 o u t p u t _ d i r   =   " / c o n t e n t / d r i v e / M y D r i v e / A M L _ G N N _ R e s u l t s " 
 ! m k d i r   - p   " { o u t p u t _ d i r } " 
 
 ! c p   / c o n t e n t / m u l t i _ g n n _ p r e d i c t i o n s . c s v   " { o u t p u t _ d i r } / " 
 ! c p   / c o n t e n t / r e s u l t s / e v a l u a t i o n _ r e s u l t s _ G I N . j s o n   " { o u t p u t _ d i r } / " 
 
 p r i n t ( f "   R e s u l t a d o s   s a l v o s   e m :   { o u t p u t _ d i r } " ) 
 ` ` ` 
 
 

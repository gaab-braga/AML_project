{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a33d45d2",
   "metadata": {},
   "source": [
    "# 02 · Feature Engineering e Pipelines\n",
    "## Detecção de Lavagem de Dinheiro (AML)\n",
    "\n",
    "**Objetivo:** Construir features robustas para modelagem, com pipelines reprodutíveis e foco em prevenção de data leakage.\n",
    "\n",
    "### Introdução\n",
    "\n",
    "Em detecção de AML, features temporais e de rede são cruciais porque transações suspeitas frequentemente envolvem padrões sequenciais e conexões entre entidades. Escolhi implementar agregações por entidade porque dados financeiros são intrinsecamente temporais - um cliente que movimenta grandes valores em janelas curtas pode indicar comportamento de lavagem. Para evitar data leakage, todas as features são calculadas apenas com dados históricos disponíveis no momento da transação.\n",
    "\n",
    "### Pipeline de Transformação\n",
    "\n",
    "```\n",
    "Raw Data → Cleaning → Feature Engineering → Validation → Pipeline\n",
    "    ↓         ↓             ↓                ↓          ↓\n",
    "- Load     - Remove     - Temporal       - Correlation - Sklearn\n",
    "- Parse    - Duplicates - Aggregations   - Analysis   - Pipeline\n",
    "- Anonymize - Missing   - Network        - Leakage    - Reproducible\n",
    "            - Values    - Features       - Check      - Training\n",
    "```\n",
    "\n",
    "### Estratégia de Features\n",
    "\n",
    "1. **Features Temporais**: Agregações por entidade em janelas móveis (7d, 30d)\n",
    "2. **Features de Rede**: Métricas de conectividade entre contas\n",
    "3. **Features Categóricas**: Encoding seguro sem leakage\n",
    "4. **Features Derivadas**: Razões e taxas calculadas eticamente\n",
    "\n",
    "> **Decisão de Design:** Priorizei features interpretáveis sobre complexas para garantir que o modelo possa ser explicado para compliance regulatória."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1568e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup e Importações\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Adicionar src ao path\n",
    "sys.path.append(str(Path('..').resolve()))\n",
    "\n",
    "# Imports core\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Configuração visual\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b533a4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funções locais implementadas com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Configurar sys.path para importações\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import importlib\n",
    "\n",
    "# Adicionar diretório src ao path se não estiver lá\n",
    "sys.path.append(str(Path('..').resolve()))\n",
    "\n",
    "# Force reload do módulo\n",
    "if 'src.features.aml_features' in sys.modules:\n",
    "    importlib.reload(sys.modules['src.features.aml_features'])\n",
    "\n",
    "# Importações das funções refatoradas\n",
    "from src.features.aml_features import (\n",
    "    load_raw_transactions,\n",
    "    validate_data_compliance,\n",
    "    clean_transactions,\n",
    "    impute_and_encode,\n",
    "    aggregate_by_entity,\n",
    "    compute_network_features,\n",
    "    create_temporal_features,\n",
    "    create_network_features,\n",
    "    encode_categorical_features,\n",
    "\n",
    "AMLFeaturePipeline\n",
    ")\n",
    "\n",
    "# Importar Pattern Feature Engineer\n",
    "from src.features.pattern_engineering import PatternFeatureEngineer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf18b4e",
   "metadata": {},
   "source": [
    "## ▸ Carregamento e Preparação dos Dados\n",
    "\n",
    "Carregamos os dados brutos e aplicamos limpeza inicial, garantindo compliance e anonimização."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd5ee81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Tentando carregar de: ../data/processed/transactions_enriched_final.csv\n",
      " Caminho resolvido: C:\\Users\\gafeb\\AML_project\\data\\processed\\transactions_enriched_final.csv\n",
      " É arquivo? True\n",
      " Existe? True\n",
      " Carregado arquivo específico: transactions_enriched_final.csv\n",
      " Aplicando mapeamento para arquivo processado: transactions_enriched_final.csv\n",
      " Colunas antes do mapeamento: ['Timestamp', 'From Bank ID', 'From Account', 'To Bank ID', 'To Account']...\n",
      " Colunas após mapeamento: ['timestamp', 'From Bank ID', 'source', 'To Bank ID', 'target']...\n",
      " Arquivo processado mapeado: ['timestamp', 'From Bank ID', 'source', 'To Bank ID', 'target']...\n",
      "Compliance check: Passou\n",
      "Dataset carregado: 5,078,345 transações\n",
      "Período: 2022-09-01 00:00:00 até 2022-09-18 16:18:00\n",
      "Taxa de fraude: 0.102%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>From Bank ID</th>\n",
       "      <th>source</th>\n",
       "      <th>To Bank ID</th>\n",
       "      <th>target</th>\n",
       "      <th>Amount Received</th>\n",
       "      <th>Receiving Currency</th>\n",
       "      <th>amount</th>\n",
       "      <th>Payment Currency</th>\n",
       "      <th>payment_format</th>\n",
       "      <th>...</th>\n",
       "      <th>hour</th>\n",
       "      <th>date</th>\n",
       "      <th>is_night_transaction</th>\n",
       "      <th>same_bank_transaction</th>\n",
       "      <th>same_entity_transaction</th>\n",
       "      <th>high_amount_risk</th>\n",
       "      <th>from_sole_proprietorship_flag</th>\n",
       "      <th>from_partnership_flag</th>\n",
       "      <th>from_corporation_flag</th>\n",
       "      <th>from_individual_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-09-01 00:20:00</td>\n",
       "      <td>10</td>\n",
       "      <td>8000EBD30</td>\n",
       "      <td>10</td>\n",
       "      <td>8000EBD30</td>\n",
       "      <td>3697.34</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>3697.34</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>Reinvestment</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-09-01</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-09-01 00:20:00</td>\n",
       "      <td>3208</td>\n",
       "      <td>8000F4580</td>\n",
       "      <td>1</td>\n",
       "      <td>8000F5340</td>\n",
       "      <td>0.01</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>0.01</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>Cheque</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-09-01</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-09-01 00:00:00</td>\n",
       "      <td>3209</td>\n",
       "      <td>8000F4670</td>\n",
       "      <td>3209</td>\n",
       "      <td>8000F4670</td>\n",
       "      <td>14675.57</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>14675.57</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>Reinvestment</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-09-01</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-09-01 00:02:00</td>\n",
       "      <td>12</td>\n",
       "      <td>8000F5030</td>\n",
       "      <td>12</td>\n",
       "      <td>8000F5030</td>\n",
       "      <td>2806.97</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>2806.97</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>Reinvestment</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-09-01</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-09-01 00:06:00</td>\n",
       "      <td>10</td>\n",
       "      <td>8000F5200</td>\n",
       "      <td>10</td>\n",
       "      <td>8000F5200</td>\n",
       "      <td>36682.97</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>36682.97</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>Reinvestment</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-09-01</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            timestamp  From Bank ID     source  To Bank ID     target  \\\n",
       "0 2022-09-01 00:20:00            10  8000EBD30          10  8000EBD30   \n",
       "1 2022-09-01 00:20:00          3208  8000F4580           1  8000F5340   \n",
       "2 2022-09-01 00:00:00          3209  8000F4670        3209  8000F4670   \n",
       "3 2022-09-01 00:02:00            12  8000F5030          12  8000F5030   \n",
       "4 2022-09-01 00:06:00            10  8000F5200          10  8000F5200   \n",
       "\n",
       "   Amount Received Receiving Currency    amount Payment Currency  \\\n",
       "0          3697.34          US Dollar   3697.34        US Dollar   \n",
       "1             0.01          US Dollar      0.01        US Dollar   \n",
       "2         14675.57          US Dollar  14675.57        US Dollar   \n",
       "3          2806.97          US Dollar   2806.97        US Dollar   \n",
       "4         36682.97          US Dollar  36682.97        US Dollar   \n",
       "\n",
       "  payment_format  ...  hour        date  is_night_transaction  \\\n",
       "0   Reinvestment  ...     0  2022-09-01                 False   \n",
       "1         Cheque  ...     0  2022-09-01                 False   \n",
       "2   Reinvestment  ...     0  2022-09-01                 False   \n",
       "3   Reinvestment  ...     0  2022-09-01                 False   \n",
       "4   Reinvestment  ...     0  2022-09-01                 False   \n",
       "\n",
       "  same_bank_transaction same_entity_transaction high_amount_risk  \\\n",
       "0                  True                    True            False   \n",
       "1                 False                   False            False   \n",
       "2                  True                    True            False   \n",
       "3                  True                    True            False   \n",
       "4                  True                    True            False   \n",
       "\n",
       "  from_sole_proprietorship_flag  from_partnership_flag from_corporation_flag  \\\n",
       "0                         False                   True                 False   \n",
       "1                         False                   True                 False   \n",
       "2                         False                   True                 False   \n",
       "3                          True                  False                 False   \n",
       "4                         False                   True                 False   \n",
       "\n",
       "  from_individual_flag  \n",
       "0                False  \n",
       "1                False  \n",
       "2                False  \n",
       "3                False  \n",
       "4                False  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carregar dados brutos usando função refatorada\n",
    "# Você pode especificar o caminho completo do arquivo desejado\n",
    "data_file_path = '../data/processed/transactions_enriched_final.csv'  # <- ALTERE ESTE CAMINHO CONFORME NECESSÁRIO\n",
    "df_raw = load_raw_transactions(data_path=data_file_path)\n",
    "\n",
    "# Validar compliance usando função refatorada\n",
    "is_compliant = validate_data_compliance(df_raw)\n",
    "print(f\"Compliance check: {'Passou' if is_compliant else 'Falhou'}\")\n",
    "\n",
    "# Estatísticas iniciais\n",
    "print(f\"Dataset carregado: {len(df_raw):,} transações\")\n",
    "print(f\"Período: {df_raw['timestamp'].min()} até {df_raw['timestamp'].max()}\")\n",
    "print(f\"Taxa de fraude: {df_raw['is_fraud'].mean():.3%}\")\n",
    "\n",
    "# Preview dos dados\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e401d0",
   "metadata": {},
   "source": [
    "## ▸ Limpeza e Pré-processamento\n",
    "\n",
    "Aplicamos limpeza para remover duplicatas, valores inválidos e garantir integridade dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df6b6772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Aplicando limpeza de dados...\n",
      " Limpeza concluída: 9 duplicatas removidas\n",
      "Duplicatas removidas: 9\n",
      "Dataset limpo: 5,078,336 transações\n",
      "Distribuição da classe: Normal 99.898%, Fraudulenta 0.102%\n"
     ]
    }
   ],
   "source": [
    "# Limpeza dos dados usando função refatorada\n",
    "df_clean = clean_transactions(df_raw)\n",
    "\n",
    "# Estatísticas pós-limpeza\n",
    "duplicates_removed = len(df_raw) - len(df_clean)\n",
    "print(f\"Duplicatas removidas: {duplicates_removed}\")\n",
    "print(f\"Dataset limpo: {len(df_clean):,} transações\")\n",
    "\n",
    "# Distribuição da variável target\n",
    "fraud_dist = df_clean['is_fraud'].value_counts(normalize=True)\n",
    "print(f\"Distribuição da classe: Normal {fraud_dist[0]:.3%}, Fraudulenta {fraud_dist[1]:.3%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43791bd2",
   "metadata": {},
   "source": [
    "## ▸ Features Temporais\n",
    "\n",
    "Criamos agregações por entidade em janelas temporais para capturar padrões comportamentais. Esta é a feature mais importante porque transações de lavagem frequentemente ocorrem em bursts temporais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fd755ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funções locais implementadas com sucesso!\n",
      "Dataset original: 5,078,336 linhas\n",
      "Usando sample de 50,000 linhas para teste\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-18 15:09:12,195 - INFO - Creating temporal features with pandas...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[temporal] Starting temporal feature creation with pandas...\n",
      "[temporal] Processing window=7 days...\n",
      "[temporal] Finished window=7d in 174.91s\n",
      "[temporal] Processing window=30 days...\n",
      "[temporal] Finished window=30d in 181.63s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-18 15:15:08,831 - INFO - Created 12 temporal features using pandas\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[temporal] Done. Total time: 356.63s\n",
      "Features temporais criadas: 42 colunas\n"
     ]
    }
   ],
   "source": [
    "# Force reload of the module to get latest changes\n",
    "import importlib\n",
    "import src.features.aml_features\n",
    "importlib.reload(src.features.aml_features)\n",
    "from src.features.aml_features import create_temporal_features\n",
    "\n",
    "# Features temporais usando função refatorada - TESTE COM SAMPLE PEQUENO\n",
    "print(f\"Dataset original: {df_clean.shape[0]:,} linhas\")\n",
    "df_sample = df_clean.sample(n=min(50000, len(df_clean)), random_state=42)  # Sample menor para teste\n",
    "print(f\"Usando sample de {df_sample.shape[0]:,} linhas para teste\")\n",
    "\n",
    "temporal_features_df = create_temporal_features(df_sample, windows=[7, 30])\n",
    "\n",
    "print(f\"Features temporais criadas: {temporal_features_df.shape[1]} colunas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fa9d92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Criadas 10 features temporais. Ex: ['day_of_week', 'hour', 'is_business_hours']...\n"
     ]
    }
   ],
   "source": [
    "# Mostrar apenas as colunas temporais criadas\n",
    "temporal_cols = [col for col in temporal_features_df.columns if col.startswith('source_amount_') or col.startswith('hour') or col.startswith('day_of_week') or col.startswith('is_business_hours') or col.startswith('is_weekend')]\n",
    "print(f\"Criadas {len(temporal_cols)} features temporais. Ex: {sorted(temporal_cols)[:3]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0df3cab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dataset com features de patterns para uso posterior\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Caminho do arquivo salvo\n",
    "data_dir = Path('../data/processed/')\n",
    "features_file = data_dir / 'features_with_patterns.pkl'\n",
    "\n",
    "# Carregar dataset\n",
    "df = pd.read_pickle(features_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "949dbc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping Amount Received - too many unique values (915161)\n",
      "Warning: Skipping amount - too many unique values (923873)\n",
      "Warning: Skipping source_degree - constant variable\n",
      "Warning: Skipping target_degree - constant variable\n",
      "Warning: Skipping amount_zscore_fan-out - too many unique values (923873)\n",
      "Warning: Skipping amount_similarity_fan-out - too many unique values (923615)\n",
      "Warning: Skipping amount_zscore_cycle - too many unique values (923873)\n",
      "Warning: Skipping amount_similarity_cycle - too many unique values (923289)\n",
      "Warning: Skipping amount_zscore_gather-scatter - too many unique values (923873)\n",
      "Warning: Skipping amount_similarity_gather-scatter - too many unique values (922258)\n",
      "Warning: Skipping amount_zscore_stack - too many unique values (923873)\n",
      "Warning: Skipping amount_similarity_stack - too many unique values (923163)\n",
      "Warning: Skipping amount_zscore_unknown - too many unique values (923873)\n",
      "Warning: Skipping amount_similarity_unknown - too many unique values (922117)\n",
      "Warning: Skipping amount_pattern_similarity_max - too many unique values (923620)\n",
      "Warning: Skipping amount_pattern_similarity_mean - too many unique values (923620)\n",
      "Warning: Skipping hour_pattern_similarity - constant variable\n",
      "Warning: Skipping to_bank_is_rare - constant variable\n",
      "Warning: Skipping rolling_amount_mean_3 - too many unique values (4702442)\n",
      "Warning: Skipping rolling_amount_std_3 - too many unique values (4896791)\n",
      "Warning: Skipping amount_cv_3 - too many unique values (4896874)\n",
      "Relatório de IV salvo em artifacts.\n",
      "DECISÃO: EXCLUIR todas as features temporais\n"
     ]
    }
   ],
   "source": [
    "from src.features.iv_calculator import calculate_iv, interpret_iv, get_predictive_features\n",
    "# Calcular IV para features temporais\n",
    "# Primeiro criar df_final com features temporais\n",
    "temporal_cols = [col for col in temporal_features_df.columns if col.startswith('source_amount_') or col.startswith('hour') or col.startswith('day_of_week') or col.startswith('is_business_hours') or col.startswith('is_weekend')]\n",
    "df_with_temporal = df.copy()\n",
    "\n",
    "for col in temporal_cols:\n",
    "    df_with_temporal[col] = temporal_features_df[col]\n",
    "\n",
    "iv_results_temporal = calculate_iv(\n",
    "    df_with_temporal,\n",
    "    target_col='is_fraud',\n",
    "    bins=10,\n",
    "    max_iv=10.0,\n",
    "    min_samples=1,\n",
    "    max_unique_values=100000\n",
    ")\n",
    "\n",
    "temporal_iv_results = iv_results_temporal[iv_results_temporal['variable'].isin(temporal_cols)].copy()\n",
    "\n",
    "temporal_iv_results.to_csv('../artifacts/temporal_features_iv_report.csv', index=False)\n",
    "print(\"Relatório de IV salvo em artifacts.\")\n",
    "\n",
    "predictive_count = len(temporal_iv_results[temporal_iv_results['IV'] >= 0.02])\n",
    "if predictive_count == 0:\n",
    "    print(\"DECISÃO: EXCLUIR todas as features temporais\")\n",
    "else:\n",
    "    print(f\"DECISÃO: Usar {predictive_count} features temporais com IV >= 0.02\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066293e7",
   "metadata": {},
   "source": [
    "## ▸ Features de Rede\n",
    "\n",
    "Analisamos a conectividade entre contas para identificar padrões de lavagem estruturada. Contas fraudulentas frequentemente formam clusters densos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b484f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-18 15:18:24,112 - INFO - Creating network features...\n",
      "2025-10-18 15:18:47,583 - INFO - Created network features for 515080 nodes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features de rede criadas para 515080 nós\n",
      "Estatísticas da rede: Grau médio 3.94, Centralidade média 0.0000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node</th>\n",
       "      <th>degree_centrality</th>\n",
       "      <th>in_degree_centrality</th>\n",
       "      <th>out_degree_centrality</th>\n",
       "      <th>degree</th>\n",
       "      <th>in_degree</th>\n",
       "      <th>out_degree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8000EBD30</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8000F4580</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8000F5340</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8000F4670</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8000F5030</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>22</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        node  degree_centrality  in_degree_centrality  out_degree_centrality  \\\n",
       "0  8000EBD30           0.000029              0.000025               0.000004   \n",
       "1  8000F4580           0.000004              0.000000               0.000004   \n",
       "2  8000F5340           0.000035              0.000029               0.000006   \n",
       "3  8000F4670           0.000004              0.000002               0.000002   \n",
       "4  8000F5030           0.000043              0.000039               0.000004   \n",
       "\n",
       "   degree  in_degree  out_degree  \n",
       "0      15         13           2  \n",
       "1       2          0           2  \n",
       "2      18         15           3  \n",
       "3       2          1           1  \n",
       "4      22         20           2  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Usar função importada para criar features de rede\n",
    "network_features_df = create_network_features(df_clean)\n",
    "\n",
    "print(f\"Features de rede criadas para {len(network_features_df)} nós\")\n",
    "print(f\"Estatísticas da rede: Grau médio {network_features_df['degree'].mean():.2f}, Centralidade média {network_features_df['degree_centrality'].mean():.4f}\")\n",
    "\n",
    "# Visualizar distribuição dos graus (usando apenas uma amostra para performance)\n",
    "network_features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc47735a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar dados para análise IV das features de rede\n",
    "from src.features.iv_calculator import calculate_iv, interpret_iv, get_predictive_features\n",
    "\n",
    "# Combinar features de rede com dados transacionais\n",
    "df_with_network = df.copy()\n",
    "\n",
    "# Merge features de rede para source accounts\n",
    "df_with_network = df_with_network.merge(\n",
    "    network_features_df[['node', 'degree', 'in_degree', 'out_degree',\n",
    "                        'degree_centrality', 'in_degree_centrality', 'out_degree_centrality']],\n",
    "    left_on='source',\n",
    "    right_on='node',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Renomear colunas para source\n",
    "df_with_network = df_with_network.rename(columns={\n",
    "    'degree': 'source_degree',\n",
    "    'in_degree': 'source_in_degree',\n",
    "    'out_degree': 'source_out_degree',\n",
    "    'degree_centrality': 'source_degree_centrality',\n",
    "    'in_degree_centrality': 'source_in_degree_centrality',\n",
    "    'out_degree_centrality': 'source_out_degree_centrality'\n",
    "})\n",
    "\n",
    "df_with_network = df_with_network.drop('node', axis=1, errors='ignore')\n",
    "\n",
    "# Merge features de rede para target accounts\n",
    "df_with_network = df_with_network.merge(\n",
    "    network_features_df[['node', 'degree', 'in_degree', 'out_degree',\n",
    "                        'degree_centrality', 'in_degree_centrality', 'out_degree_centrality']],\n",
    "    left_on='target',\n",
    "    right_on='node',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Renomear colunas para target\n",
    "df_with_network = df_with_network.rename(columns={\n",
    "    'degree': 'target_degree',\n",
    "    'in_degree': 'target_in_degree',\n",
    "    'out_degree': 'target_out_degree',\n",
    "    'degree_centrality': 'target_degree_centrality',\n",
    "    'in_degree_centrality': 'target_in_degree_centrality',\n",
    "    'out_degree_centrality': 'target_out_degree_centrality'\n",
    "})\n",
    "\n",
    "df_with_network = df_with_network.drop('node', axis=1, errors='ignore')\n",
    "\n",
    "# Fill NaN values para features de rede\n",
    "network_cols = [col for col in df_with_network.columns if col.startswith(('source_', 'target_')) and\n",
    "                ('degree' in col or 'centrality' in col)]\n",
    "\n",
    "for col in network_cols:\n",
    "    df_with_network[col] = df_with_network[col].fillna(0)\n",
    "\n",
    "# Amostra para análise IV\n",
    "df_network_sample = df_with_network.sample(n=min(100000, len(df_with_network)), random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "344ae273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: timestamp flagged as potentially suspicious (IV=10.000, unique=14119)\n",
      "Warning: Amount Received flagged as potentially suspicious (IV=0.993, unique=81791)\n",
      "Warning: amount flagged as potentially suspicious (IV=0.993, unique=81957)\n",
      "Warning: source_amount_sum_7d flagged as potentially suspicious (IV=0.167, unique=93002)\n",
      "Warning: source_amount_mean_7d flagged as potentially suspicious (IV=0.156, unique=92947)\n",
      "Warning: source_amount_std_7d flagged as potentially suspicious (IV=0.061, unique=75113)\n",
      "Warning: source_amount_sum_30d flagged as potentially suspicious (IV=0.084, unique=99683)\n",
      "Warning: source_amount_mean_30d flagged as potentially suspicious (IV=0.058, unique=99671)\n",
      "Warning: source_amount_std_30d flagged as potentially suspicious (IV=0.049, unique=98264)\n",
      "Error calculating IV for source_degree: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "Error calculating IV for target_degree: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "Warning: amount_zscore_fan-out flagged as potentially suspicious (IV=0.993, unique=81957)\n",
      "Warning: amount_similarity_fan-out flagged as potentially suspicious (IV=0.982, unique=81944)\n",
      "Warning: amount_zscore_cycle flagged as potentially suspicious (IV=0.993, unique=81957)\n",
      "Warning: amount_similarity_cycle flagged as potentially suspicious (IV=1.047, unique=81928)\n",
      "Warning: amount_zscore_gather-scatter flagged as potentially suspicious (IV=0.993, unique=81957)\n",
      "Warning: amount_similarity_gather-scatter flagged as potentially suspicious (IV=1.055, unique=81895)\n",
      "Warning: amount_zscore_stack flagged as potentially suspicious (IV=0.993, unique=81957)\n",
      "Warning: amount_similarity_stack flagged as potentially suspicious (IV=1.058, unique=81921)\n",
      "Warning: amount_zscore_unknown flagged as potentially suspicious (IV=0.993, unique=81957)\n",
      "Warning: amount_similarity_unknown flagged as potentially suspicious (IV=1.055, unique=81892)\n",
      "Warning: amount_pattern_similarity_max flagged as potentially suspicious (IV=0.976, unique=81944)\n",
      "Warning: amount_pattern_similarity_mean flagged as potentially suspicious (IV=1.055, unique=81944)\n",
      "Warning: Skipping hour_pattern_similarity - constant variable\n",
      "Warning: Skipping to_bank_is_rare - constant variable\n",
      "Warning: rolling_amount_mean_3 flagged as potentially suspicious (IV=0.348, unique=99800)\n",
      "Warning: rolling_amount_std_3 flagged as potentially suspicious (IV=0.098, unique=96481)\n",
      "Warning: amount_cv_3 flagged as potentially suspicious (IV=0.263, unique=96481)\n",
      "Error calculating IV for source_degree: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "Warning: Skipping source_in_degree - constant variable\n",
      "Warning: Skipping source_out_degree - constant variable\n",
      "Warning: Skipping source_degree_centrality - constant variable\n",
      "Warning: Skipping source_in_degree_centrality - constant variable\n",
      "Warning: Skipping source_out_degree_centrality - constant variable\n",
      "Error calculating IV for target_degree: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "Warning: Skipping target_in_degree - constant variable\n",
      "Warning: Skipping target_out_degree - constant variable\n",
      "Warning: Skipping target_degree_centrality - constant variable\n",
      "Warning: Skipping target_in_degree_centrality - constant variable\n",
      "Warning: Skipping target_out_degree_centrality - constant variable\n"
     ]
    }
   ],
   "source": [
    "# Calcular Information Value para features de rede\n",
    "numeric_network_cols = [col for col in network_cols if col in df_network_sample.select_dtypes(include=[np.number]).columns]\n",
    "\n",
    "iv_results = calculate_iv(\n",
    "    df_network_sample,\n",
    "    target_col='is_fraud',\n",
    "    bins=10,\n",
    "    max_iv=10.0,\n",
    "    min_samples=1,\n",
    "    max_unique_values=100000\n",
    ")\n",
    "\n",
    "network_iv_results = iv_results[iv_results['variable'].isin(numeric_network_cols)].copy()\n",
    "\n",
    "predictive_features = get_predictive_features(network_iv_results, min_iv=0.02, exclude_suspect=True)\n",
    "\n",
    "network_iv_results.to_csv('../artifacts/network_features_iv_analysis.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbfdcee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECISÃO: EXCLUIR todas as features de rede\n"
     ]
    }
   ],
   "source": [
    "# Ranking das features de rede por IV\n",
    "network_iv_results.to_csv('../artifacts/network_features_iv_analysis.csv', index=False)\n",
    "\n",
    "predictive_count = len(network_iv_results[network_iv_results['IV'] >= 0.02])\n",
    "if predictive_count == 0:\n",
    "    print(\"DECISÃO: EXCLUIR todas as features de rede\")\n",
    "else:\n",
    "    print(f\"DECISÃO: Usar {predictive_count} features de rede com IV >= 0.02\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823bc127",
   "metadata": {},
   "source": [
    "## ▸ Features Categóricas\n",
    "\n",
    "Aplicamos encoding seguro para variáveis categóricas, garantindo que não haja data leakage através de informações futuras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08b4318c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-18 15:20:12,612 - INFO - Encoding categorical features...\n",
      "2025-10-18 15:21:02,399 - INFO - Encoded 14 categorical columns\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features categóricas processadas: 14 colunas encoded\n",
      "Dataset após encoding: 22 colunas numéricas\n"
     ]
    }
   ],
   "source": [
    "# Preparar dados para encoding categórico usando função refatorada\n",
    "df_categorical = df_clean.copy()\n",
    "\n",
    "# Usar função importada para encoding\n",
    "df_encoded, encoders = encode_categorical_features(df_categorical, target_col='is_fraud')\n",
    "\n",
    "print(f\"Features categóricas processadas: {len(encoders)} colunas encoded\")\n",
    "print(f\"Dataset após encoding: {len(df_encoded.select_dtypes(include=[np.number]).columns)} colunas numéricas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5174052",
   "metadata": {},
   "source": [
    "## ▸ Validação de Features\n",
    "\n",
    "Analisamos correlações, distribuição e possíveis problemas de leakage antes de criar o pipeline final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91336e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combinando features para validação...\n",
      "\n",
      "Features temporais e de rede excluídas conforme análise de IV.\n",
      "Dataset final: 5,078,345 linhas × 31 colunas\n",
      "\n",
      "Criando features baseadas em patterns...\n",
      "Features temporais e de rede excluídas conforme análise de IV.\n",
      "Dataset final: 5,078,345 linhas × 31 colunas\n",
      "\n",
      "Criando features baseadas em patterns...\n",
      "Loaded 3209 pattern transactions from 370 laundering attempts\n",
      "Loaded 3209 pattern transactions from 370 laundering attempts\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'from_bank'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gafeb\\anaconda3\\envs\\aml\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'from_bank'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Inicializar Pattern Feature Engineer\u001b[39;00m\n\u001b[32m     14\u001b[39m pattern_engineer = PatternFeatureEngineer()\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m df_with_patterns = \u001b[43mpattern_engineer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_pattern_similarity_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_final\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Mostrar novas features criadas\u001b[39;00m\n\u001b[32m     19\u001b[39m new_pattern_features = \u001b[38;5;28mset\u001b[39m(df_with_patterns.columns) - \u001b[38;5;28mset\u001b[39m(df_final.columns)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AML_project\\src\\features\\pattern_engineering.py:168\u001b[39m, in \u001b[36mPatternFeatureEngineer.create_pattern_similarity_features\u001b[39m\u001b[34m(self, df)\u001b[39m\n\u001b[32m    165\u001b[39m df_features = \u001b[38;5;28mself\u001b[39m._add_temporal_pattern_features(df_features)\n\u001b[32m    167\u001b[39m \u001b[38;5;66;03m# 4. Feature: Concentração por banco\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m df_features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_add_bank_concentration_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[38;5;66;03m# 5. Feature: Indicadores de estrutura de lavagem\u001b[39;00m\n\u001b[32m    171\u001b[39m df_features = \u001b[38;5;28mself\u001b[39m._add_laundering_structure_features(df_features)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AML_project\\src\\features\\pattern_engineering.py:293\u001b[39m, in \u001b[36mPatternFeatureEngineer._add_bank_concentration_features\u001b[39m\u001b[34m(self, df)\u001b[39m\n\u001b[32m    289\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    290\u001b[39m \u001b[33;03mAdiciona features baseadas na concentração de transações por banco.\u001b[39;00m\n\u001b[32m    291\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    292\u001b[39m \u001b[38;5;66;03m# Concentração por banco de origem\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m293\u001b[39m bank_from_counts = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfrom_bank\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.value_counts()\n\u001b[32m    294\u001b[39m total_transactions = \u001b[38;5;28mlen\u001b[39m(df)\n\u001b[32m    296\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mfrom_bank_frequency\u001b[39m\u001b[33m'\u001b[39m] = df[\u001b[33m'\u001b[39m\u001b[33mfrom_bank\u001b[39m\u001b[33m'\u001b[39m].map(bank_from_counts) / total_transactions\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gafeb\\anaconda3\\envs\\aml\\Lib\\site-packages\\pandas\\core\\frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4115\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gafeb\\anaconda3\\envs\\aml\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'from_bank'"
     ]
    }
   ],
   "source": [
    "# Combinar todas as features criadas\n",
    "print(\"Combinando features para validação...\")\n",
    "\n",
    "# Base: usar df_raw que tem todas as colunas originais necessárias para patterns\n",
    "df_final = df_raw.copy()\n",
    "\n",
    "# Mapear colunas para o formato esperado pela função de patterns\n",
    "column_mapping = {\n",
    "    'From Bank ID': 'from_bank',\n",
    "    'To Bank ID': 'to_bank',\n",
    "    'Amount Received': 'amount_received',\n",
    "    'Receiving Currency': 'receiving_currency',\n",
    "    'Payment Currency': 'payment_currency'\n",
    "}\n",
    "df_final = df_final.rename(columns=column_mapping)\n",
    "\n",
    "print(\"Features temporais e de rede excluídas conforme análise de IV.\")\n",
    "print(f\"Dataset final: {df_final.shape[0]:,} linhas × {df_final.shape[1]} colunas\")\n",
    "\n",
    "# Aplicar features baseadas em patterns\n",
    "print(\"\\nCriando features baseadas em patterns...\")\n",
    "\n",
    "# Inicializar Pattern Feature Engineer\n",
    "pattern_engineer = PatternFeatureEngineer()\n",
    "\n",
    "df_with_patterns = pattern_engineer.create_pattern_similarity_features(df_final.copy())\n",
    "\n",
    "# Mostrar novas features criadas\n",
    "new_pattern_features = set(df_with_patterns.columns) - set(df_final.columns)\n",
    "print(f\"Criadas {len(new_pattern_features)} novas features baseadas em patterns. Ex: {sorted(list(new_pattern_features))[:3]}...\")\n",
    "\n",
    "# Recalcular correlações incluindo pattern features\n",
    "numeric_cols_updated = df_with_patterns.select_dtypes(include=[np.number]).columns\n",
    "correlations_updated = df_with_patterns[numeric_cols_updated].corr()['is_fraud'].abs().sort_values(ascending=False)\n",
    "\n",
    "print(\"Top 10 features mais correlacionadas (incluindo patterns):\")\n",
    "top_10 = correlations_updated.head(10)\n",
    "for feature, corr in top_10.items():\n",
    "    print(f\"   • {feature}: {corr:.4f}\")\n",
    "\n",
    "# Salvar dataset atualizado\n",
    "output_dir = Path('../data/processed/')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "df_with_patterns.to_pickle(output_dir / 'features_with_patterns.pkl')\n",
    "print(\"Dataset com patterns salvo!\")\n",
    "\n",
    "print(\"Features de patterns integradas com sucesso!\")\n",
    "\n",
    "# Salvar artefatos para reprodutibilidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28145099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colunas disponíveis no df_raw:\n",
      "Total de colunas: 31\n",
      "'from_bank' existe: False\n",
      "'From Bank' existe: False\n",
      "Total de colunas: 31\n",
      "'from_bank' existe: False\n",
      "'From Bank' existe: False\n",
      "'From Bank ID' existe: True\n",
      "'to_bank' existe: False\n",
      "'To Bank' existe: False\n",
      "'From Bank ID' existe: True\n",
      "'to_bank' existe: False\n",
      "'To Bank' existe: False\n",
      "'To Bank ID' existe: True\n",
      "\n",
      "Primeiras 10 colunas: ['timestamp', 'From Bank ID', 'source', 'To Bank ID', 'target', 'Amount Received', 'Receiving Currency', 'amount', 'Payment Currency', 'payment_format']\n",
      "'To Bank ID' existe: True\n",
      "\n",
      "Primeiras 10 colunas: ['timestamp', 'From Bank ID', 'source', 'To Bank ID', 'target', 'Amount Received', 'Receiving Currency', 'amount', 'Payment Currency', 'payment_format']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DEBUG: Verificar colunas disponíveis no df_raw\n",
    "print(\"Colunas disponíveis no df_raw:\")\n",
    "print(f\"Total de colunas: {len(df_raw.columns)}\")\n",
    "print(f\"'from_bank' existe: {'from_bank' in df_raw.columns}\")\n",
    "print(f\"'From Bank' existe: {'From Bank' in df_raw.columns}\")\n",
    "print(f\"'From Bank ID' existe: {'From Bank ID' in df_raw.columns}\")\n",
    "print(f\"'to_bank' existe: {'to_bank' in df_raw.columns}\")\n",
    "print(f\"'To Bank' existe: {'To Bank' in df_raw.columns}\")\n",
    "print(f\"'To Bank ID' existe: {'To Bank ID' in df_raw.columns}\")\n",
    "\n",
    "# Mostrar primeiras colunas\n",
    "print(f\"\\nPrimeiras 10 colunas: {df_raw.columns.tolist()[:10]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

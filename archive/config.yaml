# Pipeline Configuration - Money Laundering Detection
# Centralized parameters for all notebooks

# General Settings
random_state: 42
project_name: "lavagem_dev"

# Paths
paths:
  data: "data"
  artifacts: "artifacts" 
  models: "models"
  utils: "utils"
  notebooks: "notebooks"
  docs: "docs"

# Data Processing
data_processing:
  # n01 - Data Prep
  sampling:
    negative_fraction: 0.0025  # 0.25% of negative cases
    positive_fraction: 0.0400  # 4% of positive cases
    
  # Feature Scaling/Normalization (NEW)
  scaling:
    enabled: false  # Temporariamente desabilitado
    method: "standard"  # standard, robust, minmax, maxabs, none
    exclude_cols: []    # Columns to exclude from scaling (e.g., binary flags)
    save_scaler: true
    scaler_path: "artifacts/scaler.pkl"
    
  # n04 - Feature Engineering
  feature_engineering:
    min_iv_threshold: 0.01
    frequency_encoding:
      min_freq: 5
      high_cardinality_cols: ["From Bank", "To Bank", "Account", "Dest Account"]
    
  # Balancing (SMOTE + Undersampling)
  balancing:
    smote_target_positive: 600
    undersampling_ratio:
      0: 1800  # negatives
      1: 600   # positives (3:1 ratio)

# Model Training
modeling:
  # Train/test split
  test_size: 0.3
  stratify: true
  
  # Cross-validation
  cv_folds: 5
  cv_repeats: 5
  
  # Primary metric
  primary_metric: "pr_auc"  # Average Precision (PR-AUC)
  
  # Model Selection Strategy (unified across n06, n07, n08)
  model_selection:
    primary_metric: "PR_AUC"  # Main ranking criterion
    tie_breakers: ["ROC_AUC", "Recall_at_k", "F1"]  # Secondary criteria in order
    
    # Core vs Full variant policy
    prefer_core_if_retention_gte: 0.95  # Use core if retention >= 95%
    
    # Improvement thresholds
    min_improvement_primary: 0.005  # 0.5pp minimum gain for complexity increase
    
    # K reference for Recall@K tie-breaker
    k_reference: 100  # From k_values list
    
    # Model complexity ranking (simpler = better in ties)
    complexity_order: 
      - "LogisticRegression"
      - "GradientBoosting" 
      - "RandomForest"
      - "LightGBM"
      - "XGBoost"
      
    # Output artifact
    best_model_artifact: "best_model_meta.json"
  
  # n05 - Core Feature Selection
  core_selection:
    permutation_repeats: 5
    elbow_method: true
    
  # n06 - Baselines
  baseline_evaluation:
    # Model calibration
    calibrate_models: true  # Changed from false to true
    calibrate_names: ["RandomForest", "GradientBoosting", "LightGBM"]
    
    # Cross-validation for stability
    cv_folds: 3  # Quick CV for baseline comparison
    
    # Curves and artifacts
    save_curves: true
    
  baseline_models:
    LogReg_L2:
      max_iter: 1000
      class_weight: "balanced"
    RandomForest:
      n_estimators: 300
      n_jobs: -1
      class_weight: "balanced"
    GradientBoosting:
      random_state: 42
    LightGBM:
      n_estimators: 500
      class_weight: "balanced"
  
  # n07 - Hyperparameter Tuning
  tuning:
    method: "optuna"  # or "random_search" or "grid_search"
    n_trials: 30
    timeout_minutes: 60
    
    # GradientBoosting search space
    gbm_params:
      n_estimators: [50, 400]
      learning_rate: [0.01, 0.3]  # log scale
      max_depth: [2, 8]
      subsample: [0.6, 1.0]
      min_samples_leaf: [5, 80]
  
  # Candidate Gating (NEW) - Temporariamente relaxado
  gating:
    pr_auc_retention: 0.80        # Keep models >= 80% of best PR_AUC (era 0.90)
    max_cv_stability: 0.20        # Max coefficient of variation (era 0.15)
    max_candidates: 3             # Limit tuning to top N candidates
    min_pr_auc_absolute: 0.004    # Absolute performance floor (era 0.01)
    exclude_dummy: true           # Skip DummyClassifier from tuning
    
    # Multi-stage tuning
    coarse_tuning_trials: 10      # Initial exploration trials
    fine_tuning_trials: 20        # Refinement trials for top performers
    min_improvement_for_fine: 0.003  # Minimum gain to proceed to fine tuning

# Scoring & Thresholds
scoring:
  # n08 - Threshold Strategy
  threshold_search:
    min_threshold: 0.01
    max_threshold: 0.99
    step: 0.01
    optimize_metric: "expected_value"  # Changed from "f1" to "expected_value"
    capacity_limit: 200  # Maximum cases that can be reviewed per period
    
  # Expected Value for AML context
  expected_value:
    enabled: true
    # Cost/benefit parameters (adjust for your organization)
    v_tp: 10.0        # Value per True Positive ($K risk mitigated)
    c_fp: 2.0         # Cost per False Positive (analyst hours * rate)
    c_review: 0.5     # Fixed cost per case opened (overhead)
    c_fn: 50.0        # Cost per False Negative (regulatory/reputational)
    
  # FASE 1 - Business Cost Matrix for Threshold Optimization
  business_costs:
    false_positive_cost: 50        # Custo de investigação manual (R$)
    false_negative_cost: 5000      # Custo de fraude não detectada (R$)
    true_positive_benefit: 4500    # Valor recuperado ao detectar fraude (R$)
    true_negative_benefit: 0       # Benefício de rejeição correta (R$)
    investigation_capacity: 200    # Máximo de alertas por dia/período
    avg_fraud_amount: 10000        # Valor médio de transação fraudulenta (R$)
    regulatory_penalty: 50000      # Penalidade regulatória por fraude não detectada (R$)
    
    # Capacity scenarios to compare
    capacity_scenarios: [50, 100, 200, 500]  # Cases per period
    
  # Risk levels (percentiles)
  risk_levels:
    HIGH: 0.95
    MEDIUM: 0.85
    LOW: 0.70
    
  # Evaluation @K
  k_values: [50, 100, 200, 500]

# === ROADMAP FASE 1 - GOVERNANÇA ===
roadmap_governance:
  # Hash e reproducibilidade
  require_dataset_hash: true
  require_git_commit: true
  sampling_manifest_required: true
  
  # Correção de probabilidade pós-sampling
  probability_correction:
    enabled: true
    original_prevalence_source: "sampling_manifest.json"
    
  # Métricas AML obrigatórias
  mandatory_aml_metrics:
    - "precision_at_k"
    - "recall_at_k" 
    - "lift_at_k"
    - "expected_cost"
    - "pr_auc"

# Anomaly Detection (n09)
anomaly_detection:
  methods: ["iforest", "lof"]
  combine_method: "rank_average"
  
  # IsolationForest params
  isolation_forest:
    contamination: "auto"
    n_estimators: 100
    
  # LOF params  
  local_outlier_factor:
    n_neighbors: 20
    contamination: "auto"

# Graph Features (n10)
graph_features:
  enabled: true
  sender_col: "sender_id"  # From Bank + Account
  receiver_col: "receiver_id"  # To Bank + Dest Account
  amount_col: "Amount Received"
  
  # Centrality measures
  centralities: ["degree", "betweenness", "pagerank"]
  max_graph_nodes: 10000  # Limit for performance

# Ensemble (n11)
ensemble:
  # Enhanced ensemble configuration
  enabled: true
  method: "meta_learner"  # meta_learner, weighted_average, rank_mean
  
  # Component weights (if not using meta_learner)
  weights:
    supervised: 0.7
    anomaly: 0.2
    graph: 0.1  # Now enabled for graph features
    
  # Blending method
  blend_method: "meta_learner"  # meta_learner, rank_mean, weighted_average
  
  # Risk band classification
  band_thresholds:
    use_from_n08: true  # Use thresholds.json from n08
    
  # Enhanced ensemble settings
  enhanced_ensemble:
    calibration_method: "isotonic"  # isotonic, sigmoid, none
    anomaly_integration_weight: 0.2
    graph_features_enabled: true
    meta_learner_cv: 3
    component_models:
      - "baseline"
      - "tuned" 
      - "anomaly_specialist"
      - "graph_specialist"

# Monitoring (n12, n14)
monitoring:
  # PSI thresholds
  psi_thresholds:
    stable: 0.1
    small_change: 0.2
    significant_change: 0.3
    
  # n12 - Basic monitoring
  basic_monitoring:
    simulation_drift_std: 0.08  # 8% noise for simulation
    
  # n14 - Temporal drift
  temporal_drift:
    n_windows: 5
    sample_fraction: 0.85  # Per window
    max_features_analyzed: 15
    drift_intensities: [0.02, 0.05, 0.08, 0.12, 0.15]
    affected_features_per_window: [3, 5, 4, 6, 7]

# Explainability (n13)
explainability:
  # Permutation importance
  permutation:
    n_repeats: 3
    n_jobs: 1  # Single-threaded for stability
    
  # SHAP
  shap:
    enabled: true
    max_samples: 500  # For performance
    save_plots: true
    plot_formats: ["png"]
    
  # Sample size for explainability analysis
  max_analysis_samples: 1500

# Performance & Resource Limits
performance:
  # Memory management
  max_dataset_rows: 50000  # For full dataset operations
  chunk_size: 10000
  
  # Parallel processing
  n_jobs: -1  # Use all cores (where applicable)
  
  # Model serving
  model_format: "pkl"  # or "joblib"

# Logging & Outputs
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  save_logs: true
  log_file: "artifacts/pipeline.log"

# Artifacts & Versioning
artifacts:
  # Timestamp in filenames
  timestamp_format: "%Y%m%d_%H%M%S"
  
  # Retention
  keep_intermediate: true
  compress_large_files: false
  
  # Key outputs to always preserve
  critical_outputs:
    - "best_model_tuned.pkl"
    - "risk_scores_ensemble.csv"
    - "thresholds.json"
    - "core_features.txt"
    - "ensemble_metadata.json"
    - "temporal_validation.json"         # NEW: Temporal validation report
    - "backtest_results.json"            # NEW: Backtesting results
    - "enhanced_ensemble_scores.csv"     # NEW: Enhanced ensemble scores
    - "enhanced_ensemble_evaluation.json" # NEW: Enhanced ensemble evaluation
    - "feature_manifest.json"            # NEW: Feature versioning manifest

# NEW: Medium Priority Improvements
medium_priority_features:
  # Temporal validation
  temporal_validation:
    enabled: true
    min_gap_days: 1
    entity_column: "Account"
    date_column: "Timestamp"
    max_entity_overlap_pct: 10.0
    
  # Backtesting configuration
  backtesting:
    enabled: true
    initial_train_months: 12
    step_months: 1
    test_months: 3
    max_windows: 12
    min_train_samples: 1000
    min_test_samples: 100
    purge_days: 0
    
  # Feature manifest versioning
  feature_manifest:
    enabled: true
    track_feature_importance: true
    track_feature_stability: true
    track_feature_types: true
    save_feature_lineage: true
    
  # Cost-adjusted metrics
  cost_adjusted_metrics:
    enabled: true
    monitoring_cost_per_case: 2.0
    investigation_cost_per_case: 10.0
    regulatory_penalty_per_miss: 100.0
    reputation_cost_multiplier: 5.0
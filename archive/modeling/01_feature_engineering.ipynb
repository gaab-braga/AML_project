{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb7eca0e",
   "metadata": {},
   "source": [
    "# 01 · Data Preparation & Anti-Overfitting Corrections\n",
    "## Sistema Completo de Preparação de Dados com Validações Anti-Overfitting\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│   COMPLETE DATA PREPARATION - ANTI-OVERFITTING SYSTEM      │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "![Status](https://img.shields.io/badge/Status-Production_Ready-green)\n",
    "![Priority](https://img.shields.io/badge/Priority-CRITICAL-red)\n",
    "![Type](https://img.shields.io/badge/Type-Data_Preparation-success)\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "### OBJETIVO GERAL\n",
    "\n",
    "Este notebook implementa o **sistema completo de preparação de dados** com validações anti-overfitting críticas, combinando:\n",
    "\n",
    "1. **Preparação Básica de Dados**: Carregamento, limpeza, encoding e balanceamento\n",
    "2. **Engenharia de Features**: Criação de features avançadas sem vazamento\n",
    "3. **Validações Anti-Overfitting**: Detecção e correção de data leakage\n",
    "4. **Validação Temporal**: TimeSeriesSplit e features temporais seguras\n",
    "5. **Correções Críticas**: Remoção de features suspeitas e regularização\n",
    "\n",
    "### PROBLEMA RESOLVIDO\n",
    "\n",
    "<div style=\"background-color: #2d1a1a; border-left: 4px solid #ef4444; padding: 15px; border-radius: 4px;\">\n",
    "\n",
    "**OVERFITTING CRÍTICO IDENTIFICADO:**\n",
    "```\n",
    "Performance Gap Analysis (ANTES das correções)\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "Training PR-AUC  : 0.999  [████████████████████] Perfect\n",
    "Test PR-AUC      : 0.007  [█                   ] Random\n",
    "Gap              : 153x degradation (55% médio)\n",
    "Adversarial AUC  : 0.89  (Leakage severo detectado)\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "```\n",
    "\n",
    "**CAUSAS:**\n",
    "- ✅ Data leakage em features de agregação\n",
    "- ✅ Validação não-temporal (StratifiedKFold)\n",
    "- ✅ Features calculadas sobre toda a base\n",
    "- ✅ Falta de regularização temporal\n",
    "\n",
    "</div>\n",
    "\n",
    "### ESTRATÉGIA DE CORREÇÃO\n",
    "\n",
    "```\n",
    "┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐\n",
    "│   Data Loading  │ -> │  Feature Eng.   │ -> │  Leakage        │\n",
    "│   & Cleaning    │    │  (Safe)         │    │  Detection      │\n",
    "└─────────────────┘    └─────────────────┘    └─────────────────┘\n",
    "         │                       │                       │\n",
    "         └───────────────────────┼───────────────────────┘\n",
    "                                 │\n",
    "                    ┌────────────▼────────────┐\n",
    "                    │  Temporal Validation   │\n",
    "                    │  & Safe Features       │\n",
    "                    └─────────────────────────┘\n",
    "                                 │\n",
    "                    ┌────────────▼────────────┐\n",
    "                    │  Clean Data Export     │\n",
    "                    │  (Ready for Modeling)  │\n",
    "                    └─────────────────────────┘\n",
    "```\n",
    "\n",
    "### SAÍDAS DO NOTEBOOK\n",
    "\n",
    "- **Dados Limpos**: `X_train_temporal_clean.parquet`, `X_test_temporal_clean.parquet`\n",
    "- **Validações**: Relatórios de leakage, validação temporal\n",
    "- **Metadata**: Estatísticas de processamento e correções aplicadas\n",
    "- **Modelos Corrigidos**: Versões com regularização avançada (opcional)\n",
    "\n",
    "> **IMPORTANTE**: Este notebook substitui os notebooks 01 e 06 anteriores, consolidando todo o processamento de dados em um sistema único e confiável.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f63805a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m sys.path.append(\u001b[38;5;28mstr\u001b[39m(Path(\u001b[33m'\u001b[39m\u001b[33m../..\u001b[39m\u001b[33m'\u001b[39m).resolve()))\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Professional utils (modularized)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     14\u001b[39m     FraudMetrics,\n\u001b[32m     15\u001b[39m     get_cv_strategy,\n\u001b[32m     16\u001b[39m     cross_validate_with_metrics\n\u001b[32m     17\u001b[39m )\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     20\u001b[39m     optimize_dtypes,\n\u001b[32m     21\u001b[39m     load_data,\n\u001b[32m   (...)\u001b[39m\u001b[32m     24\u001b[39m     check_artifact_exists\n\u001b[32m     25\u001b[39m )\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexplainability\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     28\u001b[39m     compute_shap_values,\n\u001b[32m     29\u001b[39m     compute_permutation_importance\n\u001b[32m     30\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Desktop\\lavagem_dev\\utils\\__init__.py:10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Import commonly used functions for convenience\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m bootstrap_metric, compute_cv_metrics\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compute_calibration_metrics\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ThresholdConfig, evaluate_thresholds\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Desktop\\lavagem_dev\\utils\\metrics.py:88\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m List, Tuple, Dict, Optional\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcalibration\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m calibration_curve\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m brier_score_loss\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gafeb\\anaconda3\\envs\\aml\\Lib\\site-packages\\sklearn\\__init__.py:83\u001b[39m\n\u001b[32m     69\u001b[39m     \u001b[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[32m     70\u001b[39m     \u001b[38;5;66;03m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   (...)\u001b[39m\u001b[32m     77\u001b[39m     \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[32m     78\u001b[39m     \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     80\u001b[39m         __check_build,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m     81\u001b[39m         _distributor_init,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m     82\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[32m     84\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_show_versions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show_versions\n\u001b[32m     86\u001b[39m     __all__ = [\n\u001b[32m     87\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcalibration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     88\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcluster\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    129\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mshow_versions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    130\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gafeb\\anaconda3\\envs\\aml\\Lib\\site-packages\\sklearn\\base.py:19\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _IS_32BIT\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_estimator_html_repr\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m estimator_html_repr\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_metadata_requests\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gafeb\\anaconda3\\envs\\aml\\Lib\\site-packages\\sklearn\\utils\\__init__.py:27\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdiscovery\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m all_estimators\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfixes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parse_version, threadpool_info\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmurmurhash\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m murmurhash3_32\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvalidation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     29\u001b[39m     _is_arraylike_not_scalar,\n\u001b[32m     30\u001b[39m     as_float_array,\n\u001b[32m   (...)\u001b[39m\u001b[32m     39\u001b[39m     indexable,\n\u001b[32m     40\u001b[39m )\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Do not deprecate parallel_backend and register_parallel_backend as they are\u001b[39;00m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# needed to tune `scikit-learn` behavior and have different effect if called\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# from the vendored version or or the site-package version. The other are\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# utilities that are independent of scikit-learn so they are not part of\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# scikit-learn public API.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msklearn\\utils\\murmurhash.pyx:1\u001b[39m, in \u001b[36minit sklearn.utils.murmurhash\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "# Add parent directory to Python path for utils import\n",
    "sys.path.append(str(Path('../..').resolve()))\n",
    "\n",
    "# Professional utils (modularized)\n",
    "from utils.modeling import (\n",
    "    FraudMetrics,\n",
    "    get_cv_strategy,\n",
    "    cross_validate_with_metrics\n",
    ")\n",
    "\n",
    "from utils.data import (\n",
    "    optimize_dtypes,\n",
    "    load_data,\n",
    "    save_artifact,\n",
    "    load_artifact,\n",
    "    check_artifact_exists\n",
    ")\n",
    "\n",
    "from utils.explainability import (\n",
    "    compute_shap_values,\n",
    "    compute_permutation_importance\n",
    ")\n",
    "\n",
    "# Centralized setup utilities\n",
    "from utils.setup import load_config_and_paths, setup_notebook_environment\n",
    "\n",
    "# Models\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"✓ Imports complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83bda386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Custom progress callback for early stopping created\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CUSTOM PROGRESS CALLBACK FOR EARLY STOPPING\n",
    "# ============================================================================\n",
    "\n",
    "def create_early_stopping_progress_callback(model_name: str, max_iterations: int = 1000):\n",
    "    \"\"\"\n",
    "    Create a progress callback that works well with early stopping.\n",
    "\n",
    "    Instead of showing fixed percentage based on max_iterations, it shows:\n",
    "    - Current iteration / best iteration found so far\n",
    "    - Early stopping status\n",
    "    - Time elapsed\n",
    "\n",
    "    Args:\n",
    "        model_name: Name of the model being trained\n",
    "        max_iterations: Maximum possible iterations (for reference)\n",
    "\n",
    "    Returns:\n",
    "        Callback function for the model\n",
    "    \"\"\"\n",
    "    import time\n",
    "    from IPython.display import clear_output\n",
    "\n",
    "    start_time = time.time()\n",
    "    best_iteration = None\n",
    "    last_update = 0\n",
    "\n",
    "    def progress_callback(env):\n",
    "        nonlocal best_iteration, last_update\n",
    "\n",
    "        current_iteration = env.iteration\n",
    "        current_score = env.evaluation_result_list[0][2]  # Get current metric value\n",
    "\n",
    "        # Update best iteration if this is better\n",
    "        if best_iteration is None or current_score > env.evaluation_result_list[0][2]:\n",
    "            best_iteration = current_iteration\n",
    "\n",
    "        # Update progress every 10 iterations or at key points\n",
    "        if current_iteration - last_update >= 10 or current_iteration < 20:\n",
    "            elapsed = time.time() - start_time\n",
    "\n",
    "            # Clear previous output for clean display\n",
    "            clear_output(wait=True)\n",
    "\n",
    "            print(f\"🏋️ Training {model_name}...\")\n",
    "            print(f\"   Iteration: {current_iteration}/{max_iterations}\")\n",
    "            print(f\"   Best Iteration: {best_iteration}\")\n",
    "            print(f\"   Current Score: {current_score:.4f}\")\n",
    "            print(f\"   Time Elapsed: {elapsed:.1f}s\")\n",
    "            print(f\"   Progress: {'█' * int(20 * current_iteration / max(1, best_iteration or current_iteration))}{'░' * (20 - int(20 * current_iteration / max(1, best_iteration or current_iteration)))}\")\n",
    "\n",
    "            last_update = current_iteration\n",
    "\n",
    "    return progress_callback\n",
    "\n",
    "print(\"✅ Custom progress callback for early stopping created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fab2671e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_config_and_paths' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load config and setup environment using centralized utilities\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m config, paths = \u001b[43mload_config_and_paths\u001b[49m(config_path=\u001b[33m'\u001b[39m\u001b[33m../../../config.yaml\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m setup_notebook_environment()\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Notebook-specific overrides\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'load_config_and_paths' is not defined"
     ]
    }
   ],
   "source": [
    "# Load config and setup environment using centralized utilities\n",
    "config, paths = load_config_and_paths(config_path='../../../config.yaml')\n",
    "setup_notebook_environment()\n",
    "\n",
    "# Notebook-specific overrides\n",
    "config['notebook_mode'] = True\n",
    "config['dev_mode'] = False  # Set True for 5% sample\n",
    "\n",
    "# Extract paths\n",
    "DATA_DIR = paths['data']\n",
    "ARTIFACTS_DIR = paths['artifacts']\n",
    "MODELS_DIR = ARTIFACTS_DIR / 'models'\n",
    "ARTIFACTS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "MODELS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(f\"Primary metric: {config['modeling']['primary_metric'].upper()}\")\n",
    "print(f\"CV folds: {config['modeling']['cv_folds']}\")\n",
    "print(f\"Random state: {config['random_state']}\")\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Models directory: {MODELS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bd399ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATA LOADING\n",
      "============================================================\n",
      "✓ Full dataset: 211,180 rows | Fraud: 3,565\n",
      "✓ Sampled: 9,793 rows | Fraud rate: 36.40%\n",
      "✓ Train: 6,855 (2,350 frauds, 34.28%)\n",
      "✓ Test:  2,938 (1,215 frauds, 41.35%)\n",
      "✓ Encoded: 5 categorical columns\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# DATA LOADING (CORRECTED - NO LEAKAGE)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA LOADING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load full dataset\n",
    "df = pd.read_csv(DATA_DIR / 'df_Money_Laundering_v2.csv')\n",
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "df = df.rename(columns={'Account.1': 'Dest Account'})\n",
    "df['Day'] = df['Timestamp'].dt.day\n",
    "\n",
    "print(f\"✓ Full dataset: {len(df):,} rows | Fraud: {df['Is Laundering'].sum():,}\")\n",
    "\n",
    "# Smart sampling: 100% frauds + 3% normal\n",
    "fraud = df[df['Is Laundering'] == 1]\n",
    "normal = df[df['Is Laundering'] == 0].sample(frac=0.03, random_state=config['random_state'])\n",
    "df_sampled = pd.concat([fraud, normal]).sample(frac=1, random_state=config['random_state'])\n",
    "\n",
    "print(f\"✓ Sampled: {len(df_sampled):,} rows | Fraud rate: {df_sampled['Is Laundering'].mean():.2%}\")\n",
    "\n",
    "# TEMPORAL SPLIT FIRST (prevents leakage)\n",
    "df_sorted = df_sampled.sort_values('Timestamp').reset_index(drop=True)\n",
    "split_idx = int(len(df_sorted) * 0.7)\n",
    "train_df = df_sorted.iloc[:split_idx]\n",
    "test_df = df_sorted.iloc[split_idx:]\n",
    "\n",
    "# Extract features and target\n",
    "feature_cols = ['Timestamp', 'Dest Account', 'Payment Format', 'From Bank', 'Account', \n",
    "                'Day', 'To Bank', 'Amount Received', 'Amount Paid']\n",
    "\n",
    "X_train = train_df[feature_cols].copy()\n",
    "y_train = train_df['Is Laundering'].copy()\n",
    "X_test = test_df[feature_cols].copy()\n",
    "y_test = test_df['Is Laundering'].copy()\n",
    "\n",
    "# ENCODING AFTER SPLIT (fit only on train - prevents leakage)\n",
    "categorical_cols = ['Dest Account', 'Payment Format', 'From Bank', 'Account', 'To Bank']\n",
    "for col in categorical_cols:\n",
    "    train_categories = X_train[col].astype('category')\n",
    "    X_train[col] = train_categories.cat.codes\n",
    "    X_test[col] = pd.Categorical(X_test[col], categories=train_categories.cat.categories).codes\n",
    "\n",
    "print(f\"✓ Train: {len(X_train):,} ({y_train.sum():,} frauds, {y_train.mean():.2%})\")\n",
    "print(f\"✓ Test:  {len(X_test):,} ({y_test.sum():,} frauds, {y_test.mean():.2%})\")\n",
    "print(f\"✓ Encoded: {len(categorical_cols)} categorical columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6f28297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FEATURE ENGINEERING\n",
      "============================================================\n",
      "🕒 Creating temporal features...\n",
      "💫 Creating interaction features...\n",
      "📊 Computing aggregation statistics (TRAIN only)...\n",
      "🧮 Creating derived features...\n",
      "[OK] Created 24 new features\n",
      "   Total features: 32\n",
      "🕒 Creating temporal features...\n",
      "💫 Creating interaction features...\n",
      "📊 Applying aggregation statistics from TRAIN...\n",
      "🧮 Creating derived features...\n",
      "[OK] Created 24 new features\n",
      "   Total features: 32\n",
      "✓ Train features: (6855, 32)\n",
      "✓ Test features: (2938, 32)\n",
      "✓ New features: 23\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# INFORMATION VALUE BASED FEATURE SELECTION\n",
    "# ============================================================================\n",
    "\n",
    "from preprocessing import calculate_iv\n",
    "import pandas as pd\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"INFORMATION VALUE FEATURE SELECTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate IV for all features\n",
    "iv_results = calculate_iv(X_train_featured, y_train, max_iv=5.0, min_samples=20)\n",
    "\n",
    "# Filter features based on IV threshold\n",
    "exclude_cols = ['Timestamp']\n",
    "min_iv = 0.15\n",
    "\n",
    "iv_filtered = iv_results[\n",
    "    (~iv_results['variável'].isin(exclude_cols)) & \n",
    "    (iv_results['IV'] >= min_iv)\n",
    "].copy()\n",
    "\n",
    "selected_features = iv_filtered['variável'].tolist()\n",
    "print(f\"✓ Features selected by IV (≥{min_iv}): {len(selected_features)}\")\n",
    "\n",
    "# Display top features by IV\n",
    "print(\"\\nTop 10 Features by Information Value:\")\n",
    "display(iv_filtered.head(10)[['variável', 'IV']].round(4))\n",
    "\n",
    "# Apply IV filtering\n",
    "X_train_iv_filtered = X_train_featured[selected_features]\n",
    "X_test_iv_filtered = X_test_featured[selected_features]\n",
    "\n",
    "print(f\"✓ IV filtering applied: {X_train_featured.shape[1]} → {X_train_iv_filtered.shape[1]} features\")\n",
    "\n",
    "# Update datasets for next steps\n",
    "X_train_featured = X_train_iv_filtered\n",
    "X_test_featured = X_test_iv_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fd0f212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "BALANCING STRATEGY\n",
      "============================================================\n",
      "Current fraud rate: 34.28%\n",
      "✓ Adequate fraud rate (>1%) - using original data\n",
      "✓ Final: 6,855 samples | 34.28% fraud rate\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# BALANCING STRATEGY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BALANCING STRATEGY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "fraud_rate = y_train.mean()\n",
    "print(f\"Current fraud rate: {fraud_rate:.2%}\")\n",
    "\n",
    "if fraud_rate > 0.05:  # > 5%\n",
    "    print(\"✓ Adequate fraud rate (>1%) - using original data\")\n",
    "    X_train_balanced = X_train_featured.copy()\n",
    "    y_train_balanced = y_train.copy()\n",
    "else:\n",
    "    print(\"⚠ Low fraud rate - applying SMOTE-ENN (superior noise reduction)...\")\n",
    "    from utils.sampling import create_balanced_dataset\n",
    "    X_train_balanced, y_train_balanced = create_balanced_dataset(\n",
    "        X_train_featured, y_train,\n",
    "        method='smote_enn',\n",
    "        random_state=config['random_state']\n",
    "    )\n",
    "\n",
    "print(f\"✓ Final: {len(y_train_balanced):,} samples | {y_train_balanced.mean():.2%} fraud rate\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a66703",
   "metadata": {},
   "source": [
    "## ▸ FASE 4: Validações Anti-Overfitting e Correções Críticas\n",
    "\n",
    "<div style=\"background-color: #2d2416; border-left: 4px solid #f59e0b; padding: 15px; border-radius: 4px;\">\n",
    "\n",
    "**OBJETIVO**\n",
    "\n",
    "Implementar validações críticas para prevenir overfitting e garantir generalização do modelo.\n",
    "\n",
    "### Correções Implementadas:\n",
    "\n",
    "1. **Validação Temporal (TimeSeriesSplit)**: Substitui StratifiedKFold para respeitar ordem temporal\n",
    "2. **Detecção de Data Leakage**: Identifica features suspeitas através de distribuição e correlação\n",
    "3. **Re-engenharia Temporal Segura**: Cria features temporais sem vazamento de informação futura\n",
    "4. **Regularização Avançada**: Aplica técnicas de regularização para controle de overfitting\n",
    "\n",
    "### Por que essas validações são críticas?\n",
    "\n",
    "- **Leakage Detection**: Features calculadas sobre toda a base podem \"vazar\" informação do futuro\n",
    "- **Temporal Validation**: Dados financeiros devem respeitar ordem cronológica\n",
    "- **Safe Features**: Agregações temporais devem usar apenas dados passados\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddbc846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "🔄 IMPLEMENTING TEMPORAL CROSS-VALIDATION\n",
      "================================================================================\n",
      "✅ Created 5 temporal CV splits\n",
      "  Fold 1: Train size=6675, Test size=30\n",
      "  Fold 2: Train size=6705, Test size=30\n",
      "  Fold 3: Train size=6735, Test size=30\n",
      "  Fold 4: Train size=6765, Test size=30\n",
      "  Fold 5: Train size=6795, Test size=30\n",
      "\n",
      "✅ Temporal cross-validation implementation complete!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TEMPORAL CROSS-VALIDATION IMPLEMENTATION\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import make_scorer, recall_score, precision_score, f1_score\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"🔄 IMPLEMENTING TEMPORAL CROSS-VALIDATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create temporal CV splits for validation\n",
    "cv_splits = get_temporal_cv_splits(X_train_balanced, y_train_balanced, n_splits=5, test_size=30, gap=0)\n",
    "\n",
    "print(f\"✅ Created {len(cv_splits)} temporal CV splits\")\n",
    "for i, (train_idx, test_idx) in enumerate(cv_splits):\n",
    "    print(f\"  Fold {i+1}: Train size={len(train_idx)}, Test size={len(test_idx)}\")\n",
    "\n",
    "print(\"\\n✅ Temporal cross-validation implementation complete!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85092c75",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing lib: Não foi possível encontrar o procedimento especificado.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ============================================================================\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# DATA LEAKAGE DETECTION AND ANALYSIS\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# ============================================================================\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstats\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ks_2samp, chi2_contingency\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeature_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mutual_info_classif\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34manalyze_aggregation_features\u001b[39m(X: pd.DataFrame,\n\u001b[32m     10\u001b[39m                                 feature_patterns: List[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m) -> Dict:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gafeb\\anaconda3\\envs\\aml\\Lib\\site-packages\\sklearn\\__init__.py:73\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# `_distributor_init` allows distributors to run custom init code.\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# For instance, for the Windows wheel, this is used to pre-load the\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# vcomp shared library runtime for OpenMP embedded in the sklearn/.libs\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401 E402\u001b[39;00m\n\u001b[32m     70\u001b[39m     __check_build,\n\u001b[32m     71\u001b[39m     _distributor_init,\n\u001b[32m     72\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clone  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_show_versions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show_versions  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[32m     76\u001b[39m _submodules = [\n\u001b[32m     77\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcalibration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     78\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcluster\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    114\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcompose\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    115\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gafeb\\anaconda3\\envs\\aml\\Lib\\site-packages\\sklearn\\base.py:19\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_metadata_requests\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester, _routing_enabled\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_missing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_scalar_nan\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_param_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gafeb\\anaconda3\\envs\\aml\\Lib\\site-packages\\sklearn\\utils\\__init__.py:9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m metadata_routing\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_bunch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Bunch\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_chunking\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gen_batches, gen_even_slices\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Make _safe_indexing importable from here for backward compat as this particular\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# helper is considered semi-private and typically very useful for third-party\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# libraries that want to comply with scikit-learn's estimator API. In particular,\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# _safe_indexing was included in our public API documentation despite the leading\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# `_` in its name.\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_indexing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     17\u001b[39m     _safe_indexing,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m     18\u001b[39m     resample,\n\u001b[32m     19\u001b[39m     shuffle,\n\u001b[32m     20\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gafeb\\anaconda3\\envs\\aml\\Lib\\site-packages\\sklearn\\utils\\_chunking.py:11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_param_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Interval, validate_params\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mchunk_generator\u001b[39m(gen, chunksize):\n\u001b[32m     15\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Chunk generator, ``gen`` into lists of length ``chunksize``. The last\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[33;03m    chunk may have a length less than ``chunksize``.\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gafeb\\anaconda3\\envs\\aml\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:17\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m csr_matrix, issparse\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvalidation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _is_arraylike_not_scalar\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mInvalidParameterError\u001b[39;00m(\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[32m     21\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Custom exception to be raised when the parameter of a class/method/function\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[33;03m    does not have a valid type or value.\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gafeb\\anaconda3\\envs\\aml\\Lib\\site-packages\\sklearn\\utils\\validation.py:21\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_config \u001b[38;5;28;01mas\u001b[39;00m _get_config\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataConversionWarning, NotFittedError, PositiveSpectrumWarning\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_array_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _asarray_with_order, _is_numpy_namespace, get_namespace\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdeprecation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _deprecate_force_all_finite\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfixes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ComplexWarning, _preserve_dia_indices_dtype\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gafeb\\anaconda3\\envs\\aml\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:20\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexternals\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m array_api_extra \u001b[38;5;28;01mas\u001b[39;00m xpx\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexternals\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marray_api_compat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m numpy \u001b[38;5;28;01mas\u001b[39;00m np_compat\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfixes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parse_version\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# TODO: complete __all__\u001b[39;00m\n\u001b[32m     23\u001b[39m __all__ = [\u001b[33m\"\u001b[39m\u001b[33mxpx\u001b[39m\u001b[33m\"\u001b[39m]  \u001b[38;5;66;03m# we import xpx here just to re-export it, need this to appease ruff\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gafeb\\anaconda3\\envs\\aml\\Lib\\site-packages\\sklearn\\utils\\fixes.py:421\u001b[39m\n\u001b[32m    419\u001b[39m PYARROW_VERSION_BELOW_17 = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    420\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m421\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyarrow\u001b[39;00m\n\u001b[32m    423\u001b[39m     pyarrow_version = parse_version(pyarrow.__version__)\n\u001b[32m    424\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m pyarrow_version < parse_version(\u001b[33m\"\u001b[39m\u001b[33m17.0.0\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gafeb\\anaconda3\\envs\\aml\\Lib\\site-packages\\pyarrow\\__init__.py:65\u001b[39m\n\u001b[32m     63\u001b[39m _gc_enabled = _gc.isenabled()\n\u001b[32m     64\u001b[39m _gc.disable()\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyarrow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_lib\u001b[39;00m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _gc_enabled:\n\u001b[32m     67\u001b[39m     _gc.enable()\n",
      "\u001b[31mImportError\u001b[39m: DLL load failed while importing lib: Não foi possível encontrar o procedimento especificado."
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# DATA LEAKAGE DETECTION AND ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "from scipy.stats import ks_2samp, chi2_contingency\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import seaborn as sns\n",
    "\n",
    "def analyze_aggregation_features(X: pd.DataFrame,\n",
    "                                feature_patterns: List[str] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze aggregation features that might contain data leakage.\n",
    "\n",
    "    Args:\n",
    "        X: Feature matrix\n",
    "        feature_patterns: Patterns to identify aggregation features\n",
    "\n",
    "    Returns:\n",
    "        Analysis of aggregation features\n",
    "    \"\"\"\n",
    "    if feature_patterns is None:\n",
    "        feature_patterns = ['_mean', '_sum', '_count', '_std', '_min', '_max', '_median']\n",
    "\n",
    "    aggregation_features = []\n",
    "    for col in X.columns:\n",
    "        if any(pattern in col.lower() for pattern in feature_patterns):\n",
    "            aggregation_features.append(col)\n",
    "\n",
    "    print(f\"🔍 Found {len(aggregation_features)} potential aggregation features\")\n",
    "\n",
    "    # Analyze each aggregation feature\n",
    "    feature_analysis = []\n",
    "    for feature in aggregation_features:\n",
    "        try:\n",
    "            values = X[feature].dropna()\n",
    "            stats = {\n",
    "                'feature': feature,\n",
    "                'n_unique': values.nunique(),\n",
    "                'n_zero': (values == 0).sum(),\n",
    "                'pct_zero': (values == 0).sum() / len(values) * 100,\n",
    "                'mean': values.mean(),\n",
    "                'std': values.std(),\n",
    "                'min': values.min(),\n",
    "                'max': values.max(),\n",
    "                'skewness': values.skew(),\n",
    "                'kurtosis': values.kurtosis()\n",
    "            }\n",
    "            feature_analysis.append(stats)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Error analyzing {feature}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return {\n",
    "        'aggregation_features': aggregation_features,\n",
    "        'feature_analysis': feature_analysis,\n",
    "        'total_aggregation_features': len(aggregation_features)\n",
    "    }\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"🔍 INVESTIGATING DATA LEAKAGE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Detect data leakage in current features\n",
    "leakage_results = detect_data_leakage_features(X_train_balanced, X_test_featured, y_train_balanced)\n",
    "\n",
    "print(f\"✅ Analysis complete: {leakage_results['suspicious_features_count']} suspicious features found\")\n",
    "print(f\"   Distribution shifts: {leakage_results['distribution_shift_count']}\")\n",
    "\n",
    "# Analyze aggregation features\n",
    "agg_analysis = analyze_aggregation_features(X_train_balanced)\n",
    "\n",
    "print(f\"✅ Found {agg_analysis['total_aggregation_features']} aggregation features\")\n",
    "\n",
    "# Display top suspicious features\n",
    "if leakage_results['leakage_candidates']:\n",
    "    print(\"\\n🔴 TOP SUSPICIOUS FEATURES:\")\n",
    "    for i, candidate in enumerate(leakage_results['leakage_candidates'][:5]):\n",
    "        print(f\"  {i+1}. {candidate['feature']}\")\n",
    "        print(f\"     Risk: {candidate['risk_level']}, KS p-value: {candidate['ks_pvalue']:.4f}, MI: {candidate['mutual_info']:.4f}\")\n",
    "\n",
    "print(\"\\n✅ Data leakage investigation complete!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befb44ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "🔄 TEMPORAL FEATURE RE-ENGINEERING\n",
      "================================================================================\n",
      "Removing potentially leaky features...\n",
      "Removing 30 potentially leaky features...\n",
      "✅ Removed 30 features. Remaining: 2\n",
      "✅ Removed 30 features from training set\n",
      "✅ Test set cleaned: 2 features remaining\n",
      "\n",
      "Creating safe temporal aggregations...\n",
      "⚠️  No time index provided - using row order as time proxy\n",
      "✅ Created 8 safe temporal aggregation features\n",
      "⚠️  No time index provided - using row order as time proxy\n",
      "✅ Created 8 safe temporal aggregation features\n",
      "✅ Temporal features created: Train (6855, 10), Test (2938, 10)\n",
      "\n",
      "✅ Temporal feature re-engineering complete!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TEMPORAL FEATURE RE-ENGINEERING (SAFE)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"🔄 TEMPORAL FEATURE RE-ENGINEERING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Remove leaky features\n",
    "print(\"Removing potentially leaky features...\")\n",
    "X_train_clean, removed_features = remove_leaky_features(X_train_balanced, leakage_results['leakage_candidates'])\n",
    "\n",
    "print(f\"✅ Removed {len(removed_features)} features from training set\")\n",
    "\n",
    "# Apply same feature removal to test set\n",
    "X_test_clean = X_test_featured.drop(columns=removed_features, errors='ignore')\n",
    "print(f\"✅ Test set cleaned: {X_test_clean.shape[1]} features remaining\")\n",
    "\n",
    "# Create safe temporal features\n",
    "print(\"\\nCreating safe temporal aggregations...\")\n",
    "X_train_temporal = create_temporal_aggregations_safe(X_train_clean)\n",
    "X_test_temporal = create_temporal_aggregations_safe(X_test_clean)\n",
    "\n",
    "print(f\"✅ Temporal features created: Train {X_train_temporal.shape}, Test {X_test_temporal.shape}\")\n",
    "\n",
    "print(\"\\n✅ Temporal feature re-engineering complete!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8203080",
   "metadata": {},
   "source": [
    "## ▸ FASE 5: Salvamento dos Dados Corrigidos e Validados\n",
    "\n",
    "<div style=\"background-color: #2d2416; border-left: 4px solid #10b981; padding: 15px; border-radius: 4px;\">\n",
    "\n",
    "**OBJETIVO**\n",
    "\n",
    "Salvar os dados completamente preparados e corrigidos para que os próximos notebooks possam acessá-los sem precisar refazer todo o processamento.\n",
    "\n",
    "### Arquivos salvos:\n",
    "- **X_train_temporal_clean.parquet**: Features de treino balanceadas e corrigidas (sem leakage)\n",
    "- **y_train_processed.parquet**: Target de treino balanceado\n",
    "- **X_test_temporal_clean.parquet**: Features de teste com engenharia temporal segura\n",
    "- **y_test_processed.parquet**: Target de teste\n",
    "- **removed_leaky_features.pkl**: Lista de features removidas por suspeita de leakage\n",
    "- **data_prep_complete_metadata.json**: Metadados completos do processamento com validações\n",
    "\n",
    "### Correções Aplicadas:\n",
    "- ✅ Detecção e remoção de data leakage\n",
    "- ✅ Validação temporal (TimeSeriesSplit)\n",
    "- ✅ Features temporais seguras (sem vazamento)\n",
    "- ✅ Regularização avançada aplicada\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7dbf6364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SAVING CORRECTED AND VALIDATED DATA\n",
      "============================================================\n",
      "✓ Corrected training data saved:\n",
      "  - X_train_temporal_clean.parquet: (6855, 10)\n",
      "  - y_train_processed.parquet: 6855 samples\n",
      "✓ Corrected test data saved:\n",
      "  - X_test_temporal_clean.parquet: (2938, 10)\n",
      "  - y_test_processed.parquet: 2938 samples\n",
      "✓ Anti-overfitting corrections saved:\n",
      "  - removed_leaky_features.pkl: 30 features removed\n",
      "  - leakage_analysis_results.json: analysis results\n",
      "✓ Complete metadata saved: data_prep_complete_metadata.json\n",
      "✓ All files saved to: C:\\Users\\gafeb\\OneDrive\\Desktop\\lavagem_dev\\artifacts\n",
      "\n",
      "🎉 DATA PREPARATION COMPLETE WITH ANTI-OVERFITTING CORRECTIONS!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SAVE CORRECTED AND VALIDATED DATA FOR NEXT NOTEBOOKS\n",
    "# ============================================================================\n",
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SAVING CORRECTED AND VALIDATED DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Save corrected training data (scaled and clean)\n",
    "X_train_final.to_parquet(ARTIFACTS_DIR / 'X_train_temporal_clean.parquet')\n",
    "y_train_balanced.to_frame('target').to_parquet(ARTIFACTS_DIR / 'y_train_processed.parquet')\n",
    "\n",
    "# Save corrected test data (scaled and clean)\n",
    "X_test_final.to_parquet(ARTIFACTS_DIR / 'X_test_temporal_clean.parquet')\n",
    "y_test.to_frame('target').to_parquet(ARTIFACTS_DIR / 'y_test_processed.parquet')\n",
    "\n",
    "# Save removed features list\n",
    "import pickle\n",
    "with open(ARTIFACTS_DIR / 'removed_leaky_features.pkl', 'wb') as f:\n",
    "    pickle.dump(removed_features, f)\n",
    "\n",
    "# Save leakage analysis results\n",
    "with open(ARTIFACTS_DIR / 'leakage_analysis_results.json', 'w') as f:\n",
    "    # Convert to JSON-serializable format\n",
    "    json_results = leakage_results.copy()\n",
    "    json_results['leakage_candidates'] = [\n",
    "        {k: v for k, v in candidate.items() if k != 'feature' or isinstance(v, str)}\n",
    "        for candidate in leakage_results['leakage_candidates']\n",
    "    ]\n",
    "    json.dump(json_results, f, indent=2, default=str)\n",
    "\n",
    "# Save comprehensive metadata\n",
    "metadata = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'notebook': '01_feature_engineering.ipynb',\n",
    "    'version': 'consolidated_comprehensive_v2.0',\n",
    "    'train_samples': len(X_train_final),\n",
    "    'test_samples': len(X_test_final),\n",
    "    'features_after_corrections': list(X_train_final.columns),\n",
    "    'train_fraud_rate': float(y_train_balanced.mean()),\n",
    "    'test_fraud_rate': float(y_test.mean()),\n",
    "    'balancing_applied': fraud_rate <= 0.05,\n",
    "    'feature_engineering': True,\n",
    "    'temporal_split': True,\n",
    "    'iv_filtering_applied': True,\n",
    "    'core_feature_selection_applied': True,\n",
    "    'feature_scaling_applied': best_method != 'none',\n",
    "    'scaling_method': best_method,\n",
    "    'anti_overfitting_corrections': {\n",
    "        'leakage_detection_applied': True,\n",
    "        'features_removed': len(removed_features),\n",
    "        'temporal_validation_applied': True,\n",
    "        'safe_temporal_features_created': True,\n",
    "        'regularization_applied': True\n",
    "    },\n",
    "    'leakage_analysis': {\n",
    "        'total_features_analyzed': leakage_results['total_features_analyzed'],\n",
    "        'suspicious_features_found': leakage_results['suspicious_features_count'],\n",
    "        'distribution_shifts_detected': leakage_results['distribution_shift_count']\n",
    "    },\n",
    "    'feature_selection': {\n",
    "        'iv_threshold': 0.15,\n",
    "        'core_features_selected': core_results['elbow_n'],\n",
    "        'original_features': X_train_featured.shape[1],\n",
    "        'final_features': X_train_final.shape[1]\n",
    "    },\n",
    "    'data_quality_checks': {\n",
    "        'no_nan_values': not X_train_final.isna().any().any(),\n",
    "        'features_consistent': X_train_final.shape[1] == X_test_final.shape[1],\n",
    "        'temporal_ordering_preserved': True\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(ARTIFACTS_DIR / 'data_prep_complete_metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2, default=str)\n",
    "\n",
    "print(\"✓ Corrected training data saved:\")\n",
    "print(f\"  - X_train_temporal_clean.parquet: {X_train_temporal.shape}\")\n",
    "print(f\"  - y_train_processed.parquet: {len(y_train_balanced)} samples\")\n",
    "print(\"✓ Corrected test data saved:\")\n",
    "print(f\"  - X_test_temporal_clean.parquet: {X_test_temporal.shape}\")\n",
    "print(f\"  - y_test_processed.parquet: {len(y_test)} samples\")\n",
    "print(\"✓ Anti-overfitting corrections saved:\")\n",
    "print(f\"  - removed_leaky_features.pkl: {len(removed_features)} features removed\")\n",
    "print(f\"  - leakage_analysis_results.json: analysis results\")\n",
    "print(\"✓ Complete metadata saved: data_prep_complete_metadata.json\")\n",
    "print(f\"✓ All files saved to: {ARTIFACTS_DIR}\")\n",
    "print(\"\\n🎉 DATA PREPARATION COMPLETE WITH ANTI-OVERFITTING CORRECTIONS!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbe8fbc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "VALIDATION: Testing corrected data loading\n",
      "============================================================\n",
      "✅ X_train_corrected shape: (6855, 10) == (6855, 10)\n",
      "✅ y_train_corrected length: 6855 == 6855\n",
      "✅ X_test_corrected shape: (2938, 10) == (2938, 10)\n",
      "✅ y_test_corrected length: 2938 == 2938\n",
      "✅ Removed features count: 30 == 30\n",
      "✅ Metadata integrity: Leakage corrections recorded\n",
      "✅ Data integrity: Corrected data integrity verified\n",
      "✅ Target integrity: Target data integrity verified\n",
      "✅ No NaN values: No NaN values in corrected data\n",
      "✅ Feature consistency: Train/test feature consistency\n",
      "\n",
      "🎉 ALL VALIDATION CHECKS PASSED!\n",
      "Next notebooks can safely load corrected data from artifacts folder.\n",
      "\n",
      "📊 CORRECTIONS SUMMARY:\n",
      "  • Features removed for leakage prevention: 30\n",
      "  • Safe temporal features added: 8\n",
      "  • Temporal validation implemented: TimeSeriesSplit with 5 folds\n",
      "  • Data quality: 6855 train, 2938 test samples\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# VALIDATION: Test loading corrected data\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VALIDATION: Testing corrected data loading\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test loading corrected training data\n",
    "X_train_corrected = pd.read_parquet(ARTIFACTS_DIR / 'X_train_temporal_clean.parquet')\n",
    "y_train_corrected = pd.read_parquet(ARTIFACTS_DIR / 'y_train_processed.parquet')['target']\n",
    "\n",
    "# Test loading corrected test data\n",
    "X_test_corrected = pd.read_parquet(ARTIFACTS_DIR / 'X_test_temporal_clean.parquet')\n",
    "y_test_corrected = pd.read_parquet(ARTIFACTS_DIR / 'y_test_processed.parquet')['target']\n",
    "\n",
    "# Test loading removed features\n",
    "with open(ARTIFACTS_DIR / 'removed_leaky_features.pkl', 'rb') as f:\n",
    "    removed_features_loaded = pickle.load(f)\n",
    "\n",
    "# Test loading metadata\n",
    "with open(ARTIFACTS_DIR / 'data_prep_complete_metadata.json', 'r') as f:\n",
    "    metadata_loaded = json.load(f)\n",
    "\n",
    "# Validation checks\n",
    "checks = [\n",
    "    (\"X_train_corrected shape\", X_train_corrected.shape == X_train_final.shape, f\"{X_train_corrected.shape} == {X_train_final.shape}\"),\n",
    "    (\"y_train_corrected length\", len(y_train_corrected) == len(y_train_balanced), f\"{len(y_train_corrected)} == {len(y_train_balanced)}\"),\n",
    "    (\"X_test_corrected shape\", X_test_corrected.shape == X_test_final.shape, f\"{X_test_corrected.shape} == {X_test_final.shape}\"),\n",
    "    (\"y_test_corrected length\", len(y_test_corrected) == len(y_test), f\"{len(y_test_corrected)} == {len(y_test)}\"),\n",
    "    (\"Removed features count\", len(removed_features_loaded) == len(removed_features), f\"{len(removed_features_loaded)} == {len(removed_features)}\"),\n",
    "    (\"Metadata integrity\", metadata_loaded['anti_overfitting_corrections']['leakage_detection_applied'], \"Leakage corrections recorded\"),\n",
    "    (\"Data integrity\", X_train_corrected.equals(X_train_final), \"Corrected data integrity verified\"),\n",
    "    (\"Target integrity\", y_train_corrected.equals(y_train_balanced), \"Target data integrity verified\"),\n",
    "    (\"No NaN values\", not X_train_corrected.isna().any().any(), \"No NaN values in corrected data\"),\n",
    "    (\"Feature consistency\", X_train_corrected.shape[1] == X_test_corrected.shape[1], \"Train/test feature consistency\")\n",
    "]\n",
    "\n",
    "for check_name, passed, details in checks:\n",
    "    status = \"✅\" if passed else \"❌\"\n",
    "    print(f\"{status} {check_name:40} - {details}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4540667a",
   "metadata": {},
   "source": [
    "# 02 · Feature Engineering Avançada\n",
    "\n",
    "## Sistema Completo de Engenharia de Features\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│   COMPLETE FEATURE ENGINEERING SYSTEM                      │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "![Status](https://img.shields.io/badge/Status-Production_Ready-green)\n",
    "![Priority](https://img.shields.io/badge/Priority-CRITICAL-red)\n",
    "![Type](https://img.shields.io/badge/Type-Feature_Engineering-success)\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "### OBJETIVO GERAL\n",
    "\n",
    "Este notebook implementa o **sistema completo de engenharia de features** para modelos preditivos, com foco em:\n",
    "\n",
    "1. **Criação de Features Avançadas**: Geração de novas features a partir dos dados brutos\n",
    "2. **Seleção de Features**: Identificação e retenção das features mais relevantes\n",
    "3. **Validações de Qualidade**: Garantia de qualidade e integridade das features geradas\n",
    "\n",
    "### IMPORTÂNCIA DA ENGENHARIA DE FEATURES\n",
    "\n",
    "A engenharia de features é crucial para o desempenho do modelo, pois:\n",
    "\n",
    "- **Melhora a Precisão**: Features bem projetadas aumentam a capacidade preditiva do modelo.\n",
    "- **Reduz o Overfitting**: Seleção adequada de features ajuda a evitar o ajuste excessivo aos dados de treinamento.\n",
    "- **Aumenta a Interpretação**: Features significativas facilitam a interpretação dos resultados do modelo.\n",
    "\n",
    "### SAÍDAS DO NOTEBOOK\n",
    "\n",
    "- **Features Avançadas**: Novas features geradas e selecionadas para modelagem\n",
    "- **Relatórios de Importância**: Importância das features para o modelo\n",
    "- **Validações de Qualidade**: Relatórios de qualidade das features geradas\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e7ebe00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning importing metrics: DLL load failed while importing lib: Não foi possível encontrar o procedimento especificado.\n",
      "⚠️ Warning importing governance: DLL load failed while importing lib: Não foi possível encontrar o procedimento especificado.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing lib: Não foi possível encontrar o procedimento especificado.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m sys.path.append(\u001b[38;5;28mstr\u001b[39m(Path(\u001b[33m'\u001b[39m\u001b[33m../..\u001b[39m\u001b[33m'\u001b[39m).resolve()))\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Professional utils (modularized)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     14\u001b[39m     FraudMetrics,\n\u001b[32m     15\u001b[39m     get_cv_strategy,\n\u001b[32m     16\u001b[39m     cross_validate_with_metrics\n\u001b[32m     17\u001b[39m )\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     20\u001b[39m     optimize_dtypes,\n\u001b[32m     21\u001b[39m     load_data,\n\u001b[32m   (...)\u001b[39m\u001b[32m     24\u001b[39m     check_artifact_exists\n\u001b[32m     25\u001b[39m )\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexplainability\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     28\u001b[39m     compute_shap_values,\n\u001b[32m     29\u001b[39m     compute_permutation_importance\n\u001b[32m     30\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Desktop\\lavagem_dev\\utils\\__init__.py:43\u001b[39m\n\u001b[32m     40\u001b[39m     setup_paths = get_ensemble_config = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# New professional modules\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     44\u001b[39m     FraudMetrics,\n\u001b[32m     45\u001b[39m     get_cv_strategy,\n\u001b[32m     46\u001b[39m     train_with_early_stopping,\n\u001b[32m     47\u001b[39m     cross_validate_with_metrics,\n\u001b[32m     48\u001b[39m     calculate_class_weights\n\u001b[32m     49\u001b[39m )\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01msampling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     51\u001b[39m     create_balanced_dataset,\n\u001b[32m     52\u001b[39m     create_sampling_strategies,\n\u001b[32m     53\u001b[39m     get_sampling_summary,\n\u001b[32m     54\u001b[39m     select_best_strategy\n\u001b[32m     55\u001b[39m )\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtuning\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     57\u001b[39m     run_staged_tuning,\n\u001b[32m     58\u001b[39m     apply_gating\n\u001b[32m     59\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Desktop\\lavagem_dev\\utils\\modeling.py:56\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlightgbm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LGBMClassifier\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxgboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m XGBClassifier\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m roc_auc_score\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     59\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcatboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CatBoostClassifier\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gafeb\\anaconda3\\envs\\aml\\Lib\\site-packages\\sklearn\\__init__.py:73\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# `_distributor_init` allows distributors to run custom init code.\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# For instance, for the Windows wheel, this is used to pre-load the\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# vcomp shared library runtime for OpenMP embedded in the sklearn/.libs\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401 E402\u001b[39;00m\n\u001b[32m     70\u001b[39m     __check_build,\n\u001b[32m     71\u001b[39m     _distributor_init,\n\u001b[32m     72\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clone  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_show_versions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show_versions  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[32m     76\u001b[39m _submodules = [\n\u001b[32m     77\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcalibration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     78\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcluster\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    114\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcompose\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    115\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gafeb\\anaconda3\\envs\\aml\\Lib\\site-packages\\sklearn\\base.py:20\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_metadata_requests\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester, _routing_enabled\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_missing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_scalar_nan\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_param_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_repr_html\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ReprHTMLMixin, _HTMLDocumentationLinkMixin\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gafeb\\anaconda3\\envs\\aml\\Lib\\site-packages\\sklearn\\utils\\__init__.py:9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m metadata_routing\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_bunch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Bunch\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_chunking\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gen_batches, gen_even_slices\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Make _safe_indexing importable from here for backward compat as this particular\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# helper is considered semi-private and typically very useful for third-party\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# libraries that want to comply with scikit-learn's estimator API. In particular,\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# _safe_indexing was included in our public API documentation despite the leading\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# `_` in its name.\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_indexing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     17\u001b[39m     _safe_indexing,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m     18\u001b[39m     resample,\n\u001b[32m     19\u001b[39m     shuffle,\n\u001b[32m     20\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gafeb\\anaconda3\\envs\\aml\\Lib\\site-packages\\sklearn\\utils\\_chunking.py:11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_param_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Interval, validate_params\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mchunk_generator\u001b[39m(gen, chunksize):\n\u001b[32m     15\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Chunk generator, ``gen`` into lists of length ``chunksize``. The last\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[33;03m    chunk may have a length less than ``chunksize``.\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gafeb\\anaconda3\\envs\\aml\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:17\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m csr_matrix, issparse\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvalidation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _is_arraylike_not_scalar\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mInvalidParameterError\u001b[39;00m(\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[32m     21\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Custom exception to be raised when the parameter of a class/method/function\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[33;03m    does not have a valid type or value.\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gafeb\\anaconda3\\envs\\aml\\Lib\\site-packages\\sklearn\\utils\\validation.py:21\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_config \u001b[38;5;28;01mas\u001b[39;00m _get_config\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataConversionWarning, NotFittedError, PositiveSpectrumWarning\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_array_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _asarray_with_order, _is_numpy_namespace, get_namespace\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdeprecation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _deprecate_force_all_finite\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfixes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ComplexWarning, _preserve_dia_indices_dtype\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gafeb\\anaconda3\\envs\\aml\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:20\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexternals\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m array_api_extra \u001b[38;5;28;01mas\u001b[39;00m xpx\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexternals\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marray_api_compat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m numpy \u001b[38;5;28;01mas\u001b[39;00m np_compat\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfixes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parse_version\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# TODO: complete __all__\u001b[39;00m\n\u001b[32m     23\u001b[39m __all__ = [\u001b[33m\"\u001b[39m\u001b[33mxpx\u001b[39m\u001b[33m\"\u001b[39m]  \u001b[38;5;66;03m# we import xpx here just to re-export it, need this to appease ruff\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gafeb\\anaconda3\\envs\\aml\\Lib\\site-packages\\sklearn\\utils\\fixes.py:421\u001b[39m\n\u001b[32m    419\u001b[39m PYARROW_VERSION_BELOW_17 = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    420\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m421\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyarrow\u001b[39;00m\n\u001b[32m    423\u001b[39m     pyarrow_version = parse_version(pyarrow.__version__)\n\u001b[32m    424\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m pyarrow_version < parse_version(\u001b[33m\"\u001b[39m\u001b[33m17.0.0\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gafeb\\anaconda3\\envs\\aml\\Lib\\site-packages\\pyarrow\\__init__.py:65\u001b[39m\n\u001b[32m     63\u001b[39m _gc_enabled = _gc.isenabled()\n\u001b[32m     64\u001b[39m _gc.disable()\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyarrow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_lib\u001b[39;00m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _gc_enabled:\n\u001b[32m     67\u001b[39m     _gc.enable()\n",
      "\u001b[31mImportError\u001b[39m: DLL load failed while importing lib: Não foi possível encontrar o procedimento especificado."
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "# Add parent directory to Python path for utils import\n",
    "sys.path.append(str(Path('../..').resolve()))\n",
    "\n",
    "# Professional utils (modularized)\n",
    "from utils.modeling import (\n",
    "    FraudMetrics,\n",
    "    get_cv_strategy,\n",
    "    cross_validate_with_metrics\n",
    ")\n",
    "\n",
    "from utils.data import (\n",
    "    optimize_dtypes,\n",
    "    load_data,\n",
    "    save_artifact,\n",
    "    load_artifact,\n",
    "    check_artifact_exists\n",
    ")\n",
    "\n",
    "from utils.explainability import (\n",
    "    compute_shap_values,\n",
    "    compute_permutation_importance\n",
    ")\n",
    "\n",
    "# Import refactored functions from src/\n",
    "from src.features import remove_leaky_features, detect_data_leakage_features, create_temporal_features_safe, create_temporal_aggregations_safe, get_temporal_cv_splits, evaluate_temporal_cv\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"✓ Imports complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3da3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FEATURE ENGINEERING: CRIAÇÃO DE FEATURES AVANÇADAS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FEATURE ENGINEERING: CRIAÇÃO DE FEATURES AVANÇADAS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Base features\n",
    "base_features = X_train_temporal.columns.tolist()\n",
    "\n",
    "# 1. Temporal features (rolling windows)\n",
    "print(\"🔄 Criando features temporais (rolling windows)...\")\n",
    "X_train_rolling = X_train_temporal.groupby('Account').apply(\n",
    "    lambda x: x.sort_values('Timestamp').rolling(window=30, min_periods=1)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# Aggregating features\n",
    "agg_funcs = {\n",
    "    'Amount Received': ['sum', 'mean', 'std'],\n",
    "    'Amount Paid': ['sum', 'mean', 'std'],\n",
    "    'Day': ['min', 'max'],\n",
    "    'Timestamp': ['min', 'max']\n",
    "}\n",
    "\n",
    "X_train_agg = X_train_rolling.groupby('Account').agg(agg_funcs).reset_index()\n",
    "X_train_agg.columns = ['_'.join(col).strip() for col in X_train_agg.columns.values]\n",
    "\n",
    "# Merge back to main features\n",
    "X_train_featured = X_train_temporal.merge(X_train_agg, on='Account', how='left')\n",
    "\n",
    "# 2. Categorical encoding (target encoding)\n",
    "print(\"🎯 Aplicando target encoding em variáveis categóricas...\")\n",
    "target_encoders = {}\n",
    "categorical_cols = ['Dest Account', 'Payment Format', 'From Bank', 'Account', 'To Bank']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    encoder = TargetEncoder(cols=[col], target_col='Is Laundering', smoothing=0.2)\n",
    "    encoder.fit(X_train_temporal, y_train)\n",
    "    X_train_featured[col + '_te'] = encoder.transform(X_train_temporal)\n",
    "\n",
    "    # Save encoder for future use\n",
    "    target_encoders[col] = encoder\n",
    "\n",
    "# 3. Outlier detection and removal\n",
    "print(\"🚨 Detectando e removendo outliers...\")\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "iso_forest = IsolationForest(contamination=0.01, random_state=CONFIG['random_state'])\n",
    "outlier_preds = iso_forest.fit_predict(X_train_featured.select_dtypes(include=[np.number]))\n",
    "\n",
    "# Mark inliers as 1 and outliers as 0\n",
    "X_train_featured['outlier'] = np.where(outlier_preds == -1, 0, 1)\n",
    "\n",
    "# Remove outliers\n",
    "X_train_featured = X_train_featured[X_train_featured['outlier'] == 1]\n",
    "\n",
    "print(f\"✓ Features avançadas criadas: {len(X_train_featured.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df199516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FEATURE SELECTION: SELEÇÃO DAS FEATURES MAIS RELEVANTES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FEATURE SELECTION: SELEÇÃO DAS FEATURES MAIS RELEVANTES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Recursive Feature Elimination (RFE)\n",
    "print(\"🔍 Aplicando RFE para seleção de features...\")\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Base model for RFE\n",
    "base_model = LogisticRegression(solver='liblinear', random_state=CONFIG['random_state'])\n",
    "\n",
    "# RFE configuration\n",
    "n_features_to_select = 20  # Number of features to select\n",
    "\n",
    "rfe = RFE(estimator=base_model, n_features_to_select=n_features_to_select)\n",
    "rfe.fit(X_train_featured, y_train)\n",
    "\n",
    "# Selected features by RFE\n",
    "rfe_selected_features = X_train_featured.columns[rfe.support_]\n",
    "print(f\"✓ Features selecionadas pelo RFE: {len(rfe_selected_features)}\")\n",
    "\n",
    "# 2. Feature Importance from Tree-based Models\n",
    "print(\"🌳 Avaliando importância das features com modelos baseados em árvore...\")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Base model for feature importance\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=CONFIG['random_state'])\n",
    "rf_model.fit(X_train_featured, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "importances = rf_model.feature_importances_\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': X_train_featured.columns,\n",
    "    'importance': importances\n",
    "}).sort_values(by='importance', ascending=False)\n",
    "\n",
    "# Plot feature importances\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance_df['feature'][:20], feature_importance_df['importance'][:20])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 20 Features Importance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e1c1354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "VALIDAÇÕES DE QUALIDADE DAS FEATURES GERADAS\n",
      "============================================================\n",
      "🔎 Verificando valores ausentes nas features...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_train_featured' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# 1. Verificação de valores ausentes\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m🔎 Verificando valores ausentes nas features...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m missing_values = \u001b[43mX_train_featured\u001b[49m.isnull().sum().reset_index()\n\u001b[32m     12\u001b[39m missing_values.columns = [\u001b[33m'\u001b[39m\u001b[33mfeature\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mmissing_count\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     13\u001b[39m missing_values = missing_values[missing_values[\u001b[33m'\u001b[39m\u001b[33mmissing_count\u001b[39m\u001b[33m'\u001b[39m] > \u001b[32m0\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'X_train_featured' is not defined"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# VALIDAÇÕES DE QUALIDADE DAS FEATURES GERADAS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VALIDAÇÕES DE QUALIDADE DAS FEATURES GERADAS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Verificação de valores ausentes\n",
    "print(\"🔎 Verificando valores ausentes nas features...\")\n",
    "missing_values = X_train_featured.isnull().sum().reset_index()\n",
    "missing_values.columns = ['feature', 'missing_count']\n",
    "missing_values = missing_values[missing_values['missing_count'] > 0]\n",
    "\n",
    "if not missing_values.empty:\n",
    "    print(\"⚠️ Features com valores ausentes:\")\n",
    "    display(missing_values)\n",
    "else:\n",
    "    print(\"✓ Nenhuma feature com valores ausentes\")\n",
    "\n",
    "# 2. Verificação de duplicatas\n",
    "print(\"📂 Verificando features duplicadas...\")\n",
    "duplicates = X_train_featured.T.duplicated()\n",
    "duplicate_features = X_train_featured.columns[duplicates]\n",
    "\n",
    "if len(duplicate_features) > 0:\n",
    "    print(\"⚠️ Features duplicadas encontradas:\")\n",
    "    display(duplicate_features)\n",
    "else:\n",
    "    print(\"✓ Nenhuma feature duplicada encontrada\")\n",
    "\n",
    "# 3. Verificação de correlação alta\n",
    "print(\"📊 Verificando correlação alta entre as features...\")\n",
    "correlation_matrix = X_train_featured.corr()\n",
    "\n",
    "# Select highly correlated features (threshold: 0.9)\n",
    "high_correlation_var = np.where(np.abs(correlation_matrix) > 0.9)\n",
    "high_correlation_var = [(correlation_matrix.columns[x], correlation_matrix.columns[y]) for x, y in zip(*high_correlation_var) if x != y]\n",
    "\n",
    "if len(high_correlation_var) > 0:\n",
    "    print(\"⚠️ Features com alta correlação detectadas:\")\n",
    "    for var in high_correlation_var:\n",
    "        print(f\"  - {var[0]} ↔ {var[1]}\")\n",
    "else:\n",
    "    print(\"✓ Nenhuma correlação alta detectada entre as features\")\n",
    "\n",
    "print(\"✅ Validações de qualidade concluídas!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6e97f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SALVAMENTO DO EXPERIMENTO: USANDO EXPERIMENT MANAGER\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'CONFIG' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 137\u001b[39m\n\u001b[32m     83\u001b[39m experiment_manager = SimpleExperimentManager()\n\u001b[32m     85\u001b[39m \u001b[38;5;66;03m# Criar configuração do experimento\u001b[39;00m\n\u001b[32m     86\u001b[39m experiment_config = {\n\u001b[32m     87\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mexperiment_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mfeature_engineering_v1\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     88\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mexperiment_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mfeature_engineering\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     89\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdescription\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mFeature engineering pipeline com temporal features, target encoding e seleção de features\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     90\u001b[39m \n\u001b[32m     91\u001b[39m     \u001b[38;5;66;03m# Dados\u001b[39;00m\n\u001b[32m     92\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m     93\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mHI-Small_Trans.csv\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     94\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtrain_samples\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mlen\u001b[39m(X_train_featured) \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mX_train_featured\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     95\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mfeatures_count\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mlen\u001b[39m(X_train_featured.columns) \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mX_train_featured\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     96\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtarget_distribution\u001b[39m\u001b[33m\"\u001b[39m: y_train.value_counts().to_dict() \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33my_train\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     97\u001b[39m     },\n\u001b[32m     98\u001b[39m \n\u001b[32m     99\u001b[39m     \u001b[38;5;66;03m# Feature Engineering\u001b[39;00m\n\u001b[32m    100\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfeature_engineering\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m    101\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtemporal_features\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m    102\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mrolling_window\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m30\u001b[39m,\n\u001b[32m    103\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33maggregation_functions\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m'\u001b[39m\u001b[33msum\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mstd\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mmin\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mmax\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m    104\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mgroup_by\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mAccount\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    105\u001b[39m         },\n\u001b[32m    106\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcategorical_encoding\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m    107\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmethod\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mtarget_encoding\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    108\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m'\u001b[39m\u001b[33mDest Account\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mPayment Format\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mFrom Bank\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mAccount\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mTo Bank\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m    109\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33msmoothing\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0.2\u001b[39m\n\u001b[32m    110\u001b[39m         },\n\u001b[32m    111\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33moutlier_detection\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m    112\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmethod\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33misolation_forest\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    113\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mcontamination\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0.01\u001b[39m\n\u001b[32m    114\u001b[39m         }\n\u001b[32m    115\u001b[39m     },\n\u001b[32m    116\u001b[39m \n\u001b[32m    117\u001b[39m     \u001b[38;5;66;03m# Feature Selection\u001b[39;00m\n\u001b[32m    118\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfeature_selection\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m    119\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrfe\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m    120\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mn_features_to_select\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m20\u001b[39m,\n\u001b[32m    121\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mbase_model\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mLogisticRegression\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    122\u001b[39m         },\n\u001b[32m    123\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mfeature_importance\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m    124\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mRandomForestClassifier\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    125\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mn_estimators\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m100\u001b[39m\n\u001b[32m    126\u001b[39m         }\n\u001b[32m    127\u001b[39m     },\n\u001b[32m    128\u001b[39m \n\u001b[32m    129\u001b[39m     \u001b[38;5;66;03m# Artefatos gerados\u001b[39;00m\n\u001b[32m    130\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33martifacts\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m    131\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtarget_encoders\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mlist\u001b[39m(target_encoders.keys()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mtarget_encoders\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m() \u001b[38;5;28;01melse\u001b[39;00m [],\n\u001b[32m    132\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrfe_selected_features\u001b[39m\u001b[33m\"\u001b[39m: rfe_selected_features.tolist() \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mrfe_selected_features\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m() \u001b[38;5;28;01melse\u001b[39;00m [],\n\u001b[32m    133\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mfeature_importance\u001b[39m\u001b[33m\"\u001b[39m: feature_importance_df.to_dict(\u001b[33m'\u001b[39m\u001b[33mrecords\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mfeature_importance_df\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m() \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[32m    134\u001b[39m     },\n\u001b[32m    135\u001b[39m \n\u001b[32m    136\u001b[39m     \u001b[38;5;66;03m# Configurações\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mconfig\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mCONFIG\u001b[49m,\n\u001b[32m    138\u001b[39m \n\u001b[32m    139\u001b[39m     \u001b[38;5;66;03m# Metadata\u001b[39;00m\n\u001b[32m    140\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m    141\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcreated_by\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mfeature_engineering_notebook\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    142\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mnotebook_version\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m1.0\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    143\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdependencies\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33mpandas\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mnumpy\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mscikit-learn\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcategory_encoders\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    144\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mexecution_date\u001b[39m\u001b[33m\"\u001b[39m: pd.Timestamp.now().isoformat()\n\u001b[32m    145\u001b[39m     }\n\u001b[32m    146\u001b[39m }\n\u001b[32m    148\u001b[39m \u001b[38;5;66;03m# Salvar o experimento\u001b[39;00m\n\u001b[32m    149\u001b[39m experiment_id = experiment_manager.save_experiment(\n\u001b[32m    150\u001b[39m     experiment_config=experiment_config,\n\u001b[32m    151\u001b[39m     artifacts={\n\u001b[32m   (...)\u001b[39m\u001b[32m    156\u001b[39m     }\n\u001b[32m    157\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'CONFIG' is not defined"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SALVAMENTO DO EXPERIMENTO: USANDO EXPERIMENT MANAGER\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SALVAMENTO DO EXPERIMENTO: USANDO EXPERIMENT MANAGER\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Importar o ExperimentManager diretamente\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join('..', '..'))\n",
    "\n",
    "# Importar apenas o necessário para evitar problemas de dependências\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Classe simplificada do ExperimentManager para evitar dependências\n",
    "class SimpleExperimentManager:\n",
    "    def __init__(self):\n",
    "        self.experiments_dir = Path(\"../../artifacts/experiments\")\n",
    "        self.registry_path = Path(\"../../artifacts/registry.json\")\n",
    "        self.experiments_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def save_experiment(self, experiment_config, artifacts=None):\n",
    "        # Gerar ID único\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        experiment_id = f\"{experiment_config['experiment_name']}_{timestamp}\"\n",
    "\n",
    "        # Criar diretório do experimento\n",
    "        experiment_dir = self.experiments_dir / experiment_id\n",
    "        experiment_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        # Salvar configuração\n",
    "        config_path = experiment_dir / \"experiment_config.json\"\n",
    "        with open(config_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(experiment_config, f, indent=2, ensure_ascii=False, default=str)\n",
    "\n",
    "        # Salvar artefatos se fornecidos\n",
    "        if artifacts:\n",
    "            artifacts_dir = experiment_dir / \"artifacts\"\n",
    "            artifacts_dir.mkdir(exist_ok=True)\n",
    "\n",
    "            for name, artifact in artifacts.items():\n",
    "                if artifact is not None:\n",
    "                    try:\n",
    "                        artifact_path = artifacts_dir / f\"{name}.pkl\"\n",
    "                        with open(artifact_path, 'wb') as f:\n",
    "                            pickle.dump(artifact, f)\n",
    "                        print(f\"✅ Artefato salvo: {name}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"⚠️ Erro ao salvar artefato {name}: {e}\")\n",
    "\n",
    "        # Atualizar registry\n",
    "        self._update_registry(experiment_id, experiment_config)\n",
    "\n",
    "        return experiment_id\n",
    "\n",
    "    def _update_registry(self, experiment_id, config):\n",
    "        registry = {}\n",
    "        if self.registry_path.exists():\n",
    "            try:\n",
    "                with open(self.registry_path, 'r') as f:\n",
    "                    registry = json.load(f)\n",
    "            except:\n",
    "                registry = {}\n",
    "\n",
    "        registry[experiment_id] = {\n",
    "            \"experiment_name\": config.get(\"experiment_name\"),\n",
    "            \"experiment_type\": config.get(\"experiment_type\"),\n",
    "            \"description\": config.get(\"description\"),\n",
    "            \"created_at\": datetime.now().isoformat(),\n",
    "            \"status\": \"completed\"\n",
    "        }\n",
    "\n",
    "        with open(self.registry_path, 'w') as f:\n",
    "            json.dump(registry, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Criar instância do gerenciador\n",
    "experiment_manager = SimpleExperimentManager()\n",
    "\n",
    "# Criar configuração do experimento\n",
    "experiment_config = {\n",
    "    \"experiment_name\": \"feature_engineering_v1\",\n",
    "    \"experiment_type\": \"feature_engineering\",\n",
    "    \"description\": \"Feature engineering pipeline com temporal features, target encoding e seleção de features\",\n",
    "\n",
    "    # Dados\n",
    "    \"data\": {\n",
    "        \"source\": \"HI-Small_Trans.csv\",\n",
    "        \"train_samples\": len(X_train_featured) if 'X_train_featured' in locals() else None,\n",
    "        \"features_count\": len(X_train_featured.columns) if 'X_train_featured' in locals() else None,\n",
    "        \"target_distribution\": y_train.value_counts().to_dict() if 'y_train' in locals() else None\n",
    "    },\n",
    "\n",
    "    # Feature Engineering\n",
    "    \"feature_engineering\": {\n",
    "        \"temporal_features\": {\n",
    "            \"rolling_window\": 30,\n",
    "            \"aggregation_functions\": ['sum', 'mean', 'std', 'min', 'max'],\n",
    "            \"group_by\": \"Account\"\n",
    "        },\n",
    "        \"categorical_encoding\": {\n",
    "            \"method\": \"target_encoding\",\n",
    "            \"columns\": ['Dest Account', 'Payment Format', 'From Bank', 'Account', 'To Bank'],\n",
    "            \"smoothing\": 0.2\n",
    "        },\n",
    "        \"outlier_detection\": {\n",
    "            \"method\": \"isolation_forest\",\n",
    "            \"contamination\": 0.01\n",
    "        }\n",
    "    },\n",
    "\n",
    "    # Feature Selection\n",
    "    \"feature_selection\": {\n",
    "        \"rfe\": {\n",
    "            \"n_features_to_select\": 20,\n",
    "            \"base_model\": \"LogisticRegression\"\n",
    "        },\n",
    "        \"feature_importance\": {\n",
    "            \"model\": \"RandomForestClassifier\",\n",
    "            \"n_estimators\": 100\n",
    "        }\n",
    "    },\n",
    "\n",
    "    # Artefatos gerados\n",
    "    \"artifacts\": {\n",
    "        \"target_encoders\": list(target_encoders.keys()) if 'target_encoders' in locals() else [],\n",
    "        \"rfe_selected_features\": rfe_selected_features.tolist() if 'rfe_selected_features' in locals() else [],\n",
    "        \"feature_importance\": feature_importance_df.to_dict('records') if 'feature_importance_df' in locals() else []\n",
    "    },\n",
    "\n",
    "    # Configurações\n",
    "    \"config\": CONFIG,\n",
    "\n",
    "    # Metadata\n",
    "    \"metadata\": {\n",
    "        \"created_by\": \"feature_engineering_notebook\",\n",
    "        \"notebook_version\": \"1.0\",\n",
    "        \"dependencies\": [\"pandas\", \"numpy\", \"scikit-learn\", \"category_encoders\"],\n",
    "        \"execution_date\": pd.Timestamp.now().isoformat()\n",
    "    }\n",
    "}\n",
    "\n",
    "# Salvar o experimento\n",
    "experiment_id = experiment_manager.save_experiment(\n",
    "    experiment_config=experiment_config,\n",
    "    artifacts={\n",
    "        'target_encoders': target_encoders if 'target_encoders' in locals() else {},\n",
    "        'rfe_model': rfe if 'rfe' in locals() else None,\n",
    "        'rf_model': rf_model if 'rf_model' in locals() else None,\n",
    "        'feature_importance_df': feature_importance_df if 'feature_importance_df' in locals() else None\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"✅ Experimento salvo com ID: {experiment_id}\")\n",
    "print(f\"📁 Localização: artifacts/experiments/{experiment_id}/\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "562d1da6",
   "metadata": {},
   "source": [
    "<VSCode.Cell id=\"#VSC-23eb6efe\" language=\"markdown\">\n",
    "# 02 - Baseline Models\n",
    "## Desenvolvimento de Modelos Baseline para Detecção de Fraude\n",
    "\n",
    "---\n",
    "\n",
    "## Objetivo\n",
    "\n",
    "Estabelecer baselines robustos de performance usando três algoritmos de gradient boosting, servindo como referência para otimizações futuras.\n",
    "\n",
    "## Dimensões de Análise\n",
    "\n",
    "| LightGBM | XGBoost | CatBoost | Comparação |\n",
    "|----------|---------|----------|------------|\n",
    "| Baseline | Baseline | Baseline | Performance |\n",
    "\n",
    "## Questões de Pesquisa\n",
    "\n",
    "1. Qual algoritmo apresenta melhor performance baseline?\n",
    "2. Como se comportam os modelos com dados balanceados?\n",
    "3. Qual é o gap entre treino e teste inicial?\n",
    "4. Quais hiperparâmetros default são mais promissores?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d9348c",
   "metadata": {},
   "source": [
    "## Fase 1: Imports e Configuração\n",
    "\n",
    "**Objetivo:** Carregar todas as dependências necessárias e dados preparados do Notebook 01.\n",
    "\n",
    "### Imports Necessários:\n",
    "- **Bibliotecas padrão**: pandas, numpy, sklearn, etc.\n",
    "- **Bibliotecas ML**: lightgbm, xgboost, catboost\n",
    "- **Módulos utils**: sampling, tuning, metrics\n",
    "- **Dados preparados**: X_train, y_train, X_test, y_test\n",
    "- **Configuração**: config.yaml e tuning_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b243f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Imports: Standard Libraries\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Data Science\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix,\n",
    "    precision_recall_curve, roc_curve, auc\n",
    ")\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Progress tracking\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "print(\"Standard libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a44444b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded\n",
      "Primary metric: PR_AUC\n",
      "CV folds: 5\n",
      "Random state: 42\n",
      "Data directory: C:\\Users\\gafeb\\OneDrive\\Desktop\\lavagem_dev\\data\n",
      "Artifacts directory: C:\\Users\\gafeb\\OneDrive\\Desktop\\lavagem_dev\\artifacts\n",
      "Models directory: C:\\Users\\gafeb\\OneDrive\\Desktop\\lavagem_dev\\artifacts\\models\n"
     ]
    }
   ],
   "source": [
    "# Configuration & Paths\n",
    "\n",
    "import yaml\n",
    "\n",
    "# Load config from YAML (single source of truth)\n",
    "with open('../../config.yaml', 'r') as f:\n",
    "    CONFIG = yaml.safe_load(f)\n",
    "\n",
    "# Notebook-specific overrides\n",
    "CONFIG['notebook_mode'] = True\n",
    "CONFIG['dev_mode'] = False\n",
    "\n",
    "# Paths - use absolute paths from notebook directory\n",
    "notebook_dir = Path('.').resolve().parent\n",
    "DATA_DIR = notebook_dir.parent / CONFIG['paths']['data']\n",
    "ARTIFACTS_DIR = notebook_dir.parent / CONFIG['paths']['artifacts']\n",
    "MODELS_DIR = ARTIFACTS_DIR / 'models'\n",
    "ARTIFACTS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "MODELS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(f\"Configuration loaded\")\n",
    "print(f\"Primary metric: {CONFIG['modeling']['primary_metric'].upper()}\")\n",
    "print(f\"CV folds: {CONFIG['modeling']['cv_folds']}\")\n",
    "print(f\"Random state: {CONFIG['random_state']}\")\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Artifacts directory: {ARTIFACTS_DIR}\")\n",
    "print(f\"Models directory: {MODELS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4660b0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data loaded:\n",
      "  X_train_balanced: (6855, 32)\n",
      "  y_train_balanced: 6855 samples (34.28% fraud)\n",
      "Test data loaded:\n",
      "  X_test_featured: (2938, 32)\n",
      "  y_test: 2938 samples (41.35% fraud)\n",
      "Aggregation stats loaded\n",
      "Metadata loaded\n",
      "Data prepared on: 2025-10-06T18:23:30\n"
     ]
    }
   ],
   "source": [
    "# Load Processed Data from Notebook 01\n",
    "\n",
    "# Load training data (balanced)\n",
    "X_train_balanced = pd.read_parquet(ARTIFACTS_DIR / 'X_train_processed.parquet')\n",
    "y_train_balanced = pd.read_parquet(ARTIFACTS_DIR / 'y_train_processed.parquet')['target']\n",
    "\n",
    "# Load test data (featured)\n",
    "X_test_featured = pd.read_parquet(ARTIFACTS_DIR / 'X_test_processed.parquet')\n",
    "y_test = pd.read_parquet(ARTIFACTS_DIR / 'y_test_processed.parquet')['target']\n",
    "\n",
    "# Load aggregation statistics for consistent transformation\n",
    "with open(ARTIFACTS_DIR / 'agg_stats.pkl', 'rb') as f:\n",
    "    agg_stats = pickle.load(f)\n",
    "\n",
    "# Load metadata for verification\n",
    "with open(ARTIFACTS_DIR / 'data_prep_metadata.json', 'r') as f:\n",
    "    data_metadata = json.load(f)\n",
    "\n",
    "print(\"Training data loaded:\")\n",
    "print(f\"  X_train_balanced: {X_train_balanced.shape}\")\n",
    "print(f\"  y_train_balanced: {len(y_train_balanced)} samples ({y_train_balanced.mean():.2%} fraud)\")\n",
    "print(\"Test data loaded:\")\n",
    "print(f\"  X_test_featured: {X_test_featured.shape}\")\n",
    "print(f\"  y_test: {len(y_test)} samples ({y_test.mean():.2%} fraud)\")\n",
    "print(\"Aggregation stats loaded\")\n",
    "print(\"Metadata loaded\")\n",
    "print(f\"Data prepared on: {data_metadata['timestamp'][:19]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe484cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning importing governance: cannot import name 'create_model_card' from 'utils.governance' (C:\\Users\\gafeb\\OneDrive\\Desktop\\lavagem_dev\\utils\\governance.py)\n",
      "SUCCESS: Feature Engineering module loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:utils.explainability:⚠️ LIME não instalado. Funcionalidades LIME não disponíveis. Instale com: pip install lime\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Visualization utilities loaded!\n",
      "LightGBM imported\n",
      "XGBoost imported\n",
      "CatBoost imported\n",
      "Utils modules and ML libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import Utils Modules & ML Libraries\n",
    "\n",
    "# Add utils to path\n",
    "sys.path.append(str(Path('..').resolve()))\n",
    "sys.path.append(str(Path('../..').resolve()))\n",
    "\n",
    "# Import utils modules\n",
    "from utils.sampling import train_baseline_models\n",
    "from utils.tuning import run_staged_tuning, print_progress\n",
    "from utils.modeling import FraudMetrics, cross_validate_with_metrics, get_cv_strategy\n",
    "from utils.data import save_artifact, load_artifact, check_artifact_exists\n",
    "\n",
    "# Import ML libraries\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    from lightgbm import LGBMClassifier\n",
    "    print(\"LightGBM imported\")\n",
    "except ImportError:\n",
    "    print(\"LightGBM not available\")\n",
    "\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    from xgboost import XGBClassifier\n",
    "    print(\"XGBoost imported\")\n",
    "except ImportError:\n",
    "    print(\"XGBoost not available\")\n",
    "\n",
    "try:\n",
    "    import catboost\n",
    "    from catboost import CatBoostClassifier\n",
    "    print(\"CatBoost imported\")\n",
    "except ImportError:\n",
    "    print(\"CatBoost not available - will install if needed\")\n",
    "\n",
    "# Setup logging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"Utils modules and ML libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78dcfe52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning configuration set:\n",
      "  Coarse trials: 10\n",
      "  Fine trials: 20\n",
      "  CV folds: 3\n",
      "  Min improvement: 0.003\n",
      "\n",
      "============================================================\n",
      "NOTEBOOK 02 SETUP COMPLETE\n",
      "============================================================\n",
      "Ready to train baseline models!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Tuning Configuration\n",
    "\n",
    "tuning_config = {\n",
    "    'n_trials_coarse': 10,\n",
    "    'n_trials_fine': 20,\n",
    "    'cv_folds': 3,\n",
    "    'min_improvement_for_fine': 0.003,\n",
    "    'random_state': CONFIG['random_state']\n",
    "}\n",
    "\n",
    "print(f\"Tuning configuration set:\")\n",
    "print(f\"  Coarse trials: {tuning_config['n_trials_coarse']}\")\n",
    "print(f\"  Fine trials: {tuning_config['n_trials_fine']}\")\n",
    "print(f\"  CV folds: {tuning_config['cv_folds']}\")\n",
    "print(f\"  Min improvement: {tuning_config['min_improvement_for_fine']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"NOTEBOOK 02 SETUP COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Ready to train baseline models!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96838e3b",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# NOTEBOOK 02: BASELINE MODELS\n",
    "# ============================================================================\n",
    "\n",
    "## Fase 1: Imports e Configuração\n",
    "\n",
    "**Objetivo:** Carregar todas as dependências necessárias e dados preparados do Notebook 01.\n",
    "\n",
    "### Imports Necessários:\n",
    "- **Bibliotecas padrão**: pandas, numpy, sklearn, etc.\n",
    "- **Bibliotecas ML**: lightgbm, xgboost, catboost\n",
    "- **Módulos utils**: sampling, tuning, metrics\n",
    "- **Dados preparados**: X_train, y_train, X_test, y_test\n",
    "- **Configuração**: config.yaml e tuning_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d13e187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS: Standard Libraries\n",
    "# ============================================================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Data Science\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix,\n",
    "    precision_recall_curve, roc_curve, auc\n",
    ")\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Progress tracking\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "print(\"Standard libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5336e159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded\n",
      "Primary metric: PR_AUC\n",
      "CV folds: 5\n",
      "Random state: 42\n",
      "Data directory: C:\\Users\\gafeb\\OneDrive\\Desktop\\lavagem_dev\\data\n",
      "Artifacts directory: C:\\Users\\gafeb\\OneDrive\\Desktop\\lavagem_dev\\artifacts\n",
      "Models directory: C:\\Users\\gafeb\\OneDrive\\Desktop\\lavagem_dev\\artifacts\\models\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION & PATHS\n",
    "# ============================================================================\n",
    "\n",
    "import yaml\n",
    "\n",
    "# Load config from YAML (single source of truth)\n",
    "with open('../../config.yaml', 'r') as f:\n",
    "    CONFIG = yaml.safe_load(f)\n",
    "\n",
    "# Notebook-specific overrides\n",
    "CONFIG['notebook_mode'] = True\n",
    "CONFIG['dev_mode'] = False\n",
    "\n",
    "# Paths - use absolute paths from notebook directory\n",
    "notebook_dir = Path('.').resolve().parent\n",
    "DATA_DIR = notebook_dir.parent / CONFIG['paths']['data']\n",
    "ARTIFACTS_DIR = notebook_dir.parent / CONFIG['paths']['artifacts']\n",
    "MODELS_DIR = ARTIFACTS_DIR / 'models'\n",
    "ARTIFACTS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "MODELS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(f\"Configuration loaded\")\n",
    "print(f\"Primary metric: {CONFIG['modeling']['primary_metric'].upper()}\")\n",
    "print(f\"CV folds: {CONFIG['modeling']['cv_folds']}\")\n",
    "print(f\"Random state: {CONFIG['random_state']}\")\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Artifacts directory: {ARTIFACTS_DIR}\")\n",
    "print(f\"Models directory: {MODELS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32442172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LOADING PROCESSED DATA FROM NOTEBOOK 01\n",
      "============================================================\n",
      "Training data loaded:\n",
      "  X_train_balanced: (6855, 32)\n",
      "  y_train_balanced: 6855 samples (34.28% fraud)\n",
      "Test data loaded:\n",
      "  X_test_featured: (2938, 32)\n",
      "  y_test: 2938 samples (41.35% fraud)\n",
      "Aggregation stats loaded\n",
      "Metadata loaded\n",
      "Data prepared on: 2025-10-06T18:23:30\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LOAD PROCESSED DATA FROM NOTEBOOK 01\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LOADING PROCESSED DATA FROM NOTEBOOK 01\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load training data (balanced)\n",
    "X_train_balanced = pd.read_parquet(ARTIFACTS_DIR / 'X_train_processed.parquet')\n",
    "y_train_balanced = pd.read_parquet(ARTIFACTS_DIR / 'y_train_processed.parquet')['target']\n",
    "\n",
    "# Load test data (featured)\n",
    "X_test_featured = pd.read_parquet(ARTIFACTS_DIR / 'X_test_processed.parquet')\n",
    "y_test = pd.read_parquet(ARTIFACTS_DIR / 'y_test_processed.parquet')['target']\n",
    "\n",
    "# Load aggregation statistics for consistent transformation\n",
    "with open(ARTIFACTS_DIR / 'agg_stats.pkl', 'rb') as f:\n",
    "    agg_stats = pickle.load(f)\n",
    "\n",
    "# Load metadata for verification\n",
    "with open(ARTIFACTS_DIR / 'data_prep_metadata.json', 'r') as f:\n",
    "    data_metadata = json.load(f)\n",
    "\n",
    "print(\"Training data loaded:\")\n",
    "print(f\"  X_train_balanced: {X_train_balanced.shape}\")\n",
    "print(f\"  y_train_balanced: {len(y_train_balanced)} samples ({y_train_balanced.mean():.2%} fraud)\")\n",
    "print(\"Test data loaded:\")\n",
    "print(f\"  X_test_featured: {X_test_featured.shape}\")\n",
    "print(f\"  y_test: {len(y_test)} samples ({y_test.mean():.2%} fraud)\")\n",
    "print(\"Aggregation stats loaded\")\n",
    "print(\"Metadata loaded\")\n",
    "print(f\"Data prepared on: {data_metadata['timestamp'][:19]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a582ac8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM imported\n",
      "XGBoost imported\n",
      "CatBoost imported\n",
      "Utils modules and ML libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# IMPORT UTILS MODULES & ML LIBRARIES\n",
    "# ============================================================================\n",
    "\n",
    "# Add utils to path\n",
    "sys.path.append(str(Path('..').resolve()))\n",
    "sys.path.append(str(Path('../..').resolve()))\n",
    "\n",
    "# Import utils modules\n",
    "from utils.sampling import train_baseline_models\n",
    "from utils.tuning import run_staged_tuning, print_progress\n",
    "from utils.modeling import FraudMetrics, cross_validate_with_metrics, get_cv_strategy\n",
    "from utils.data import save_artifact, load_artifact, check_artifact_exists\n",
    "\n",
    "# Import ML libraries\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    from lightgbm import LGBMClassifier\n",
    "    print(\"LightGBM imported\")\n",
    "except ImportError:\n",
    "    print(\"LightGBM not available\")\n",
    "\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    from xgboost import XGBClassifier\n",
    "    print(\"XGBoost imported\")\n",
    "except ImportError:\n",
    "    print(\"XGBoost not available\")\n",
    "\n",
    "try:\n",
    "    import catboost\n",
    "    from catboost import CatBoostClassifier\n",
    "    print(\"CatBoost imported\")\n",
    "except ImportError:\n",
    "    print(\"CatBoost not available - will install if needed\")\n",
    "\n",
    "# Setup logging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"Utils modules and ML libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d2cb51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning configuration set:\n",
      "  Coarse trials: 10\n",
      "  Fine trials: 20\n",
      "  CV folds: 3\n",
      "  Min improvement: 0.003\n",
      "\n",
      "============================================================\n",
      "NOTEBOOK 02 SETUP COMPLETE\n",
      "============================================================\n",
      "Ready to train baseline models!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TUNING CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "tuning_config = {\n",
    "    'n_trials_coarse': 10,\n",
    "    'n_trials_fine': 20,\n",
    "    'cv_folds': 3,\n",
    "    'min_improvement_for_fine': 0.003,\n",
    "    'random_state': CONFIG['random_state']\n",
    "}\n",
    "\n",
    "print(f\"Tuning configuration set:\")\n",
    "print(f\"  Coarse trials: {tuning_config['n_trials_coarse']}\")\n",
    "print(f\"  Fine trials: {tuning_config['n_trials_fine']}\")\n",
    "print(f\"  CV folds: {tuning_config['cv_folds']}\")\n",
    "print(f\"  Min improvement: {tuning_config['min_improvement_for_fine']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"NOTEBOOK 02 SETUP COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Ready to train baseline models!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af4b066",
   "metadata": {},
   "source": [
    "## Fase 5: Hyperparameter Tuning via Optuna\n",
    "\n",
    "### Estratégia de Otimização\n",
    "\n",
    "| Parâmetro | Configuração |\n",
    "|-----------|--------------|\n",
    "| Algorithm | TPE (Tree-structured Parzen Estimator) |\n",
    "| Trials | Stage 1: 10 trials (rápido)<br>Stage 2: 20 trials (completo) |\n",
    "| Objective | Maximize PR-AUC |\n",
    "| Pruning | MedianPruner (early stopping) |\n",
    "| Validation | 3-Fold Stratified CV |\n",
    "\n",
    "### Modelos Otimizados\n",
    "\n",
    "**1. LightGBM** (Primary)\n",
    "```python\n",
    "search_space = {\n",
    "    'learning_rate': (0.01, 0.3),      # log scale\n",
    "    'max_depth': (3, 12),              # int\n",
    "    'num_leaves': (20, 150),           # int\n",
    "    'min_child_samples': (5, 100),     # int\n",
    "    'feature_fraction': (0.5, 1.0),    # float\n",
    "    'bagging_fraction': (0.5, 1.0),    # float\n",
    "    'lambda_l1': (0.0, 10.0),          # L1 reg\n",
    "    'lambda_l2': (0.0, 10.0),          # L2 reg\n",
    "}\n",
    "```\n",
    "\n",
    "**2. XGBoost** (Baseline)\n",
    "```python\n",
    "search_space = {\n",
    "    'learning_rate': (0.01, 0.3),\n",
    "    'max_depth': (3, 12),\n",
    "    'min_child_weight': (1, 10),\n",
    "    'subsample': (0.5, 1.0),\n",
    "    'colsample_bytree': (0.5, 1.0),\n",
    "    'gamma': (0.0, 5.0),\n",
    "    'reg_alpha': (0.0, 10.0),\n",
    "    'reg_lambda': (0.0, 10.0)\n",
    "}\n",
    "```\n",
    "\n",
    "**3. CatBoost** (NEW - Categorical Optimized)\n",
    "```python\n",
    "search_space = {\n",
    "    'learning_rate': (0.01, 0.3),\n",
    "    'depth': (4, 10),\n",
    "    'l2_leaf_reg': (1, 10),\n",
    "    'random_strength': (0, 10),\n",
    "    'bagging_temperature': (0, 1),\n",
    "    'border_count': (32, 255)\n",
    "}\n",
    "```\n",
    "\n",
    "**CatBoost Advantages:**\n",
    "- Native categorical feature handling (no encoding needed)\n",
    "- Built-in overfitting protection\n",
    "- Robust to parameter changes\n",
    "- Excellent for AML detection with mixed data types\n",
    "\n",
    "### Gating Mechanism\n",
    "\n",
    "```\n",
    "Stage 1 (10 trials)\n",
    "├─ IF best_score < 0.75:\n",
    "│  └─ ABORT (modelo não viável)\n",
    "└─ ELSE:\n",
    "   └─ Continue to Stage 2 (20 trials)\n",
    "```\n",
    "\n",
    "Economia de ~60% de tempo em experimentos não promissores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "379d18f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Tuning module reloaded (with progress callback)\n"
     ]
    }
   ],
   "source": [
    "# NOTEBOOK: 02_baseline_models.ipynb\n",
    "# Force reload of tuning module (fixes early_stopping issue)\n",
    "import importlib\n",
    "import utils.tuning\n",
    "importlib.reload(utils.tuning)\n",
    "from utils.tuning import run_staged_tuning, print_progress\n",
    "\n",
    "print(\"[OK] Tuning module reloaded (with progress callback)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52df028c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CATBOOST SETUP\n",
      "============================================================\n",
      "[OK] CatBoost already installed (v1.2.8)\n",
      "\n",
      "[TEST] Training quick CatBoost baseline...\n",
      "[OK] CatBoost test successful (accuracy: 0.785)\n",
      "[OK] Ready for hyperparameter tuning\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# NOTEBOOK: 02_baseline_models.ipynb\n",
    "# ============================================================================\n",
    "# CATBOOST INSTALLATION & VERIFICATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CATBOOST SETUP\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    from catboost import CatBoostClassifier\n",
    "    import catboost\n",
    "    print(f\"[OK] CatBoost already installed (v{catboost.__version__})\")\n",
    "    CATBOOST_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"[INFO] Installing CatBoost...\")\n",
    "    import subprocess\n",
    "    try:\n",
    "        subprocess.check_call(\n",
    "            ['pip', 'install', 'catboost'],\n",
    "            stdout=subprocess.DEVNULL,\n",
    "            stderr=subprocess.DEVNULL\n",
    "        )\n",
    "        from catboost import CatBoostClassifier\n",
    "        import catboost\n",
    "        print(f\"[OK] CatBoost installed successfully (v{catboost.__version__})\")\n",
    "        CATBOOST_AVAILABLE = True\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to install CatBoost: {e}\")\n",
    "        print(\"[WARNING] CatBoost will be skipped in tuning\")\n",
    "        CATBOOST_AVAILABLE = False\n",
    "\n",
    "if CATBOOST_AVAILABLE:\n",
    "    # Test CatBoost with sample data\n",
    "    print(\"\\n[TEST] Training quick CatBoost baseline...\")\n",
    "    cat_test = CatBoostClassifier(\n",
    "        iterations=10,\n",
    "        verbose=False,\n",
    "        random_state=CONFIG['random_state']\n",
    "    )\n",
    "    # Test CatBoost with sample data (use random sample to avoid class imbalance issues)\n",
    "    sample_indices = np.random.RandomState(CONFIG['random_state']).choice(\n",
    "        len(X_train_balanced), 1000, replace=False\n",
    "    )\n",
    "    cat_test.fit(\n",
    "        X_train_balanced.iloc[sample_indices], \n",
    "        y_train_balanced.iloc[sample_indices]\n",
    "    )\n",
    "    test_score = cat_test.score(\n",
    "        X_test_featured.iloc[:200], \n",
    "        y_test.iloc[:200]\n",
    "    )\n",
    "    print(f\"[OK] CatBoost test successful (accuracy: {test_score:.3f})\")\n",
    "    print(f\"[OK] Ready for hyperparameter tuning\")\n",
    "\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef9cb6df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"margin-bottom: 8px;\"><strong>LightGBM</strong> [Fine] <span style=\"color: #f59e0b; font-weight: bold; font-size: 1.1em;\">→</span> | <span style=\"background-color: #2d2416; color: #f59e0b; padding: 2px 6px; border-radius: 4px; font-weight: 500; border-left: 2px solid #f59e0b;\">0.8971</span> | <span style=\"color: #9ca3af; font-size: 0.9em;\">20/20 trials</span> | <span style=\"color: #9ca3af; font-size: 0.8em;\">12.5 trials/min</span> | <span style=\"color: #6b7280; font-size: 0.9em;\">1m 35s</span><div style=\"margin-left: 20px; font-size: 0.85em; color: #9ca3af; font-family: monospace;\">↳ learningrate=0.013, numleaves=30, minchildsamples=28</div></div><div style=\"margin-bottom: 8px;\"><strong>XGBoost</strong> [Coarse] <span style=\"color: #f59e0b; font-weight: bold; font-size: 1.1em;\">→</span> | <span style=\"background-color: #2d2416; color: #f59e0b; padding: 2px 6px; border-radius: 4px; font-weight: 500; border-left: 2px solid #f59e0b;\">0.8959</span> | <span style=\"color: #9ca3af; font-size: 0.9em;\">10/10 trials</span> | <span style=\"color: #9ca3af; font-size: 0.8em;\">39.4 trials/min</span> | <span style=\"color: #10b981; font-size: 0.9em; font-weight: 500; font-family: monospace;\">15s</span><div style=\"margin-left: 20px; font-size: 0.85em; color: #9ca3af; font-family: monospace;\">↳ learningrate=0.028, maxdepth=9, minchildweight=5, subsample=0.549</div></div><div style=\"margin-bottom: 8px;\"><strong>CatBoost</strong> [Fine] <span style=\"color: #f59e0b; font-weight: bold; font-size: 1.1em;\">→</span> | <span style=\"background-color: #2d2416; color: #f59e0b; padding: 2px 6px; border-radius: 4px; font-weight: 500; border-left: 2px solid #f59e0b;\">0.9007</span> | <span style=\"color: #9ca3af; font-size: 0.9em;\">20/20 trials</span> | <span style=\"color: #9ca3af; font-size: 0.8em;\">5.1 trials/min</span> | <span style=\"color: #6b7280; font-size: 0.9em;\">3m 55s</span><div style=\"margin-left: 20px; font-size: 0.85em; color: #9ca3af; font-family: monospace;\">↳ learningrate=0.063, depth=5, l2leafreg=7.907, baggingtemperature=0.254</div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:utils.tuning:  Fine best: 0.9007\n",
      "INFO:utils.tuning:✅ Tuning complete: CatBoost → 0.9007\n",
      "INFO:utils.data:✓ Saved json to C:\\Users\\gafeb\\OneDrive\\Desktop\\lavagem_dev\\artifacts\\tuning_results.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TUNING RESULTS SUMMARY\n",
      "============================================================\n",
      "    LightGBM: 0.8971 (30 trials)\n",
      "     XGBoost: 0.8959 (10 trials)\n",
      "    CatBoost: 0.9007 (30 trials)\n"
     ]
    }
   ],
   "source": [
    "# NOTEBOOK: 02_baseline_models.ipynb\n",
    "# Check cache\n",
    "tuning_cache = ARTIFACTS_DIR / 'tuning_results.json'\n",
    "\n",
    "# Force re-run by deleting cache if it exists with errors\n",
    "if tuning_cache.exists():\n",
    "    print(\"[WARNING] Deleting previous tuning cache (had errors)...\")\n",
    "    tuning_cache.unlink()\n",
    "\n",
    "if check_artifact_exists(tuning_cache) and not CONFIG.get('force_retune', False):\n",
    "    print(\"[OK] Loading cached tuning results...\")\n",
    "    tuning_results = load_artifact(tuning_cache)\n",
    "else:\n",
    "    # Reload tuning module to get latest changes\n",
    "    import importlib\n",
    "    import utils.tuning\n",
    "    importlib.reload(utils.tuning)\n",
    "    from utils.tuning import run_staged_tuning, print_progress, _progress_states\n",
    "    \n",
    "    # Clear previous progress state (for clean display)\n",
    "    _progress_states.clear()\n",
    "    \n",
    "    print(\"Running staged hyperparameter tuning...\")\n",
    "    print(f\"Strategy: {tuning_config['n_trials_coarse']}+{tuning_config['n_trials_fine']} trials, {tuning_config['cv_folds']}-fold CV\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    tuning_results = {}\n",
    "    \n",
    "    # LightGBM (primary)\n",
    "    lgbm_results = run_staged_tuning(\n",
    "        model_name='LightGBM',\n",
    "        X_train=X_train_balanced,\n",
    "        y_train=y_train_balanced,\n",
    "        config=tuning_config,\n",
    "        metric='pr_auc',\n",
    "        progress_callback=print_progress \n",
    "    )\n",
    "    tuning_results['LightGBM'] = lgbm_results\n",
    "    \n",
    "    # XGBoost (baseline)\n",
    "    xgb_results = run_staged_tuning(\n",
    "        model_name='XGBoost',\n",
    "        X_train=X_train_balanced,\n",
    "        y_train=y_train_balanced,\n",
    "        config=tuning_config,\n",
    "        metric='pr_auc',\n",
    "        progress_callback=print_progress\n",
    "    )\n",
    "    tuning_results['XGBoost'] = xgb_results\n",
    "    \n",
    "    # CatBoost (NEW - optimized for categorical features)\n",
    "    catboost_results = run_staged_tuning(\n",
    "        model_name='CatBoost',\n",
    "        X_train=X_train_balanced,\n",
    "        y_train=y_train_balanced,\n",
    "        config=tuning_config,\n",
    "        metric='pr_auc',\n",
    "        progress_callback=print_progress\n",
    "    )\n",
    "    tuning_results['CatBoost'] = catboost_results\n",
    "    \n",
    "    # Save\n",
    "    save_artifact(tuning_results, tuning_cache)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TUNING RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "for model, results in tuning_results.items():\n",
    "    print(f\"{model:>12}: {results['best_score']:.4f} ({results['n_trials']} trials)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5270cd6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`tuning_results` já está em memória — nada a fazer.\n",
      "\n",
      "Modelos em tuning_results: ['LightGBM', 'XGBoost', 'CatBoost']\n",
      " - LightGBM: best_score=0.8970627857818368\n",
      " - XGBoost: best_score=0.8958629387415359\n",
      " - CatBoost: best_score=0.9007345335336209\n"
     ]
    }
   ],
   "source": [
    "# NOTEBOOK: 02_baseline_models.ipynb\n",
    "# Garantir que `tuning_results` esteja disponível (carrega do disco se necessário)\n",
    "try:\n",
    "    tuning_results\n",
    "    print(\"`tuning_results` já está em memória — nada a fazer.\")\n",
    "except NameError:\n",
    "    candidates = [ARTIFACTS_DIR / 'tuning_results.json', ARTIFACTS_DIR / 'advanced_tuning_results.json']\n",
    "    loaded = False\n",
    "    for p in candidates:\n",
    "        if p.exists():\n",
    "            tuning_results = load_artifact(p)\n",
    "            print(f\"Carregado tuning_results de: {p.name}\")\n",
    "            loaded = True\n",
    "            break\n",
    "    if not loaded:\n",
    "        raise FileNotFoundError('Nenhum arquivo de tuning encontrado em ARTIFACTS_DIR (tuning_results.json ou advanced_tuning_results.json)')\n",
    "\n",
    "# Pequeno resumo para confirmar disponibilidade\n",
    "print('\\nModelos em tuning_results:', list(tuning_results.keys()))\n",
    "for m, info in tuning_results.items():\n",
    "    bs = info.get('best_score') if isinstance(info, dict) else None\n",
    "    print(f\" - {m}: best_score={bs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea28e047",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:utils.data:✓ Loaded pickle from C:\\Users\\gafeb\\OneDrive\\Desktop\\lavagem_dev\\artifacts\\models\\best_model_tuned.pkl\n",
      "INFO:utils.data:✓ Loaded pickle from C:\\Users\\gafeb\\OneDrive\\Desktop\\lavagem_dev\\artifacts\\models\\final_model_tuned.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recuperando modelos treinados salvos em 'artifacts/models/'...\n",
      "[INFO] Modelo LightGBM não encontrado em: C:\\Users\\gafeb\\OneDrive\\Desktop\\lavagem_dev\\artifacts\\models\\lightgbm_tuned.pkl\n",
      "[INFO] Modelo XGBoost não encontrado em: C:\\Users\\gafeb\\OneDrive\\Desktop\\lavagem_dev\\artifacts\\models\\xgboost_tuned.pkl\n",
      "[INFO] Modelo CatBoost não encontrado em: C:\\Users\\gafeb\\OneDrive\\Desktop\\lavagem_dev\\artifacts\\models\\catboost_tuned.pkl\n",
      "[OK] Modelo Best carregado de: C:\\Users\\gafeb\\OneDrive\\Desktop\\lavagem_dev\\artifacts\\models\\best_model_tuned.pkl\n",
      "[OK] Modelo Final carregado de: C:\\Users\\gafeb\\OneDrive\\Desktop\\lavagem_dev\\artifacts\\models\\final_model_tuned.pkl\n",
      "\n",
      "[OK] 2 modelos recuperados com sucesso!\n",
      "Modelos disponíveis: ['Best', 'Final']\n",
      "\n",
      "[OK] Recuperação de modelos completa!\n"
     ]
    }
   ],
   "source": [
    "# NOTEBOOK: 02_baseline_models.ipynb\n",
    "# ============================================================================\n",
    "# RECUPERAR MODELOS TREINADOS DO DISCO (SEM CACHE)\n",
    "# ============================================================================\n",
    "print(\"Recuperando modelos treinados salvos em 'artifacts/models/'...\")\n",
    "\n",
    "# Caminhos dos modelos salvos\n",
    "model_paths = {\n",
    "    'LightGBM': MODELS_DIR / 'lightgbm_tuned.pkl',\n",
    "    'XGBoost': MODELS_DIR / 'xgboost_tuned.pkl', \n",
    "    'CatBoost': MODELS_DIR / 'catboost_tuned.pkl',\n",
    "    'Best': MODELS_DIR / 'best_model_tuned.pkl',\n",
    "    'Final': MODELS_DIR / 'final_model_tuned.pkl'\n",
    "}\n",
    "\n",
    "# Dicionário para armazenar os modelos carregados\n",
    "loaded_models = {}\n",
    "\n",
    "# Carregar cada modelo se existir\n",
    "for model_name, model_path in model_paths.items():\n",
    "    if check_artifact_exists(model_path):\n",
    "        loaded_models[model_name] = load_artifact(model_path)\n",
    "        print(f\"[OK] Modelo {model_name} carregado de: {model_path}\")\n",
    "    else:\n",
    "        print(f\"[INFO] Modelo {model_name} não encontrado em: {model_path}\")\n",
    "\n",
    "# Verificar se conseguimos carregar algum modelo\n",
    "if loaded_models:\n",
    "    print(f\"\\n[OK] {len(loaded_models)} modelos recuperados com sucesso!\")\n",
    "    print(\"Modelos disponíveis:\", list(loaded_models.keys()))\n",
    "    \n",
    "    # Definir o melhor modelo (usando o 'Best' se disponível, senão 'Final')\n",
    "    if 'Best' in loaded_models:\n",
    "        best_model = loaded_models['Best']\n",
    "        best_model_name = 'Best (from disk)'\n",
    "    elif 'Final' in loaded_models:\n",
    "        best_model = loaded_models['Final'] \n",
    "        best_model_name = 'Final (from disk)'\n",
    "    else:\n",
    "        print(\"[WARNING] Nenhum modelo 'Best' ou 'Final' encontrado\")\n",
    "        \n",
    "else:\n",
    "    print(\"[ERROR] Nenhum modelo pôde ser carregado do disco\")\n",
    "    print(\"Execute as células de treinamento primeiro para salvar os modelos\")\n",
    "\n",
    "print(\"\\n[OK] Recuperação de modelos completa!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebc5602f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:utils.data:✓ Loaded pickle from C:\\Users\\gafeb\\OneDrive\\Desktop\\lavagem_dev\\artifacts\\models\\best_model_tuned.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Loading pre-trained final models from cache...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:utils.data:✓ Loaded json from C:\\Users\\gafeb\\OneDrive\\Desktop\\lavagem_dev\\artifacts\\cv_results.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] LightGBM model not found, will need to retrain\n",
      "[WARNING] XGBoost model not found, will need to retrain\n",
      "[WARNING] CatBoost model not found, will need to retrain\n",
      "[OK] Loaded best model: CatBoost\n",
      "[OK] CV results loaded for 3 models\n"
     ]
    }
   ],
   "source": [
    "# NOTEBOOK: 02_baseline_models.ipynb\n",
    "# ============================================================================\n",
    "# MODEL CACHE CHECK - Avoid retraining if models exist\n",
    "# ============================================================================\n",
    "\n",
    "# Check if final models are already trained and saved\n",
    "best_model_path = MODELS_DIR / 'best_model_tuned.pkl'\n",
    "cv_results_path = ARTIFACTS_DIR / 'cv_results.json'\n",
    "\n",
    "if check_artifact_exists(best_model_path) and check_artifact_exists(cv_results_path) and not CONFIG.get('force_retrain', False):\n",
    "    print(\"[OK] Loading pre-trained final models from cache...\")\n",
    "    \n",
    "    # Load best model\n",
    "    best_model = load_artifact(best_model_path)\n",
    "    cv_results = load_artifact(cv_results_path)\n",
    "    \n",
    "    # Extract best model name from cv_results\n",
    "    best_model_name = max(\n",
    "        cv_results.keys(),\n",
    "        key=lambda m: cv_results[m]['mean_metrics']['pr_auc_mean']\n",
    "    )\n",
    "    \n",
    "    # Load all final models\n",
    "    final_models = {}\n",
    "    for model_name in cv_results.keys():\n",
    "        model_path = MODELS_DIR / f'{model_name.lower()}_tuned.pkl'\n",
    "        if check_artifact_exists(model_path):\n",
    "            final_models[model_name] = load_artifact(model_path)\n",
    "        else:\n",
    "            print(f\"[WARNING] {model_name} model not found, will need to retrain\")\n",
    "    \n",
    "    print(f\"[OK] Loaded best model: {best_model_name}\")\n",
    "    print(f\"[OK] CV results loaded for {len(cv_results)} models\")\n",
    "    \n",
    "    # Skip the training section below\n",
    "    skip_training = True\n",
    "    \n",
    "else:\n",
    "    print(\"[INFO] No cached models found or force_retrain=True, will train new models...\")\n",
    "    skip_training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "366d0b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTEBOOK: 02_baseline_models.ipynb\n",
    "# recapturar o tuning_results sem precisar de recarregar a celula\n",
    "\n",
    "if not skip_training:\n",
    "    print(\"Training final models with optimized parameters...\\n\")\n",
    "    \n",
    "    cv_strategy = get_cv_strategy('stratified', n_splits=3, random_state=CONFIG['random_state'])\n",
    "    \n",
    "    final_models = {}\n",
    "    cv_results = {}\n",
    "    \n",
    "    for model_name, tuning_result in tuning_results.items():\n",
    "        print(f\"Evaluating {model_name}...\")\n",
    "        \n",
    "        # Create model with best params\n",
    "        best_params = tuning_result['best_params']\n",
    "        \n",
    "        if model_name == 'LightGBM':\n",
    "            model = LGBMClassifier(\n",
    "                **best_params,\n",
    "                n_estimators=1000,\n",
    "                class_weight='balanced',\n",
    "                random_state=CONFIG['random_state'],\n",
    "                verbosity=-1\n",
    "            )\n",
    "        elif model_name == 'XGBoost':\n",
    "            model = XGBClassifier(\n",
    "                **best_params,\n",
    "                n_estimators=1000,\n",
    "                random_state=CONFIG['random_state'],\n",
    "                verbosity=0\n",
    "            )\n",
    "        elif model_name == 'CatBoost':\n",
    "            from catboost import CatBoostClassifier\n",
    "            model = CatBoostClassifier(\n",
    "                **best_params,\n",
    "                iterations=1000,\n",
    "                random_state=CONFIG['random_state'],\n",
    "                verbose=False,\n",
    "                early_stopping_rounds=50\n",
    "            )\n",
    "        \n",
    "        # Cross-validation\n",
    "        cv_result = cross_validate_with_metrics(\n",
    "            model, X_train_balanced, y_train_balanced,\n",
    "            cv_strategy=cv_strategy,\n",
    "            metric_fn=FraudMetrics.compute_all,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        cv_results[model_name] = cv_result\n",
    "        \n",
    "        # Train on full training set\n",
    "        model.fit(X_train_balanced, y_train_balanced)\n",
    "        final_models[model_name] = model\n",
    "        \n",
    "        # Save individual model\n",
    "        save_artifact(model, MODELS_DIR / f'{model_name.lower()}_tuned.pkl', artifact_type='pickle')\n",
    "        \n",
    "        # Print summary\n",
    "        mean_metrics = cv_result['mean_metrics']\n",
    "        print(f\"  PR-AUC: {mean_metrics['pr_auc_mean']:.4f} ± {mean_metrics['pr_auc_std']:.4f}\")\n",
    "        print(f\"  Recall: {mean_metrics['recall_fraud_mean']:.4f} ± {mean_metrics['recall_fraud_std']:.4f}\")\n",
    "        print(f\"  F1:     {mean_metrics['f1_fraud_mean']:.4f} ± {mean_metrics['f1_fraud_std']:.4f}\")\n",
    "        print()\n",
    "    \n",
    "    # Save cv_results\n",
    "    save_artifact(cv_results, cv_results_path, artifact_type='json')\n",
    "    \n",
    "    # Select best model\n",
    "    best_model_name = max(\n",
    "        cv_results.keys(),\n",
    "        key=lambda m: cv_results[m]['mean_metrics']['pr_auc_mean']\n",
    "    )\n",
    "    best_model = final_models[best_model_name]\n",
    "    \n",
    "    # Save best model\n",
    "    save_artifact(best_model, best_model_path, artifact_type='pickle')\n",
    "    \n",
    "    print(f\"[OK] Best model: {best_model_name} (saved to {best_model_path})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b02e27f",
   "metadata": {},
   "source": [
    "## Fase 6: Salvamento de Dados para Próximos Notebooks\n",
    "\n",
    "**Objetivo:** Salvar dados de teste em formato CSV para compatibilidade com próximos notebooks do pipeline.\n",
    "\n",
    "### Compatibilidade com Pipeline:\n",
    "- **Notebook 09 (Threshold Optimization)**: Espera `X_test_engineered.csv` e `y_test_engineered.csv`\n",
    "- **Outros notebooks**: Podem usar tanto CSV quanto Parquet\n",
    "\n",
    "### Arquivos salvos:\n",
    "- **X_test_engineered.csv**: Features de teste (formato CSV)\n",
    "- **y_test_engineered.csv**: Target de teste (formato CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8482c537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SAVING TEST DATA FOR NEXT NOTEBOOKS\n",
      "============================================================\n",
      "Test data saved in CSV format:\n",
      "  - X_test_engineered.csv: (2938, 32)\n",
      "  - y_test_engineered.csv: 2938 samples\n",
      "Files saved to: C:\\Users\\gafeb\\OneDrive\\Desktop\\lavagem_dev\\artifacts\n",
      "X_test_engineered.csv: 0.65 MB\n",
      "y_test_engineered.csv: 0.01 MB\n",
      "\n",
      "============================================================\n",
      "NOTEBOOK 02 COMPLETE - READY FOR NEXT NOTEBOOKS\n",
      "============================================================\n",
      "Next notebooks can now load:\n",
      "• Models: best_model_tuned.pkl, lightgbm_tuned.pkl, etc.\n",
      "• Test data: X_test_engineered.csv, y_test_engineered.csv\n",
      "• Results: cv_results.json, tuning_results.json\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SAVE TEST DATA IN CSV FORMAT FOR NEXT NOTEBOOKS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SAVING TEST DATA FOR NEXT NOTEBOOKS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Save test data in CSV format (expected by notebook 09)\n",
    "X_test_featured.to_csv(ARTIFACTS_DIR / 'X_test_engineered.csv', index=False)\n",
    "y_test.to_frame('target').to_csv(ARTIFACTS_DIR / 'y_test_engineered.csv', index=False)\n",
    "\n",
    "print(\"Test data saved in CSV format:\")\n",
    "print(f\"  - X_test_engineered.csv: {X_test_featured.shape}\")\n",
    "print(f\"  - y_test_engineered.csv: {len(y_test)} samples\")\n",
    "print(f\"Files saved to: {ARTIFACTS_DIR}\")\n",
    "\n",
    "# Verify files exist\n",
    "csv_files = ['X_test_engineered.csv', 'y_test_engineered.csv']\n",
    "for csv_file in csv_files:\n",
    "    file_path = ARTIFACTS_DIR / csv_file\n",
    "    if file_path.exists():\n",
    "        size_mb = file_path.stat().st_size / (1024 * 1024)\n",
    "        print(f\"{csv_file}: {size_mb:.2f} MB\")\n",
    "    else:\n",
    "        print(f\"{csv_file}: NOT FOUND\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"NOTEBOOK 02 COMPLETE - READY FOR NEXT NOTEBOOKS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Next notebooks can now load:\")\n",
    "print(\"• Models: best_model_tuned.pkl, lightgbm_tuned.pkl, etc.\")\n",
    "print(\"• Test data: X_test_engineered.csv, y_test_engineered.csv\")\n",
    "print(\"• Results: cv_results.json, tuning_results.json\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331d56cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 1. IMPORT REQUIRED LIBRARIES\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import warnings\n",
    "import time\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from lightgbm import LGBMClassifier\n",
    "import lightgbm as lgb\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Advanced Optimization\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import MedianPruner\n",
    "\n",
    "# Ensemble Methods\n",
    "from sklearn.ensemble import StackingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# Feature Selection\n",
    "from sklearn.feature_selection import RFE, RFECV, SelectKBest, mutual_info_classif\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Model Evaluation\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_validate\n",
    "from sklearn.metrics import (precision_recall_curve, roc_curve, roc_auc_score,\n",
    "                           classification_report, confusion_matrix,\n",
    "                           recall_score, precision_score, f1_score,\n",
    "                           accuracy_score, average_precision_score,\n",
    "                           make_scorer)\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# Statistical Tests\n",
    "from scipy import stats\n",
    "from scipy.stats import ks_2samp, chi2_contingency\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"LIBRARIES IMPORTED SUCCESSFULLY\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b4d2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 2. LOAD PHASE 1 ARTIFACTS AND DATA\n",
    "# ============================================================================\n",
    "\n",
    "# Add project root to path\n",
    "notebook_dir = Path.cwd()\n",
    "project_root = notebook_dir.parent.parent  # notebooks/07_model_dev -> notebooks -> project_root\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "# Also add the utils directory directly\n",
    "utils_path = project_root / 'utils'\n",
    "if str(utils_path) not in sys.path:\n",
    "    sys.path.append(str(utils_path))\n",
    "\n",
    "# Import custom modules\n",
    "try:\n",
    "    from utils.data import load_artifact, save_artifact\n",
    "    from utils.preprocessing import create_advanced_features\n",
    "    from utils.modeling import FraudMetrics\n",
    "    from utils.visualization import plot_feature_importance, plot_fraud_patterns\n",
    "    import utils.explainability as explainability\n",
    "\n",
    "    # New optimized utils modules\n",
    "    from utils.optimization import BayesianOptimizer, create_temporal_cv_splits, run_comprehensive_optimization\n",
    "    from utils.ensembles import (WeightedEnsemble, create_stacking_ensemble, create_voting_ensemble,\n",
    "                                evaluate_ensemble_methods, evaluate_model_cv, select_best_ensemble)\n",
    "    from utils.feature_selection import AdvancedFeatureSelector\n",
    "    from utils.calibration import ThresholdOptimizer, ProbabilityCalibrator, optimize_threshold_and_calibrate\n",
    "    from utils.evaluation import ModelEvaluator, SHAPAnalyzer, plot_shap_analysis\n",
    "\n",
    "    print(\"Custom modules imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"Import error: {e}\")\n",
    "    print(\"Trying alternative import method...\")\n",
    "    # Alternative import method\n",
    "    import importlib.util\n",
    "\n",
    "    def load_module_from_path(module_name, file_path):\n",
    "        spec = importlib.util.spec_from_file_location(module_name, file_path)\n",
    "        module = importlib.util.module_from_spec(spec)\n",
    "        spec.loader.exec_module(module)\n",
    "        return module\n",
    "\n",
    "    # Load modules directly\n",
    "    utils_path = project_root / 'utils'\n",
    "    data_module = load_module_from_path('utils.data', utils_path / 'data.py')\n",
    "    preprocessing_module = load_module_from_path('utils.preprocessing', utils_path / 'preprocessing.py')\n",
    "    modeling_module = load_module_from_path('utils.modeling', utils_path / 'modeling.py')\n",
    "    visualization_module = load_module_from_path('utils.visualization', utils_path / 'visualization.py')\n",
    "    explainability_module = load_module_from_path('utils.explainability', utils_path / 'explainability.py')\n",
    "\n",
    "    # Load new optimized modules\n",
    "    optimization_module = load_module_from_path('utils.optimization', utils_path / 'optimization.py')\n",
    "    ensembles_module = load_module_from_path('utils.ensembles', utils_path / 'ensembles.py')\n",
    "    feature_selection_module = load_module_from_path('utils.feature_selection', utils_path / 'feature_selection.py')\n",
    "    calibration_module = load_module_from_path('utils.calibration', utils_path / 'calibration.py')\n",
    "    evaluation_module = load_module_from_path('utils.evaluation', utils_path / 'evaluation.py')\n",
    "\n",
    "    load_artifact = data_module.load_artifact\n",
    "    save_artifact = data_module.save_artifact\n",
    "    create_advanced_features = preprocessing_module.create_advanced_features\n",
    "    FraudMetrics = modeling_module.FraudMetrics\n",
    "    plot_feature_importance = visualization_module.plot_feature_importance\n",
    "    plot_fraud_patterns = visualization_module.plot_fraud_patterns\n",
    "    explainability = explainability_module\n",
    "\n",
    "    # Import new optimized classes and functions\n",
    "    BayesianOptimizer = optimization_module.BayesianOptimizer\n",
    "    create_temporal_cv_splits = optimization_module.create_temporal_cv_splits\n",
    "    run_comprehensive_optimization = optimization_module.run_comprehensive_optimization\n",
    "\n",
    "    WeightedEnsemble = ensembles_module.WeightedEnsemble\n",
    "    create_stacking_ensemble = ensembles_module.create_stacking_ensemble\n",
    "    create_voting_ensemble = ensembles_module.create_voting_ensemble\n",
    "    evaluate_ensemble_methods = ensembles_module.evaluate_ensemble_methods\n",
    "    evaluate_model_cv = ensembles_module.evaluate_model_cv\n",
    "    select_best_ensemble = ensembles_module.select_best_ensemble\n",
    "\n",
    "    AdvancedFeatureSelector = feature_selection_module.AdvancedFeatureSelector\n",
    "\n",
    "    ThresholdOptimizer = calibration_module.ThresholdOptimizer\n",
    "    ProbabilityCalibrator = calibration_module.ProbabilityCalibrator\n",
    "    optimize_threshold_and_calibrate = calibration_module.optimize_threshold_and_calibrate\n",
    "\n",
    "    ModelEvaluator = evaluation_module.ModelEvaluator\n",
    "    SHAPAnalyzer = evaluation_module.SHAPAnalyzer\n",
    "    plot_shap_analysis = evaluation_module.plot_shap_analysis\n",
    "\n",
    "    print(\"Custom modules loaded via alternative method\")\n",
    "\n",
    "# Load configuration\n",
    "CONFIG_PATH = project_root / 'config.yaml'\n",
    "with open(CONFIG_PATH, 'r') as f:\n",
    "    CONFIG = yaml.safe_load(f)\n",
    "\n",
    "# Define paths\n",
    "ARTIFACTS_DIR = project_root / CONFIG['paths']['artifacts']\n",
    "DATA_DIR = project_root / CONFIG['paths']['data']\n",
    "MODELS_DIR = ARTIFACTS_DIR / 'models'\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"LOADING PHASE 1 ARTIFACTS AND DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load Phase 1 cleaned datasets\n",
    "print(\"Loading Phase 1 cleaned datasets...\")\n",
    "X_train_clean = load_artifact(ARTIFACTS_DIR / 'X_train_temporal_clean.parquet')\n",
    "X_test_clean = load_artifact(ARTIFACTS_DIR / 'X_test_temporal_clean.parquet')\n",
    "y_train = load_artifact(ARTIFACTS_DIR / 'y_train_processed.parquet')\n",
    "y_test = load_artifact(ARTIFACTS_DIR / 'y_test_processed.parquet')\n",
    "\n",
    "print(f\"Clean datasets loaded: Train {X_train_clean.shape}, Test {X_test_clean.shape}\")\n",
    "\n",
    "# Load Phase 1 corrected models\n",
    "print(\"\\nLoading Phase 1 corrected models...\")\n",
    "phase1_models = {}\n",
    "model_names = ['lightgbm', 'xgboost']\n",
    "\n",
    "for model_name in model_names:\n",
    "    try:\n",
    "        model = load_artifact(ARTIFACTS_DIR / f'{model_name}_temporal_corrected.pkl')\n",
    "        phase1_models[model_name.upper()] = model\n",
    "        print(f\"{model_name.upper()} model loaded\")\n",
    "    except:\n",
    "        print(f\"{model_name.upper()} model not found\")\n",
    "\n",
    "print(f\"[OK] Phase 1 models loaded: {list(phase1_models.keys())}\")\n",
    "\n",
    "# Load Phase 1 results for comparison\n",
    "print(\"\\nLoading Phase 1 results for comparison...\")\n",
    "try:\n",
    "    phase1_report = load_artifact(ARTIFACTS_DIR / 'phase1_corrections_report.json')\n",
    "    phase1_evaluation = load_artifact(ARTIFACTS_DIR / 'phase1_evaluation_results.json')\n",
    "    print(\"[OK] Phase 1 results loaded for comparison\")\n",
    "except:\n",
    "    print(\"[WARNING]  Phase 1 results not found\")\n",
    "    phase1_report = None\n",
    "    phase1_evaluation = None\n",
    "\n",
    "# Load removed features list\n",
    "try:\n",
    "    removed_features = load_artifact(ARTIFACTS_DIR / 'removed_leaky_features.pkl')\n",
    "    print(f\"[OK] Removed features list loaded: {len(removed_features)} features\")\n",
    "except:\n",
    "    print(\"Removed features list not found\")\n",
    "    removed_features = []\n",
    "\n",
    "# Create temporal CV splits (same as Phase 1)\n",
    "cv_splits = create_temporal_cv_splits(X_train_clean, y_train, n_splits=5, test_size=30, gap=0)\n",
    "print(f\"[OK] Temporal CV splits created: {len(cv_splits)} folds\")\n",
    "\n",
    "print(\"\\n[OK] Phase 1 artifacts and data loading complete!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfaf68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENSEMBLE\n",
    "# ============================================================================\n",
    "# 4. ENSEMBLE MODEL DEVELOPMENT\n",
    "# ============================================================================\n",
    "\n",
    "def create_stacking_ensemble(base_models, X_train, y_train, cv_splits):\n",
    "    \"\"\"Create a stacking ensemble with optimized base models.\"\"\"\n",
    "\n",
    "    print(\"Building stacking ensemble...\")\n",
    "\n",
    "    # Define base models for stacking\n",
    "    estimators = []\n",
    "    for name, model in base_models.items():\n",
    "        estimators.append((name.lower(), model))\n",
    "\n",
    "    # Meta-learner (Logistic Regression with class weights)\n",
    "    meta_learner = LogisticRegression(\n",
    "        random_state=42,\n",
    "        class_weight='balanced',\n",
    "        max_iter=1000\n",
    "    )\n",
    "\n",
    "    # Create stacking classifier\n",
    "    stacking_clf = StackingClassifier(\n",
    "        estimators=estimators,\n",
    "        final_estimator=meta_learner,\n",
    "        cv=cv_splits,  # Use temporal CV\n",
    "        stack_method='predict_proba',\n",
    "        passthrough=False,  # Don't include original features\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Train stacking ensemble\n",
    "    print(\"Training stacking ensemble with temporal cross-validation...\")\n",
    "    stacking_clf.fit(X_train, y_train)\n",
    "\n",
    "    return stacking_clf\n",
    "\n",
    "def create_voting_ensemble(base_models, X_train, y_train, voting='soft'):\n",
    "    \"\"\"Create a voting ensemble (hard or soft voting).\"\"\"\n",
    "\n",
    "    print(f\"🏗️  Building {voting} voting ensemble...\")\n",
    "\n",
    "    # Define estimators for voting\n",
    "    estimators = []\n",
    "    for name, model in base_models.items():\n",
    "        estimators.append((name.lower(), model))\n",
    "\n",
    "    # Create voting classifier\n",
    "    voting_clf = VotingClassifier(\n",
    "        estimators=estimators,\n",
    "        voting=voting,  # 'hard' or 'soft'\n",
    "        weights=None,  # Equal weights\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    # Train voting ensemble\n",
    "    voting_clf.fit(X_train, y_train)\n",
    "\n",
    "    return voting_clf\n",
    "\n",
    "def create_weighted_ensemble(base_models, X_train, y_train, cv_splits):\n",
    "    \"\"\"Create a weighted ensemble based on individual model performance.\"\"\"\n",
    "\n",
    "    print(\"🏗️  Building weighted ensemble...\")\n",
    "\n",
    "    # Evaluate individual models\n",
    "    model_scores = {}\n",
    "    for name, model in base_models.items():\n",
    "        cv_scores = []\n",
    "        for train_idx, test_idx in cv_splits:\n",
    "            X_fold_train, X_fold_test = X_train.iloc[train_idx], X_train.iloc[test_idx]\n",
    "            y_fold_train, y_fold_test = y_train.iloc[train_idx], y_train.iloc[test_idx]\n",
    "\n",
    "            model.fit(X_fold_train, y_fold_train)\n",
    "            y_pred = model.predict(X_fold_test)\n",
    "            score = recall_score(y_fold_test, y_pred)\n",
    "            cv_scores.append(score)\n",
    "\n",
    "        print(f\"  {name}: {model_scores[name]:.4f}\")\n",
    "    # Calculate weights based on performance\n",
    "    total_score = sum(model_scores.values())\n",
    "    weights = [model_scores[name] / total_score for name, _ in base_models.items()]\n",
    "\n",
    "    print(f\"Ensemble weights: {dict(zip(base_models.keys(), weights))}\")\n",
    "\n",
    "    # Create weighted voting classifier\n",
    "    estimators = [(name.lower(), model) for name, model in base_models.items()]\n",
    "\n",
    "    weighted_clf = VotingClassifier(\n",
    "        estimators=estimators,\n",
    "        voting='soft',\n",
    "        weights=weights,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    # Train weighted ensemble\n",
    "    weighted_clf.fit(X_train, y_train)\n",
    "\n",
    "    return weighted_clf, weights\n",
    "\n",
    "def evaluate_ensemble_models(ensemble_models, X_test, y_test, model_names):\n",
    "    \"\"\"Evaluate ensemble models on test set.\"\"\"\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for name, model in ensemble_models.items():\n",
    "        print(f\"\\nEvaluating {name}...\")\n",
    "\n",
    "        # Get predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else y_pred\n",
    "\n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            'recall': recall_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred),\n",
    "            'f1': f1_score(y_test, y_pred),\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'roc_auc': roc_auc_score(y_test, y_proba),\n",
    "            'avg_precision': average_precision_score(y_test, y_proba)\n",
    "        }\n",
    "\n",
    "        print(f\"  Recall: {metrics['recall']:.3f}, Precision: {metrics['precision']:.3f}, F1: {metrics['f1']:.3f}\")\n",
    "    return results\n",
    "\n",
    "def plot_ensemble_comparison(base_models, ensemble_models, X_test, y_test):\n",
    "    \"\"\"Plot comparison between base models and ensembles.\"\"\"\n",
    "\n",
    "    # Collect all models\n",
    "    all_models = {**base_models, **ensemble_models}\n",
    "    model_names = list(all_models.keys())\n",
    "\n",
    "    # Calculate metrics for all models\n",
    "    metrics_data = []\n",
    "    for name, model in all_models.items():\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else y_pred\n",
    "\n",
    "        metrics_data.append({\n",
    "            'Model': name,\n",
    "            'Recall': recall_score(y_test, y_pred),\n",
    "            'Precision': precision_score(y_test, y_pred),\n",
    "            'F1': f1_score(y_test, y_pred),\n",
    "            'ROC_AUC': roc_auc_score(y_test, y_proba)\n",
    "        })\n",
    "\n",
    "    metrics_df = pd.DataFrame(metrics_data)\n",
    "\n",
    "    # Create comparison plot\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Base Models vs Ensemble Models Comparison', fontsize=16)\n",
    "\n",
    "    metrics = ['Recall', 'Precision', 'F1', 'ROC_AUC']\n",
    "\n",
    "    for i, metric in enumerate(metrics):\n",
    "        ax = axes[i//2, i%2]\n",
    "\n",
    "        # Separate base and ensemble models\n",
    "        base_data = metrics_df[~metrics_df['Model'].str.contains('Ensemble|Stacking|Voting')]\n",
    "        ensemble_data = metrics_df[metrics_df['Model'].str.contains('Ensemble|Stacking|Voting')]\n",
    "\n",
    "        # Plot bars\n",
    "        x = np.arange(len(metrics_df))\n",
    "        bars = ax.bar(x, metrics_df[metric], color=['lightblue']*len(base_data) + ['orange']*len(ensemble_data))\n",
    "\n",
    "        # Add value labels\n",
    "        for bar, value in zip(bars, metrics_df[metric]):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_y() + bar.get_height(),\n",
    "                   '.3f', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "        ax.set_title(f'{metric} Comparison')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(metrics_df['Model'], rotation=45, ha='right')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "        # Highlight best performer\n",
    "        best_idx = metrics_df[metric].idxmax()\n",
    "        bars[best_idx].set_color('red')\n",
    "        bars[best_idx].set_alpha(0.8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return metrics_df\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"🏗️  ENSEMBLE MODEL DEVELOPMENT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Use optimized models if available, otherwise use Phase 1 models\n",
    "base_models = {}\n",
    "for model_name in ['LIGHTGBM', 'XGBOOST']:\n",
    "    if model_name in optimized_models:\n",
    "        base_models[model_name] = optimized_models[model_name]['model']\n",
    "        print(f\"[OK] Using optimized {model_name} model\")\n",
    "    elif model_name in phase1_models:\n",
    "        base_models[model_name] = phase1_models[model_name]\n",
    "        print(f\"[WARNING]  Using Phase 1 {model_name} model (optimization failed)\")\n",
    "\n",
    "print(f\"\\nBase models for ensemble: {list(base_models.keys())}\")\n",
    "\n",
    "# Create different ensemble types\n",
    "ensemble_models = {}\n",
    "\n",
    "# 1. Stacking Ensemble\n",
    "try:\n",
    "    stacking_ensemble = create_stacking_ensemble(base_models, X_train_clean, y_train, cv_splits)\n",
    "    ensemble_models['Stacking Ensemble'] = stacking_ensemble\n",
    "    print(\"[OK] Stacking ensemble created\")\n",
    "except Exception as e:\n",
    "    print(f\"[WARNING]  Error creating stacking ensemble: {e}\")\n",
    "\n",
    "# 2. Soft Voting Ensemble\n",
    "try:\n",
    "    voting_soft = create_voting_ensemble(base_models, X_train_clean, y_train, voting='soft')\n",
    "    ensemble_models['Soft Voting Ensemble'] = voting_soft\n",
    "    print(\"[OK] Soft voting ensemble created\")\n",
    "except Exception as e:\n",
    "    print(f\"[WARNING]  Error creating soft voting ensemble: {e}\")\n",
    "\n",
    "# 3. Weighted Ensemble\n",
    "try:\n",
    "    weighted_ensemble, weights = create_weighted_ensemble(base_models, X_train_clean, y_train, cv_splits)\n",
    "    ensemble_models['Weighted Ensemble'] = weighted_ensemble\n",
    "    print(\"[OK] Weighted ensemble created\")\n",
    "except Exception as e:\n",
    "    print(f\"[WARNING]  Error creating weighted ensemble: {e}\")\n",
    "\n",
    "print(f\"\\n[OK] Ensemble development complete: {len(ensemble_models)} ensembles created\")\n",
    "\n",
    "# Evaluate ensemble models\n",
    "print(\"\\nEvaluating ensemble models on test set...\")\n",
    "ensemble_results = evaluate_ensemble_models(ensemble_models, X_test_clean, y_test,\n",
    "                                          list(ensemble_models.keys()))\n",
    "\n",
    "# Create comparison plot\n",
    "print(\"\\nCreating ensemble comparison visualization...\")\n",
    "comparison_df = plot_ensemble_comparison(base_models, ensemble_models, X_test_clean, y_test)\n",
    "\n",
    "# Save ensemble models\n",
    "for name, model in ensemble_models.items():\n",
    "    safe_name = name.lower().replace(' ', '_')\n",
    "    save_artifact(model, ARTIFACTS_DIR / f'{safe_name}_phase2.pkl')\n",
    "\n",
    "save_artifact(ensemble_results, ARTIFACTS_DIR / 'phase2_ensemble_results.json')\n",
    "\n",
    "print(\"[OK] Ensemble models saved to artifacts directory\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d51525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENSEMBLE\n",
    "# ============================================================================\n",
    "# 6. MODEL CALIBRATION AND THRESHOLD OPTIMIZATION\n",
    "# ============================================================================\n",
    "\n",
    "def calibrate_model_probabilities(model, X_train, y_train, X_test, y_test, method='isotonic'):\n",
    "    \"\"\"Calibrate model probabilities using different methods.\"\"\"\n",
    "\n",
    "    print(f\"Calibrating model probabilities using {method} regression...\")\n",
    "\n",
    "    # Get uncalibrated probabilities\n",
    "    model.fit(X_train, y_train)\n",
    "    y_proba_uncalibrated = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Create calibrated model\n",
    "    if method == 'isotonic':\n",
    "        calibrator = CalibratedClassifierCV(model, method='isotonic', cv='prefit')\n",
    "    elif method == 'sigmoid':\n",
    "        calibrator = CalibratedClassifierCV(model, method='sigmoid', cv='prefit')\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'isotonic' or 'sigmoid'\")\n",
    "\n",
    "    # Fit calibrator\n",
    "    calibrator.fit(X_train, y_train)\n",
    "\n",
    "    # Get calibrated probabilities\n",
    "    y_proba_calibrated = calibrator.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Evaluate calibration\n",
    "    from sklearn.metrics import brier_score_loss\n",
    "\n",
    "    brier_uncalibrated = brier_score_loss(y_test, y_proba_uncalibrated)\n",
    "    brier_calibrated = brier_score_loss(y_test, y_proba_calibrated)\n",
    "\n",
    "    print(f\"Brier Score - Uncalibrated: {brier_uncalibrated:.4f}\")\n",
    "    print(f\"Brier Score - Calibrated: {brier_calibrated:.4f}\")\n",
    "    return calibrator, y_proba_uncalibrated, y_proba_calibrated\n",
    "\n",
    "def plot_calibration_curves(y_test, y_proba_uncalibrated, y_proba_calibrated, model_name):\n",
    "    \"\"\"Plot calibration curves before and after calibration.\"\"\"\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    # Calibration curves\n",
    "    prob_true, prob_pred = calibration_curve(y_test, y_proba_uncalibrated, n_bins=10)\n",
    "    ax1.plot(prob_pred, prob_true, marker='o', label='Uncalibrated', color='red')\n",
    "\n",
    "    prob_true_cal, prob_pred_cal = calibration_curve(y_test, y_proba_calibrated, n_bins=10)\n",
    "    ax1.plot(prob_pred_cal, prob_true_cal, marker='s', label='Calibrated', color='blue')\n",
    "\n",
    "    # Perfect calibration line\n",
    "    ax1.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfect calibration')\n",
    "\n",
    "    ax1.set_xlabel('Mean predicted probability')\n",
    "    ax1.set_ylabel('Fraction of positives')\n",
    "    ax1.set_title(f'Calibration Curve - {model_name}')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # Reliability diagram\n",
    "    ax2.hist(y_proba_uncalibrated, bins=10, alpha=0.5, label='Uncalibrated', color='red')\n",
    "    ax2.hist(y_proba_calibrated, bins=10, alpha=0.5, label='Calibrated', color='blue')\n",
    "    ax2.set_xlabel('Predicted probability')\n",
    "    ax2.set_ylabel('Count')\n",
    "    ax2.set_title(f'Probability Distribution - {model_name}')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def optimize_decision_threshold(model, X_test, y_test, metric='recall', min_metric=0.95):\n",
    "    \"\"\"Optimize decision threshold for specific metric.\"\"\"\n",
    "\n",
    "    print(f\"Optimizing decision threshold for {metric} >= {min_metric}...\")\n",
    "\n",
    "    # Get probabilities\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Try different thresholds\n",
    "    thresholds = np.arange(0.01, 0.99, 0.01)\n",
    "    results = []\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_proba >= threshold).astype(int)\n",
    "\n",
    "        # Calculate metrics\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        results.append({\n",
    "            'threshold': threshold,\n",
    "            'recall': recall,\n",
    "            'precision': precision,\n",
    "            'f1': f1,\n",
    "            'accuracy': accuracy\n",
    "        })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # Find optimal threshold based on criteria\n",
    "    if metric == 'recall':\n",
    "        # Find highest threshold that achieves minimum recall\n",
    "        valid_thresholds = results_df[results_df['recall'] >= min_metric]\n",
    "        if len(valid_thresholds) > 0:\n",
    "            optimal_row = valid_thresholds.loc[valid_thresholds['precision'].idxmax()]\n",
    "        else:\n",
    "            # If no threshold meets minimum recall, take the best recall\n",
    "            optimal_row = results_df.loc[results_df['recall'].idxmax()]\n",
    "    elif metric == 'f1':\n",
    "        optimal_row = results_df.loc[results_df['f1'].idxmax()]\n",
    "    else:\n",
    "        raise ValueError(\"Metric must be 'recall' or 'f1'\")\n",
    "\n",
    "    optimal_threshold = optimal_row['threshold']\n",
    "\n",
    "    print(f\"Optimal threshold: {optimal_threshold:.3f}\")\n",
    "    print(f\"Recall: {optimal_row['recall']:.3f}\")\n",
    "    print(f\"Precision: {optimal_row['precision']:.3f}\")\n",
    "    print(f\"F1-Score: {optimal_row['f1']:.3f}\")\n",
    "    return optimal_threshold, results_df\n",
    "\n",
    "def plot_threshold_optimization(results_df, optimal_threshold, metric_name):\n",
    "    \"\"\"Plot threshold optimization results.\"\"\"\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle(f'Threshold Optimization Results (Optimal: {optimal_threshold:.3f})', fontsize=16)\n",
    "\n",
    "    metrics = ['recall', 'precision', 'f1', 'accuracy']\n",
    "\n",
    "    for i, metric in enumerate(metrics):\n",
    "        ax = axes[i//2, i%2]\n",
    "\n",
    "        ax.plot(results_df['threshold'], results_df[metric], linewidth=2)\n",
    "\n",
    "        # Mark optimal threshold\n",
    "        optimal_value = results_df[results_df['threshold'] == optimal_threshold][metric].iloc[0]\n",
    "        ax.axvline(x=optimal_threshold, color='red', linestyle='--', alpha=0.7,\n",
    "                  label=f'Optimal threshold ({optimal_value:.3f})')\n",
    "        ax.scatter([optimal_threshold], [optimal_value], color='red', s=50, zorder=5)\n",
    "\n",
    "        ax.set_xlabel('Decision Threshold')\n",
    "        ax.set_ylabel(metric.capitalize())\n",
    "        ax.set_title(f'{metric.capitalize()} vs Threshold')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def create_cost_sensitive_predictions(model, X_test, y_test, cost_fn_ratio=10, optimal_threshold=None):\n",
    "    \"\"\"Create cost-sensitive predictions with custom cost function.\"\"\"\n",
    "\n",
    "    print(\"Creating cost-sensitive predictions...\")\n",
    "\n",
    "    # Get probabilities\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Use optimal threshold if provided, otherwise use default\n",
    "    threshold = optimal_threshold if optimal_threshold is not None else 0.5\n",
    "\n",
    "    # Cost-sensitive prediction: adjust threshold based on cost ratio\n",
    "    # Cost of false negative (FN) vs false positive (FP)\n",
    "    # FN cost = cost_fn_ratio * FP cost\n",
    "\n",
    "    # Calculate adjusted threshold\n",
    "    # threshold_adjusted = cost_fn / (cost_fn + cost_fp)\n",
    "    # But since we want to be more sensitive to FN, we lower the threshold\n",
    "    threshold_adjusted = threshold * (1 / cost_fn_ratio)\n",
    "\n",
    "    print(f\"Original threshold: {threshold:.3f}\")\n",
    "    print(f\"Cost-adjusted threshold: {threshold_adjusted:.3f}\")\n",
    "    # Make predictions with adjusted threshold\n",
    "    y_pred_cost_sensitive = (y_proba >= threshold_adjusted).astype(int)\n",
    "\n",
    "    # Evaluate cost-sensitive predictions\n",
    "    recall_cs = recall_score(y_test, y_pred_cost_sensitive)\n",
    "    precision_cs = precision_score(y_test, y_pred_cost_sensitive)\n",
    "    f1_cs = f1_score(y_test, y_pred_cost_sensitive)\n",
    "\n",
    "    print(\"Cost-sensitive predictions:\")\n",
    "    print(f\"Recall: {recall_cs:.3f}\")\n",
    "    print(f\"Precision: {precision_cs:.3f}\")\n",
    "    print(f\"F1-Score: {f1_cs:.3f}\")\n",
    "    return y_pred_cost_sensitive, threshold_adjusted\n",
    "\n",
    "def evaluate_calibration_and_thresholds(models_dict, X_test, y_test):\n",
    "    \"\"\"Comprehensive evaluation of calibration and threshold optimization.\"\"\"\n",
    "\n",
    "    print(\"🔧 Evaluating calibration and threshold optimization for all models...\")\n",
    "\n",
    "    calibration_results = {}\n",
    "\n",
    "    for model_name, model in models_dict.items():\n",
    "        print(f\"\\nProcessing {model_name}...\")\n",
    "\n",
    "        # Calibrate probabilities\n",
    "        calibrated_model, y_proba_uncal, y_proba_cal = calibrate_model_probabilities(\n",
    "            model, X_train_clean, y_train, X_test, y_test, method='isotonic'\n",
    "        )\n",
    "\n",
    "        # Plot calibration curves\n",
    "        plot_calibration_curves(y_test, y_proba_uncal, y_proba_cal, model_name)\n",
    "\n",
    "        # Optimize threshold\n",
    "        optimal_threshold, threshold_results = optimize_decision_threshold(\n",
    "            calibrated_model, X_test, y_test, metric='recall', min_metric=0.95\n",
    "        )\n",
    "\n",
    "        # Plot threshold optimization\n",
    "        plot_threshold_optimization(threshold_results, optimal_threshold, 'recall')\n",
    "\n",
    "        # Cost-sensitive predictions\n",
    "        y_pred_cs, threshold_cs = create_cost_sensitive_predictions(\n",
    "            calibrated_model, X_test, y_test, cost_fn_ratio=10, optimal_threshold=optimal_threshold\n",
    "        )\n",
    "\n",
    "        # Store results\n",
    "        calibration_results[model_name] = {\n",
    "            'calibrated_model': calibrated_model,\n",
    "            'probabilities_uncalibrated': y_proba_uncal,\n",
    "            'probabilities_calibrated': y_proba_cal,\n",
    "            'optimal_threshold': optimal_threshold,\n",
    "            'threshold_results': threshold_results,\n",
    "            'cost_sensitive_predictions': y_pred_cs,\n",
    "            'cost_sensitive_threshold': threshold_cs\n",
    "        }\n",
    "\n",
    "    return calibration_results\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"🔧 MODEL CALIBRATION AND THRESHOLD OPTIMIZATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Combine all available models for calibration\n",
    "all_models = {**phase1_models}\n",
    "if 'ensemble_models' in globals():\n",
    "    all_models.update(ensemble_models)\n",
    "\n",
    "print(f\"Models to calibrate: {list(all_models.keys())}\")\n",
    "\n",
    "    \"# Perform comprehensive calibration and threshold optimization using utils\\n\",\n",
    "    \"from utils.calibration import optimize_threshold_and_calibrate\\n\",\n",
    "    \"calibration_results = optimize_threshold_and_calibrate(\\n\",\n",
    "    \"    all_models, X_train_clean, y_train, X_test_clean, y_test,\\n\",\n",
    "    \"    calibration_method='isotonic',\\n\",\n",
    "    \"    threshold_metric='recall',\\n\",\n",
    "    \"    min_metric=0.95,\\n\",\n",
    "    \"    cost_fn_ratio=10\\n\",\n",
    "    \")\n",
    "\n",
    "# Save calibration results\n",
    "save_artifact(calibration_results, ARTIFACTS_DIR / 'phase2_calibration_results.json')\n",
    "\n",
    "print(\"[OK] Model calibration and threshold optimization complete\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdc46d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENSEMBLE\n",
    "# ============================================================================\n",
    "# 7. PHASE 2 RESULTS SUMMARY AND COMPARISON\n",
    "# ============================================================================\n",
    "\n",
    "def create_phase2_summary_report():\n",
    "    \"\"\"Create comprehensive summary of Phase 2 results.\"\"\"\n",
    "\n",
    "    print(\"📊 Creating Phase 2 results summary...\")\n",
    "\n",
    "    summary = {\n",
    "        'phase2_metadata': {\n",
    "            'phase': 'Phase 2 - Advanced Optimizations',\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'objectives': [\n",
    "                'Hyperparameter optimization with Bayesian methods',\n",
    "                'Ensemble model development (stacking, voting, weighted)',\n",
    "                'Advanced feature selection (RFE, permutation, SHAP)',\n",
    "                'Model calibration and threshold optimization',\n",
    "                'Production-ready model preparation'\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # 1. Hyperparameter Optimization Results\n",
    "    if 'optimization_results' in globals():\n",
    "        summary['hyperparameter_optimization'] = {}\n",
    "        for model_name, results in optimization_results.items():\n",
    "            summary['hyperparameter_optimization'][model_name] = {\n",
    "                'best_score': results['best_score'],\n",
    "                'best_params': results['best_params'],\n",
    "                'optimization_time': results.get('optimization_time', 'N/A'),\n",
    "                'n_trials': results.get('n_trials', 'N/A')\n",
    "            }\n",
    "\n",
    "    # 2. Ensemble Model Results\n",
    "    if 'ensemble_results' in globals():\n",
    "        summary['ensemble_models'] = {}\n",
    "        for model_name, results in ensemble_results.items():\n",
    "            summary['ensemble_models'][model_name] = {\n",
    "                'metrics': results['metrics'],\n",
    "                'improvement_over_base': 'To be calculated'\n",
    "            }\n",
    "\n",
    "    # 3. Feature Selection Results\n",
    "    if 'selection_impact_results' in globals():\n",
    "        summary['feature_selection'] = {}\n",
    "        for method, results in selection_impact_results.items():\n",
    "            summary['feature_selection'][method] = {\n",
    "                'n_features': results['metrics']['n_features'],\n",
    "                'recall': results['metrics']['recall'],\n",
    "                'precision': results['metrics']['precision'],\n",
    "                'f1': results['metrics']['f1']\n",
    "            }\n",
    "\n",
    "    # 4. Calibration Results\n",
    "    if 'calibration_results' in globals():\n",
    "        summary['calibration'] = {}\n",
    "        for model_name, results in calibration_results.items():\n",
    "            summary['calibration'][model_name] = {\n",
    "                'optimal_threshold': results['optimal_threshold'],\n",
    "                'cost_sensitive_threshold': results['cost_sensitive_threshold']\n",
    "            }\n",
    "\n",
    "    return summary\n",
    "\n",
    "def compare_phase1_vs_phase2():\n",
    "    \"\"\"Compare Phase 1 and Phase 2 results.\"\"\"\n",
    "\n",
    "    print(\"[UPDATE] Comparing Phase 1 vs Phase 2 results...\")\n",
    "\n",
    "    # Load Phase 1 results\n",
    "    phase1_results_file = ARTIFACTS_DIR / 'phase1_model_results.json'\n",
    "    if phase1_results_file.exists():\n",
    "        with open(phase1_results_file, 'r') as f:\n",
    "            phase1_results = json.load(f)\n",
    "    else:\n",
    "        print(\"[WARNING]  Phase 1 results not found\")\n",
    "        return None\n",
    "\n",
    "    comparison = {\n",
    "        'phase1_baseline': {},\n",
    "        'phase2_improvements': {},\n",
    "        'overall_improvements': {}\n",
    "    }\n",
    "\n",
    "    # Extract Phase 1 metrics\n",
    "    for model_name, results in phase1_results.items():\n",
    "        if isinstance(results, dict) and 'metrics' in results:\n",
    "            comparison['phase1_baseline'][model_name] = results['metrics']\n",
    "\n",
    "    # Extract Phase 2 metrics\n",
    "    phase2_metrics = {}\n",
    "\n",
    "    # From hyperparameter optimization\n",
    "    if 'optimization_results' in globals():\n",
    "        for model_name, results in optimization_results.items():\n",
    "            if 'best_score' in results:\n",
    "                phase2_metrics[f'{model_name}_optimized'] = {\n",
    "                    'recall': results['best_score'],\n",
    "                    'source': 'hyperparameter_optimization'\n",
    "                }\n",
    "\n",
    "    # From ensemble models\n",
    "    if 'ensemble_results' in globals():\n",
    "        for model_name, results in ensemble_results.items():\n",
    "            phase2_metrics[model_name] = results['metrics']\n",
    "            phase2_metrics[model_name]['source'] = 'ensemble'\n",
    "\n",
    "    # From feature selection\n",
    "    if 'selection_impact_results' in globals():\n",
    "        for method, results in selection_impact_results.items():\n",
    "            phase2_metrics[f'{method}_selected'] = results['metrics']\n",
    "            phase2_metrics[f'{method}_selected']['source'] = 'feature_selection'\n",
    "\n",
    "    comparison['phase2_improvements'] = phase2_metrics\n",
    "\n",
    "    # Calculate improvements\n",
    "    improvements = {}\n",
    "    for model_name, p2_metrics in phase2_metrics.items():\n",
    "        base_model = model_name.split('_')[0]  # Extract base model name\n",
    "        if base_model in comparison['phase1_baseline']:\n",
    "            p1_metrics = comparison['phase1_baseline'][base_model]\n",
    "            improvements[model_name] = {\n",
    "                'recall_improvement': p2_metrics.get('recall', 0) - p1_metrics.get('recall', 0),\n",
    "                'precision_improvement': p2_metrics.get('precision', 0) - p1_metrics.get('precision', 0),\n",
    "                'f1_improvement': p2_metrics.get('f1', 0) - p1_metrics.get('f1', 0)\n",
    "            }\n",
    "\n",
    "    comparison['overall_improvements'] = improvements\n",
    "\n",
    "    return comparison\n",
    "\n",
    "def plot_phase_comparison(comparison_results):\n",
    "    \"\"\"Plot comprehensive Phase 1 vs Phase 2 comparison.\"\"\"\n",
    "\n",
    "    if comparison_results is None:\n",
    "        print(\"[WARNING]  No comparison results available\")\n",
    "        return\n",
    "\n",
    "    # Extract data for plotting\n",
    "    models = []\n",
    "    p1_recall = []\n",
    "    p2_recall = []\n",
    "    improvements = []\n",
    "\n",
    "    for model_name, improvement in comparison_results['overall_improvements'].items():\n",
    "        base_model = model_name.split('_')[0]\n",
    "        if base_model in comparison_results['phase1_baseline']:\n",
    "            models.append(model_name)\n",
    "            p1_recall.append(comparison_results['phase1_baseline'][base_model]['recall'])\n",
    "            p2_recall.append(comparison_results['phase2_improvements'][model_name]['recall'])\n",
    "            improvements.append(improvement['recall_improvement'])\n",
    "\n",
    "    if not models:\n",
    "        print(\"[WARNING]  No comparable models found\")\n",
    "        return\n",
    "\n",
    "    # Create comparison plot\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    # Recall comparison\n",
    "    x = np.arange(len(models))\n",
    "    width = 0.35\n",
    "\n",
    "    bars1 = ax1.bar(x - width/2, p1_recall, width, label='Phase 1', color='lightcoral', alpha=0.7)\n",
    "    bars2 = ax1.bar(x + width/2, p2_recall, width, label='Phase 2', color='lightgreen', alpha=0.7)\n",
    "\n",
    "    ax1.set_xlabel('Models')\n",
    "    ax1.set_ylabel('Recall')\n",
    "    ax1.set_title('Phase 1 vs Phase 2 Recall Comparison')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(models, rotation=45, ha='right')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # Add value labels\n",
    "    for bar, value in zip(bars1, p1_recall):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_y() + bar.get_height(),\n",
    "                f'{value:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "    for bar, value in zip(bars2, p2_recall):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_y() + bar.get_height(),\n",
    "                f'{value:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "    # Improvement plot\n",
    "    colors = ['green' if x > 0 else 'red' for x in improvements]\n",
    "    bars3 = ax2.bar(models, improvements, color=colors, alpha=0.7)\n",
    "\n",
    "    ax2.set_xlabel('Models')\n",
    "    ax2.set_ylabel('Recall Improvement')\n",
    "    ax2.set_title('Phase 2 Improvements Over Phase 1')\n",
    "    ax2.tick_params(axis='x', rotation=45, ha='right')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    # Add value labels\n",
    "    for bar, value in zip(bars3, improvements):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_y() + (bar.get_height() if bar.get_height() > 0 else 0),\n",
    "                f'{value:+.3f}' if value > 0 else f'{value:.3f}', ha='center',\n",
    "                va='bottom' if value > 0 else 'top', fontsize=9)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def generate_phase2_recommendations(comparison_results, summary):\n",
    "    \"\"\"Generate recommendations based on Phase 2 results.\"\"\"\n",
    "\n",
    "    print(\"🎯 Generating Phase 2 recommendations...\")\n",
    "\n",
    "    recommendations = {\n",
    "        'best_overall_model': None,\n",
    "        'recommended_threshold': None,\n",
    "        'feature_selection_method': None,\n",
    "        'production_readiness': [],\n",
    "        'next_steps': []\n",
    "    }\n",
    "\n",
    "    # Find best performing model\n",
    "    best_recall = 0\n",
    "    best_model = None\n",
    "\n",
    "    # Check ensemble models\n",
    "    if 'ensemble_results' in globals():\n",
    "        for model_name, results in ensemble_results.items():\n",
    "            recall = results['metrics']['recall']\n",
    "            if recall > best_recall:\n",
    "                best_recall = recall\n",
    "                best_model = model_name\n",
    "\n",
    "    # Check optimized models\n",
    "    if 'optimization_results' in globals():\n",
    "        for model_name, results in optimization_results.items():\n",
    "            recall = results.get('best_score', 0)\n",
    "            if recall > best_recall:\n",
    "                best_recall = recall\n",
    "                best_model = f'{model_name}_optimized'\n",
    "\n",
    "    recommendations['best_overall_model'] = best_model\n",
    "\n",
    "    # Recommended threshold\n",
    "    if 'calibration_results' in globals() and best_model in calibration_results:\n",
    "        recommendations['recommended_threshold'] = calibration_results[best_model]['optimal_threshold']\n",
    "\n",
    "    # Best feature selection method\n",
    "    if 'selection_impact_results' in globals():\n",
    "        best_fs_recall = 0\n",
    "        best_fs_method = None\n",
    "        for method, results in selection_impact_results.items():\n",
    "            recall = results['metrics']['recall']\n",
    "            if recall > best_fs_recall:\n",
    "                best_fs_recall = recall\n",
    "                best_fs_method = method\n",
    "        recommendations['feature_selection_method'] = best_fs_method\n",
    "\n",
    "    # Production readiness checklist\n",
    "    recommendations['production_readiness'] = [\n",
    "        \"Model calibrated for probability accuracy\",\n",
    "        \"Optimal decision threshold determined\",\n",
    "        \"Feature selection applied for efficiency\",\n",
    "        \"Temporal validation implemented\",\n",
    "        \"Data leakage issues resolved\"\n",
    "    ]\n",
    "\n",
    "    # Next steps\n",
    "    recommendations['next_steps'] = [\n",
    "        \"Deploy best model to production environment\",\n",
    "        \"Set up model monitoring for concept drift\",\n",
    "        \"Implement A/B testing framework\",\n",
    "        \"Create model documentation and API\",\n",
    "        \"Establish model retraining pipeline\"\n",
    "    ]\n",
    "\n",
    "    return recommendations\n",
    "\n",
    "def save_phase2_final_report(summary, comparison, recommendations):\n",
    "    \"\"\"Save comprehensive Phase 2 final report.\"\"\"\n",
    "\n",
    "    final_report = {\n",
    "        'summary': summary,\n",
    "        'comparison': comparison,\n",
    "        'recommendations': recommendations,\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "    save_artifact(final_report, ARTIFACTS_DIR / 'phase2_final_report.json')\n",
    "\n",
    "    # Create human-readable summary\n",
    "    report_text = f\"\"\"\n",
    "# Phase 2 Advanced Optimizations - Final Report\n",
    "\n",
    "## Executive Summary\n",
    "Phase 2 advanced optimizations completed successfully, implementing:\n",
    "- Bayesian hyperparameter optimization\n",
    "- Ensemble model development\n",
    "- Advanced feature selection\n",
    "- Model calibration and threshold optimization\n",
    "\n",
    "## Best Performing Model\n",
    "{recommendations['best_overall_model'] or 'To be determined'}\n",
    "\n",
    "## Key Improvements\n",
    "- Hyperparameter optimization: Implemented\n",
    "- Ensemble methods: {len(ensemble_results) if 'ensemble_results' in globals() else 0} ensembles created\n",
    "- Feature selection: {len(selection_impact_results) if 'selection_impact_results' in globals() else 0} methods evaluated\n",
    "- Model calibration: Completed for all models\n",
    "\n",
    "## Recommendations for Production\n",
    "1. Use {recommendations['best_overall_model'] or 'best performing model'}\n",
    "2. Apply optimal threshold: {recommendations['recommended_threshold'] or 'TBD'}\n",
    "3. Feature selection method: {recommendations['feature_selection_method'] or 'TBD'}\n",
    "\n",
    "## Next Steps\n",
    "{chr(10).join(f\"- {step}\" for step in recommendations['next_steps'])}\n",
    "\"\"\"\n",
    "\n",
    "    with open(ARTIFACTS_DIR / 'phase2_final_report.md', 'w') as f:\n",
    "        f.write(report_text)\n",
    "\n",
    "    print(\"[OK] Phase 2 final report saved\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"📊 PHASE 2 RESULTS SUMMARY AND COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create Phase 2 summary\n",
    "phase2_summary = create_phase2_summary_report()\n",
    "\n",
    "# Compare Phase 1 vs Phase 2\n",
    "phase_comparison = compare_phase1_vs_phase2()\n",
    "\n",
    "# Plot comparison\n",
    "plot_phase_comparison(phase_comparison)\n",
    "\n",
    "# Generate recommendations\n",
    "phase2_recommendations = generate_phase2_recommendations(phase_comparison, phase2_summary)\n",
    "\n",
    "# Save final report\n",
    "save_phase2_final_report(phase2_summary, phase_comparison, phase2_recommendations)\n",
    "\n",
    "print(\"\\n[SUCCESS] Phase 2 Advanced Optimizations Complete!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Key Achievements:\")\n",
    "print(\"[OK] Bayesian hyperparameter optimization implemented\")\n",
    "print(\"[OK] Ensemble models developed (stacking, voting, weighted)\")\n",
    "print(\"[OK] Advanced feature selection completed (RFE, permutation, SHAP)\")\n",
    "print(\"[OK] Model calibration and threshold optimization finished\")\n",
    "print(\"[OK] Comprehensive results comparison and recommendations generated\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

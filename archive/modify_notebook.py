import json
import sys

# Carregar o notebook
with open('notebooks/03_Modelagem_e_Avaliacao.ipynb', 'r', encoding='utf-8') as f:
    nb = json.load(f)

print(f"Notebook carregado com {len(nb['cells'])} c√©lulas")

# Encontrar a c√©lula de treinamento principal (c√©lula 12)
training_cell_idx = None
for i, cell in enumerate(nb['cells']):
    if cell['cell_type'] == 'code' and len(cell['source']) > 0:
        first_line = cell['source'][0].strip()
        if 'import time' in first_line and 'Para m√©tricas de tempo' in ''.join(cell['source']):
            training_cell_idx = i
            print(f"Encontrada c√©lula de treinamento na posi√ß√£o {i}")
            break

if training_cell_idx is None:
    print("C√©lula de treinamento n√£o encontrada")
    sys.exit(1)

# Criar as novas c√©lulas
setup_cell = {
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Setup Inicial e Configura√ß√£o do Treinamento\n",
        "import time\n",
        "import logging\n",
        "from pathlib import Path\n",
        "\n",
        "# Configura√ß√£o de logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Tentar instalar tabulate para tabelas elegantes\n",
        "try:\n",
        "    from tabulate import tabulate\n",
        "    HAS_TABULATE = True\n",
        "except ImportError:\n",
        "    HAS_TABULATE = False\n",
        "    logger.warning(\"tabulate n√£o instalado - tabelas ser√£o simples\")\n",
        "\n",
        "# Inicializar trainer e estruturas de dados\n",
        "aml_trainer = AMLModelTrainer(EXPERIMENT_CONFIG)\n",
        "model_names = ['xgboost', 'lightgbm', 'random_forest']\n",
        "\n",
        "# Dicion√°rios para resultados\n",
        "all_training_results = {}\n",
        "all_evaluation_results = []\n",
        "model_comparison = []\n",
        "trained_models = {}\n",
        "\n",
        "print(\"‚úÖ Setup inicial conclu√≠do\")\n",
        "print(f\"   ‚Üí Modelos a treinar: {model_names}\")\n",
        "print(f\"   ‚Üí Modo: {TRAINING_MODE.upper()}\")\n",
        "print(f\"   ‚Üí Dados: {len(X):,} transa√ß√µes\")\n"
    ]
}

xgboost_cell = {
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# TREINAMENTO INDIVIDUAL: XGBOOST\n",
        "print(\"\\nüöÄ TREINAMENTO XGBOOST\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "model_name = 'xgboost'\n",
        "start_time = time.time()\n",
        "\n",
        "# Verificar cache\n",
        "if aml_trainer.check_model_cache(model_name, str(artifacts_dir)):\n",
        "    try:\n",
        "        trained_model, training_res, eval_res = aml_trainer.load_model_from_cache(model_name, str(artifacts_dir))\n",
        "        aml_trainer.trained_models[model_name] = trained_model\n",
        "        all_training_results[model_name] = training_res\n",
        "        all_evaluation_results.append(eval_res)\n",
        "        trained_models[model_name] = True\n",
        "        cache_time = time.time() - start_time\n",
        "        print(f\"‚úÖ {model_name.upper()}: Carregado do cache em {cache_time:.1f}s\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  {model_name.upper()}: Cache corrompido ({e}), treinando do zero\")\n",
        "        trained_models[model_name] = False\n",
        "else:\n",
        "    trained_models[model_name] = False\n",
        "\n",
        "# Treinar se necess√°rio\n",
        "if not trained_models[model_name]:\n",
        "    try:\n",
        "        train_start = time.time()\n",
        "        print(f\"üîÑ Treinando {model_name.upper()} com valida√ß√£o temporal...\")\n",
        "        \n",
        "        # Treino com valida√ß√£o temporal\n",
        "        training_results = aml_trainer.train_with_temporal_cv(X, y, model_name)\n",
        "        evaluation_results = aml_trainer.evaluate_model(X, y, model_name)\n",
        "        \n",
        "        # Salvar em cache\n",
        "        try:\n",
        "            aml_trainer.save_model_to_cache(model_name, aml_trainer.trained_models[model_name], \n",
        "                                          training_results, evaluation_results, str(artifacts_dir))\n",
        "            train_time = time.time() - train_start\n",
        "            print(f\"‚úÖ {model_name.upper()}: Treinado e salvo em {train_time:.1f}s\")\n",
        "        except Exception as save_error:\n",
        "            train_time = time.time() - train_start\n",
        "            print(f\"‚ö†Ô∏è  {model_name.upper()}: Treinado em {train_time:.1f}s mas falhou ao salvar ({save_error})\")\n",
        "        \n",
        "        # Adicionar aos resultados\n",
        "        all_training_results[model_name] = training_results\n",
        "        all_evaluation_results.append(evaluation_results)\n",
        "        trained_models[model_name] = True\n",
        "        \n",
        "        # M√©tricas para compara√ß√£o\n",
        "        cv_metrics = training_results['cv_results']\n",
        "        threshold_metrics = evaluation_results['threshold_analysis']\n",
        "        optimal_metrics = threshold_metrics[threshold_metrics['threshold'] == evaluation_results['optimal_threshold']].iloc[0]\n",
        "        \n",
        "        model_comparison.append({\n",
        "            'model': model_name.upper(),\n",
        "            'roc_auc_cv': cv_metrics['roc_auc']['mean'],\n",
        "            'roc_auc_test': evaluation_results['roc_auc'],\n",
        "            'pr_auc': evaluation_results['pr_auc'],\n",
        "            'recall_cv': cv_metrics['recall']['mean'],\n",
        "            'recall_test': optimal_metrics['recall'],\n",
        "            'precision_test': optimal_metrics['precision'],\n",
        "            'f1_test': optimal_metrics['f1'],\n",
        "            'optimal_threshold': evaluation_results['optimal_threshold'],\n",
        "            'fraud_rate': optimal_metrics['predicted_fraud_rate'],\n",
        "            'compliant': evaluation_results['regulatory_compliance']['overall_compliant']\n",
        "        })\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå ERRO em {model_name.upper()}: {e}\")\n",
        "        trained_models[model_name] = False\n",
        "\n",
        "print(f\"üèÅ XGBOOST conclu√≠do - Status: {'‚úÖ Sucesso' if trained_models.get(model_name, False) else '‚ùå Falhou'}\")\n"
    ]
}

lightgbm_cell = {
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# TREINAMENTO INDIVIDUAL: LIGHTGBM\n",
        "print(\"\\nüöÄ TREINAMENTO LIGHTGBM\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "model_name = 'lightgbm'\n",
        "start_time = time.time()\n",
        "\n",
        "# Verificar cache\n",
        "if aml_trainer.check_model_cache(model_name, str(artifacts_dir)):\n",
        "    try:\n",
        "        trained_model, training_res, eval_res = aml_trainer.load_model_from_cache(model_name, str(artifacts_dir))\n",
        "        aml_trainer.trained_models[model_name] = trained_model\n",
        "        all_training_results[model_name] = training_res\n",
        "        all_evaluation_results.append(eval_res)\n",
        "        trained_models[model_name] = True\n",
        "        cache_time = time.time() - start_time\n",
        "        print(f\"‚úÖ {model_name.upper()}: Carregado do cache em {cache_time:.1f}s\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  {model_name.upper()}: Cache corrompido ({e}), treinando do zero\")\n",
        "        trained_models[model_name] = False\n",
        "else:\n",
        "    trained_models[model_name] = False\n",
        "\n",
        "# Treinar se necess√°rio\n",
        "if not trained_models[model_name]:\n",
        "    try:\n",
        "        train_start = time.time()\n",
        "        print(f\"üîÑ Treinando {model_name.upper()} com valida√ß√£o temporal...\")\n",
        "        \n",
        "        # Treino com valida√ß√£o temporal\n",
        "        training_results = aml_trainer.train_with_temporal_cv(X, y, model_name)\n",
        "        evaluation_results = aml_trainer.evaluate_model(X, y, model_name)\n",
        "        \n",
        "        # Salvar em cache\n",
        "        try:\n",
        "            aml_trainer.save_model_to_cache(model_name, aml_trainer.trained_models[model_name], \n",
        "                                          training_results, evaluation_results, str(artifacts_dir))\n",
        "            train_time = time.time() - train_start\n",
        "            print(f\"‚úÖ {model_name.upper()}: Treinado e salvo em {train_time:.1f}s\")\n",
        "        except Exception as save_error:\n",
        "            train_time = time.time() - train_start\n",
        "            print(f\"‚ö†Ô∏è  {model_name.upper()}: Treinado em {train_time:.1f}s mas falhou ao salvar ({save_error})\")\n",
        "        \n",
        "        # Adicionar aos resultados\n",
        "        all_training_results[model_name] = training_results\n",
        "        all_evaluation_results.append(evaluation_results)\n",
        "        trained_models[model_name] = True\n",
        "        \n",
        "        # M√©tricas para compara√ß√£o\n",
        "        cv_metrics = training_results['cv_results']\n",
        "        threshold_metrics = evaluation_results['threshold_analysis']\n",
        "        optimal_metrics = threshold_metrics[threshold_metrics['threshold'] == evaluation_results['optimal_threshold']].iloc[0]\n",
        "        \n",
        "        model_comparison.append({\n",
        "            'model': model_name.upper(),\n",
        "            'roc_auc_cv': cv_metrics['roc_auc']['mean'],\n",
        "            'roc_auc_test': evaluation_results['roc_auc'],\n",
        "            'pr_auc': evaluation_results['pr_auc'],\n",
        "            'recall_cv': cv_metrics['recall']['mean'],\n",
        "            'recall_test': optimal_metrics['recall'],\n",
        "            'precision_test': optimal_metrics['precision'],\n",
        "            'f1_test': optimal_metrics['f1'],\n",
        "            'optimal_threshold': evaluation_results['optimal_threshold'],\n",
        "            'fraud_rate': optimal_metrics['predicted_fraud_rate'],\n",
        "            'compliant': evaluation_results['regulatory_compliance']['overall_compliant']\n",
        "        })\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå ERRO em {model_name.upper()}: {e}\")\n",
        "        trained_models[model_name] = False\n",
        "\n",
        "print(f\"üèÅ LIGHTGBM conclu√≠do - Status: {'‚úÖ Sucesso' if trained_models.get(model_name, False) else '‚ùå Falhou'}\")\n"
    ]
}

rf_cell = {
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# TREINAMENTO INDIVIDUAL: RANDOM FOREST\n",
        "print(\"\\nüöÄ TREINAMENTO RANDOM FOREST\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "model_name = 'random_forest'\n",
        "start_time = time.time()\n",
        "\n",
        "# Verificar cache\n",
        "if aml_trainer.check_model_cache(model_name, str(artifacts_dir)):\n",
        "    try:\n",
        "        trained_model, training_res, eval_res = aml_trainer.load_model_from_cache(model_name, str(artifacts_dir))\n",
        "        aml_trainer.trained_models[model_name] = trained_model\n",
        "        all_training_results[model_name] = training_res\n",
        "        all_evaluation_results.append(eval_res)\n",
        "        trained_models[model_name] = True\n",
        "        cache_time = time.time() - start_time\n",
        "        print(f\"‚úÖ {model_name.upper()}: Carregado do cache em {cache_time:.1f}s\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  {model_name.upper()}: Cache corrompido ({e}), treinando do zero\")\n",
        "        trained_models[model_name] = False\n",
        "else:\n",
        "    trained_models[model_name] = False\n",
        "\n",
        "# Treinar se necess√°rio\n",
        "if not trained_models[model_name]:\n",
        "    try:\n",
        "        train_start = time.time()\n",
        "        print(f\"üîÑ Treinando {model_name.upper()} com valida√ß√£o temporal...\")\n",
        "        \n",
        "        # Treino com valida√ß√£o temporal\n",
        "        training_results = aml_trainer.train_with_temporal_cv(X, y, model_name)\n",
        "        evaluation_results = aml_trainer.evaluate_model(X, y, model_name)\n",
        "        \n",
        "        # Salvar em cache\n",
        "        try:\n",
        "            aml_trainer.save_model_to_cache(model_name, aml_trainer.trained_models[model_name], \n",
        "                                          training_results, evaluation_results, str(artifacts_dir))\n",
        "            train_time = time.time() - train_start\n",
        "            print(f\"‚úÖ {model_name.upper()}: Treinado e salvo em {train_time:.1f}s\")\n",
        "        except Exception as save_error:\n",
        "            train_time = time.time() - train_start\n",
        "            print(f\"‚ö†Ô∏è  {model_name.upper()}: Treinado em {train_time:.1f}s mas falhou ao salvar ({save_error})\")\n",
        "        \n",
        "        # Adicionar aos resultados\n",
        "        all_training_results[model_name] = training_results\n",
        "        all_evaluation_results.append(evaluation_results)\n",
        "        trained_models[model_name] = True\n",
        "        \n",
        "        # M√©tricas para compara√ß√£o\n",
        "        cv_metrics = training_results['cv_results']\n",
        "        threshold_metrics = evaluation_results['threshold_analysis']\n",
        "        optimal_metrics = threshold_metrics[threshold_metrics['threshold'] == evaluation_results['optimal_threshold']].iloc[0]\n",
        "        \n",
        "        model_comparison.append({\n",
        "            'model': model_name.upper(),\n",
        "            'roc_auc_cv': cv_metrics['roc_auc']['mean'],\n",
        "            'roc_auc_test': evaluation_results['roc_auc'],\n",
        "            'pr_auc': evaluation_results['pr_auc'],\n",
        "            'recall_cv': cv_metrics['recall']['mean'],\n",
        "            'recall_test': optimal_metrics['recall'],\n",
        "            'precision_test': optimal_metrics['precision'],\n",
        "            'f1_test': optimal_metrics['f1'],\n",
        "            'optimal_threshold': evaluation_results['optimal_threshold'],\n",
        "            'fraud_rate': optimal_metrics['predicted_fraud_rate'],\n",
        "            'compliant': evaluation_results['regulatory_compliance']['overall_compliant']\n",
        "        })\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå ERRO em {model_name.upper()}: {e}\")\n",
        "        trained_models[model_name] = False\n",
        "\n",
        "print(f\"üèÅ RANDOM FOREST conclu√≠do - Status: {'‚úÖ Sucesso' if trained_models.get(model_name, False) else '‚ùå Falhou'}\")\n"
    ]
}

comparison_cell = {
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# COMPARA√á√ÉO DE MODELOS E SELE√á√ÉO DO MELHOR\n",
        "print(\"\\nüìä COMPARA√á√ÉO DE MODELOS E SELE√á√ÉO\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Verificar se temos modelos treinados\n",
        "trained_count = sum(trained_models.values())\n",
        "print(f\"Modelos treinados com sucesso: {trained_count}/{len(model_names)}\")\n",
        "\n",
        "if model_comparison:\n",
        "    print(\"\\nCompara√ß√£o de Modelos (Foco em Compliance AML):\")\n",
        "    comparison_df = pd.DataFrame(model_comparison)\n",
        "    if HAS_TABULATE:\n",
        "        print(tabulate(comparison_df.round(4), headers='keys', tablefmt='grid'))\n",
        "    else:\n",
        "        print(comparison_df.round(4))\n",
        "\n",
        "    # Selecionar melhor baseado em compliance AML\n",
        "    best_model_name, _, selection_justification = aml_trainer.select_best_aml_model(comparison_df, all_evaluation_results)\n",
        "    print(f\"\\nüèÜ Melhor Modelo Selecionado: {best_model_name}\")\n",
        "    print(f\"   Justificativa: {selection_justification}\")\n",
        "    \n",
        "    # Configurar melhor modelo\n",
        "    best_model_pipeline = aml_trainer.trained_models[best_model_name]\n",
        "    best_evaluation = all_evaluation_results[model_names.index(best_model_name)]\n",
        "    \n",
        "    # Vari√°veis globais para compatibilidade\n",
        "    training_results = all_training_results[best_model_name]\n",
        "    evaluation_results = best_evaluation\n",
        "    optimal_threshold = best_evaluation['optimal_threshold']\n",
        "    best_model = best_model_pipeline\n",
        "    \n",
        "    print(\"‚úÖ Sele√ß√£o de modelo conclu√≠da\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå ERRO: Nenhum modelo foi treinado com sucesso!\")\n",
        "    raise RuntimeError(\"ERRO CR√çTICO: Nenhum modelo foi treinado com sucesso. Verifique os logs acima.\")\n",
        "\n",
        "print(f\"\\nüìà Resumo Final:\")\n",
        "print(f\"   ‚Ä¢ Total de modelos: {len(model_names)}\")\n",
        "print(f\"   ‚Ä¢ Modelos treinados: {trained_count}\")\n",
        "print(f\"   ‚Ä¢ Melhor modelo: {best_model_name}\")\n",
        "print(f\"   ‚Ä¢ Modo de treinamento: {TRAINING_MODE.upper()}\")\n"
    ]
}

plot_cell = {
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# VISUALIZA√á√ÉO DO PROGRESSO DE TREINAMENTO\n",
        "print(\"\\nüìä VISUALIZA√á√ÉO DO TREINAMENTO\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Usar dados reais dos modelos treinados\n",
        "real_training_history = {}\n",
        "\n",
        "for model_name in model_names:\n",
        "    if model_name in all_training_results:\n",
        "        training_res = all_training_results[model_name]\n",
        "        if 'training_history' in training_res:\n",
        "            real_training_history[model_name] = training_res['training_history']\n",
        "        else:\n",
        "            # Criar hist√≥rico b√°sico se n√£o dispon√≠vel\n",
        "            real_training_history[model_name] = {\n",
        "                'iterations': [],\n",
        "                'val_auc': [],\n",
        "                'train_auc': [],\n",
        "                'oob_score': [],\n",
        "                'best_iteration': None\n",
        "            }\n",
        "\n",
        "# Configurar subplots para m√∫ltiplos modelos\n",
        "n_models = len(model_names)\n",
        "fig, axes = plt.subplots(n_models, 1, figsize=(12, 4*n_models))\n",
        "if n_models == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "plot_idx = 0\n",
        "for model_name in model_names:\n",
        "    history = real_training_history.get(model_name, {})\n",
        "    ax = axes[plot_idx]\n",
        "\n",
        "    # Plotar m√©tricas de treinamento\n",
        "    if 'val_auc' in history and history['val_auc'] and len(history['val_auc']) > 0:\n",
        "        iterations = history.get('iterations', list(range(1, len(history['val_auc']) + 1)))\n",
        "        ax.plot(iterations, history['val_auc'], 'b-', label='AUC Valida√ß√£o', linewidth=2)\n",
        "        ax.fill_between(iterations, history['val_auc'], alpha=0.1, color='blue')\n",
        "    \n",
        "    if 'train_auc' in history and history['train_auc'] and len(history['train_auc']) > 0:\n",
        "        iterations = history.get('iterations', list(range(1, len(history['train_auc']) + 1)))\n",
        "        ax.plot(iterations, history['train_auc'], 'r--', label='AUC Treino', alpha=0.7)\n",
        "    \n",
        "    if 'oob_score' in history and history['oob_score'] and len(history['oob_score']) > 0:\n",
        "        iterations = history.get('iterations', list(range(1, len(history['oob_score']) + 1)))\n",
        "        ax.plot(iterations, history['oob_score'], 'g-', label='OOB Score', linewidth=2)\n",
        "        ax.fill_between(iterations, history['oob_score'], alpha=0.1, color='green')\n",
        "\n",
        "    # Marcar melhor itera√ß√£o se dispon√≠vel\n",
        "    if history.get('best_iteration') and history['best_iteration']:\n",
        "        best_iter = history['best_iteration']\n",
        "        if 'val_auc' in history and history['val_auc'] and len(history['val_auc']) >= best_iter:\n",
        "            best_val = history['val_auc'][best_iter - 1]  # -1 porque listas s√£o 0-indexed\n",
        "            ax.plot(best_iter, best_val, 'ko', markersize=8, label=f'Melhor Itera√ß√£o ({best_iter})')\n",
        "        elif 'oob_score' in history and history['oob_score'] and len(history['oob_score']) >= best_iter:\n",
        "            best_oob = history['oob_score'][best_iter - 1]\n",
        "            ax.plot(best_iter, best_oob, 'ko', markersize=8, label=f'Melhor Itera√ß√£o ({best_iter})')\n",
        "\n",
        "    ax.set_title(f'{model_name.upper()} - Evolu√ß√£o do Treinamento (DADOS REAIS)', fontsize=12, fontweight='bold')\n",
        "    ax.set_xlabel('Itera√ß√µes / Estimadores')\n",
        "    ax.set_ylabel('AUC / OOB Score')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Adicionar informa√ß√µes de estabilidade\n",
        "    if 'val_auc' in history and history['val_auc'] and len(history['val_auc']) > 0:\n",
        "        final_auc = history['val_auc'][-1]\n",
        "        stability = \"Est√°vel\" if len(history['val_auc']) > 10 and np.std(history['val_auc'][-10:]) < 0.01 else \"Inst√°vel\"\n",
        "        ax.text(0.02, 0.98, f'AUC Final: {final_auc:.4f}\\nEstabilidade: {stability}',\n",
        "               transform=ax.transAxes, verticalalignment='top',\n",
        "               bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
        "    elif 'oob_score' in history and history['oob_score'] and len(history['oob_score']) > 0:\n",
        "        final_oob = history['oob_score'][-1]\n",
        "        ax.text(0.02, 0.98, f'OOB Final: {final_oob:.4f}',\n",
        "               transform=ax.transAxes, verticalalignment='top',\n",
        "               bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
        "\n",
        "    plot_idx += 1\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Resumo da converg√™ncia\n",
        "print(\"\\n[CONVERGENCE SUMMARY - DADOS REAIS]\")\n",
        "for model_name in model_names:\n",
        "    history = real_training_history.get(model_name, {})\n",
        "    if 'val_auc' in history and history['val_auc'] and len(history['val_auc']) > 1:\n",
        "        final_auc = history['val_auc'][-1]\n",
        "        convergence_rate = (final_auc - history['val_auc'][0]) / len(history['val_auc'])\n",
        "        print(f\"   {model_name.upper()}: AUC Final {final_auc:.4f}, Taxa de Converg√™ncia {convergence_rate:.6f}/iter\")\n",
        "    elif 'oob_score' in history and history['oob_score'] and len(history['oob_score']) > 0:\n",
        "        final_oob = history['oob_score'][-1]\n",
        "        print(f\"   {model_name.upper()}: OOB Final {final_oob:.4f}\")\n",
        "\n",
        "print(\"\\n[SUCCESS] Visualiza√ß√£o conclu√≠da com dados reais dos modelos treinados!\")\n"
    ]
}

# Substituir a c√©lula de treinamento pelas novas c√©lulas
nb['cells'][training_cell_idx] = setup_cell
nb['cells'].insert(training_cell_idx + 1, xgboost_cell)
nb['cells'].insert(training_cell_idx + 2, lightgbm_cell)
nb['cells'].insert(training_cell_idx + 3, rf_cell)
nb['cells'].insert(training_cell_idx + 4, comparison_cell)
nb['cells'].insert(training_cell_idx + 5, plot_cell)

# Salvar o notebook modificado
with open('notebooks/03_Modelagem_e_Avaliacao.ipynb', 'w', encoding='utf-8') as f:
    json.dump(nb, f, indent=1, ensure_ascii=True)

print(f"Notebook modificado salvo com {len(nb['cells'])} c√©lulas")
print("‚úÖ Separa√ß√£o conclu√≠da com sucesso!")
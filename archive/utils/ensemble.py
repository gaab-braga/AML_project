"""
Ensemble Module - Advanced Ensemble Methods for AML Detection
================================================================

This module provides comprehensive ensemble functionality organized in sections:

SECTION 1: CUSTOM STACKING ENSEMBLE (Lines ~35-530)
    Classes:
    - StackingResult: Dataclass for stacking results
    - StackingEnsemble: Custom stacking with out-of-fold predictions
    - WeightedVotingEnsemble: Weighted voting ensemble
    
    Functions:
    - create_default_stacking_ensemble: Quick LightGBM+XGBoost+CatBoost
    - compare_ensemble_strategies: Compare stacking vs voting vs averaging

SECTION 2: ENHANCED ENSEMBLE WITH ANOMALY (Lines ~730-1350)
    Classes:
    - EnhancedEnsembleRiskScorer: Combines supervised + anomaly + graph
    
    Functions:
    - run_enhanced_ensemble_pipeline: Complete enhanced pipeline

SECTION 3: SCORE FUSION UTILITIES (Lines ~1350-1490)
    Functions:
    - rank_scale: Rank-based scaling
    - blend_scores: Blend multiple scores (rank_mean, weighted, etc)
    - build_ensemble_outputs: Build ensemble dataframe with risk bands
    - precision_at_k, recall_at_k, lift_at_k: DEPRECATED - use .metrics

SECTION 4: SKLEARN-BASED ENSEMBLE (Lines ~1490-1727)
    Functions:
    - create_stacking_ensemble: sklearn StackingClassifier wrapper
    - create_voting_ensemble: sklearn VotingClassifier wrapper
    - evaluate_ensemble: Evaluate with fraud metrics
    - compare_ensemble_vs_individual: Compare strategies
    - quick_lgbm_xgb_ensemble: Quick LightGBM+XGBoost

USAGE:
    # Custom stacking
    from utils.ensemble import StackingEnsemble, create_default_stacking_ensemble
    
    # Enhanced ensemble with anomaly
    from utils.ensemble import EnhancedEnsembleRiskScorer
    
    # Score fusion
    from utils.ensemble import blend_scores, build_ensemble_outputs
    
    # sklearn-based
    from utils.ensemble import create_stacking_ensemble, create_voting_ensemble

⚠️ NOTE: This module uses relative imports (.metrics, .modeling) for proper
         package structure. Some functions are DEPRECATED (marked in docstrings).

⚠️ WARNING: This file was generated by concatenation and may contain some
            duplicate code. Consider refactoring into separate modules.
"""

import numpy as np
import pandas as pd
from typing import List, Dict, Tuple, Optional, Union, Any, Iterable
from sklearn.model_selection import StratifiedKFold, cross_val_predict
from sklearn.linear_model import LogisticRegression
from sklearn.base import BaseEstimator, ClassifierMixin, clone
from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, precision_recall_curve, auc
from sklearn.ensemble import VotingClassifier, RandomForestClassifier, StackingClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.calibration import CalibratedClassifierCV
from lightgbm import LGBMClassifier
from xgboost import XGBClassifier
import logging
import time
from dataclasses import dataclass
from pathlib import Path
import warnings
import pickle
import json
from datetime import datetime

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ============================================================================
# PUBLIC API
# ============================================================================

__all__ = [
    # Section 1: Custom Stacking Ensemble
    'StackingResult',
    'StackingEnsemble',
    'WeightedVotingEnsemble',
    'create_default_stacking_ensemble',
    'compare_ensemble_strategies',
    
    # Section 2: Enhanced Ensemble with Anomaly
    'EnhancedEnsembleRiskScorer',
    'run_enhanced_ensemble_pipeline',
    
    # Section 3: Score Fusion Utilities
    'rank_scale',
    'blend_scores',
    'build_ensemble_outputs',
    # Deprecated (kept for backward compatibility)
    'precision_at_k',
    'recall_at_k', 
    'lift_at_k',
    
    # Section 4: sklearn-based Ensemble
    'create_stacking_ensemble',
    'create_voting_ensemble',
    'evaluate_ensemble',
    'compare_ensemble_vs_individual',
    'quick_lgbm_xgb_ensemble'
]


@dataclass
class StackingResult:
    """Resultado do treinamento de stacking ensemble."""
    meta_model: Any
    base_models: List[Any]
    base_predictions_train: np.ndarray
    base_predictions_test: Optional[np.ndarray]
    cv_scores: Dict[str, float]
    train_time: float
    feature_importance: Optional[pd.DataFrame]
    
    def __repr__(self):
        return (f"StackingResult(meta_model={type(self.meta_model).__name__}, "
                f"n_base_models={len(self.base_models)}, "
                f"cv_roc_auc={self.cv_scores.get('roc_auc', 0):.4f})")


class StackingEnsemble(BaseEstimator, ClassifierMixin):
    """
    Stacking Ensemble com múltiplos modelos base e meta-learner.
    
    Arquitetura:
    -----------
    Level 0 (Base Models):
        - LightGBM
        - XGBoost  
        - CatBoost
        ↓ (out-of-fold predictions)
    Level 1 (Meta-Learner):
        - Logistic Regression (padrão)
        - OU outro modelo (LightGBM, XGBoost)
        ↓
    Final Prediction
    
    Parâmetros
    ----------
    base_models : List[Tuple[str, estimator]]
        Lista de tuplas (nome, modelo) com os modelos base
    meta_learner : estimator, default=LogisticRegression
        Meta-learner para nível 1
    cv : int, default=5
        Número de folds para cross-validation
    use_probas : bool, default=True
        Se True, usa probabilidades dos base models. Se False, usa classes
    use_original_features : bool, default=False
        Se True, concatena features originais com predições
    random_state : int, default=42
        Seed para reprodutibilidade
        
    Atributos
    ---------
    trained_base_models_ : List[List[estimator]]
        Modelos base treinados para cada fold
    meta_learner_ : estimator
        Meta-learner treinado
    cv_scores_ : Dict[str, float]
        Scores de validação cruzada
    
    Exemplo
    -------
    >>> from lightgbm import LGBMClassifier
    >>> from xgboost import XGBClassifier
    >>> 
    >>> base_models = [
    ...     ('lgbm', LGBMClassifier(random_state=42)),
    ...     ('xgb', XGBClassifier(random_state=42))
    ... ]
    >>> 
    >>> stacker = StackingEnsemble(base_models=base_models, cv=5)
    >>> stacker.fit(X_train, y_train)
    >>> y_pred = stacker.predict_proba(X_test)[:, 1]
    """
    
    def __init__(
        self,
        base_models: List[Tuple[str, BaseEstimator]],
        meta_learner: Optional[BaseEstimator] = None,
        cv: int = 5,
        use_probas: bool = True,
        use_original_features: bool = False,
        random_state: int = 42
    ):
        self.base_models = base_models
        self.meta_learner = meta_learner
        self.cv = cv
        self.use_probas = use_probas
        self.use_original_features = use_original_features
        self.random_state = random_state
        
        # Inicializar meta-learner padrão
        if self.meta_learner is None:
            self.meta_learner = LogisticRegression(
                max_iter=1000,
                random_state=random_state,
                class_weight='balanced'
            )
    
    def fit(self, X: Union[np.ndarray, pd.DataFrame], y: Union[np.ndarray, pd.Series]):
        """
        Treina o stacking ensemble.
        
        Passos:
        1. Para cada fold:
           - Treina base models no train set
           - Gera predições out-of-fold
        2. Treina meta-learner com predições out-of-fold
        3. Re-treina base models no dataset completo
        
        Parâmetros
        ----------
        X : array-like of shape (n_samples, n_features)
            Features de treinamento
        y : array-like of shape (n_samples,)
            Target
            
        Returns
        -------
        self : StackingEnsemble
            Fitted estimator
        """
        start_time = time.time()
        
        if isinstance(X, pd.DataFrame):
            X = X.values
        if isinstance(y, pd.Series):
            y = y.values
            
        logger.info(f"🏗️  Iniciando Stacking Ensemble com {len(self.base_models)} base models")
        logger.info(f"   CV: {self.cv} folds | Meta-learner: {type(self.meta_learner).__name__}")
        
        # Step 1: Generate out-of-fold predictions
        oof_predictions = self._generate_oof_predictions(X, y)
        
        # Step 2: Train meta-learner
        logger.info(f"🎓 Treinando meta-learner...")
        if self.use_original_features:
            meta_features = np.hstack([X, oof_predictions])
            logger.info(f"   Usando features originais + predições: {meta_features.shape}")
        else:
            meta_features = oof_predictions
            
        self.meta_learner_ = clone(self.meta_learner)
        self.meta_learner_.fit(meta_features, y)
        
        # Step 3: Re-train base models on full dataset
        logger.info(f"♻️  Re-treinando base models no dataset completo...")
        self.final_base_models_ = []
        for name, model in self.base_models:
            logger.info(f"   Treinando {name}...")
            fitted_model = clone(model)
            fitted_model.fit(X, y)
            self.final_base_models_.append((name, fitted_model))
        
        # Calculate CV scores
        y_pred_meta = self.meta_learner_.predict_proba(meta_features)[:, 1]
        self.cv_scores_ = {
            'roc_auc': roc_auc_score(y, y_pred_meta),
            'pr_auc': average_precision_score(y, y_pred_meta),
            'f1': f1_score(y, (y_pred_meta > 0.5).astype(int))
        }
        
        self.train_time_ = time.time() - start_time
        
        logger.info(f"✅ Stacking completo em {self.train_time_:.2f}s")
        logger.info(f"   ROC-AUC: {self.cv_scores_['roc_auc']:.4f}")
        logger.info(f"   PR-AUC:  {self.cv_scores_['pr_auc']:.4f}")
        
        return self
    
    def _generate_oof_predictions(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:
        """
        Gera predições out-of-fold para evitar overfitting.
        
        Para cada fold:
        - Treina modelos no train
        - Prediz no validation
        - Armazena predições out-of-fold
        """
        n_samples = X.shape[0]
        n_models = len(self.base_models)
        oof_predictions = np.zeros((n_samples, n_models))
        
        # Store trained models for each fold
        self.trained_base_models_ = [[] for _ in range(n_models)]
        
        # Stratified K-Fold
        skf = StratifiedKFold(n_splits=self.cv, shuffle=True, random_state=self.random_state)
        
        for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X, y)):
            logger.info(f"📊 Fold {fold_idx + 1}/{self.cv}")
            
            X_train_fold, X_val_fold = X[train_idx], X[val_idx]
            y_train_fold, y_val_fold = y[train_idx], y[val_idx]
            
            for model_idx, (name, model) in enumerate(self.base_models):
                # Clone and train
                model_clone = clone(model)
                model_clone.fit(X_train_fold, y_train_fold)
                
                # Store trained model
                self.trained_base_models_[model_idx].append(model_clone)
                
                # Generate out-of-fold predictions
                if self.use_probas:
                    preds = model_clone.predict_proba(X_val_fold)[:, 1]
                else:
                    preds = model_clone.predict(X_val_fold)
                
                oof_predictions[val_idx, model_idx] = preds
                
                # Log score
                score = roc_auc_score(y_val_fold, preds)
                logger.info(f"   {name}: ROC-AUC = {score:.4f}")
        
        return oof_predictions
    
    def predict_proba(self, X: Union[np.ndarray, pd.DataFrame]) -> np.ndarray:
        """
        Prediz probabilidades usando ensemble.
        
        Passos:
        1. Gera predições de cada base model
        2. Meta-learner combina predições
        
        Parâmetros
        ----------
        X : array-like of shape (n_samples, n_features)
            Features
            
        Returns
        -------
        proba : ndarray of shape (n_samples, 2)
            Probabilidades [P(y=0), P(y=1)]
        """
        if isinstance(X, pd.DataFrame):
            X = X.values
        
        # Generate predictions from base models
        base_predictions = self._get_base_predictions(X)
        
        # Combine with original features if needed
        if self.use_original_features:
            meta_features = np.hstack([X, base_predictions])
        else:
            meta_features = base_predictions
        
        # Meta-learner prediction
        return self.meta_learner_.predict_proba(meta_features)
    
    def predict(self, X: Union[np.ndarray, pd.DataFrame]) -> np.ndarray:
        """Prediz classes."""
        return (self.predict_proba(X)[:, 1] > 0.5).astype(int)
    
    def _get_base_predictions(self, X: np.ndarray) -> np.ndarray:
        """Obtém predições dos base models."""
        n_samples = X.shape[0]
        n_models = len(self.final_base_models_)
        predictions = np.zeros((n_samples, n_models))
        
        for idx, (name, model) in enumerate(self.final_base_models_):
            if self.use_probas:
                predictions[:, idx] = model.predict_proba(X)[:, 1]
            else:
                predictions[:, idx] = model.predict(X)
        
        return predictions
    
    def get_feature_importance(self) -> pd.DataFrame:
        """
        Retorna feature importance do meta-learner.
        
        Returns
        -------
        importance : pd.DataFrame
            Feature importance com colunas [model, importance]
        """
        model_names = [name for name, _ in self.final_base_models_]
        
        # Para LogisticRegression, usa coeficientes
        if hasattr(self.meta_learner_, 'coef_'):
            importances = np.abs(self.meta_learner_.coef_[0])
        # Para tree-based, usa feature_importances_
        elif hasattr(self.meta_learner_, 'feature_importances_'):
            importances = self.meta_learner_.feature_importances_[:len(model_names)]
        else:
            importances = np.ones(len(model_names)) / len(model_names)
        
        return pd.DataFrame({
            'model': model_names,
            'importance': importances,
            'importance_pct': importances / importances.sum() * 100
        }).sort_values('importance', ascending=False)


class WeightedVotingEnsemble(BaseEstimator, ClassifierMixin):
    """
    Ensemble com votação ponderada (alternativa mais simples ao stacking).
    
    Combina predições de múltiplos modelos usando pesos configuráveis.
    
    Parâmetros
    ----------
    models : List[Tuple[str, estimator]]
        Lista de tuplas (nome, modelo)
    weights : Optional[List[float]], default=None
        Pesos para cada modelo. Se None, usa pesos iguais
    voting : str, default='soft'
        'soft': média ponderada de probabilidades
        'hard': voto majoritário
        
    Exemplo
    -------
    >>> models = [
    ...     ('lgbm', LGBMClassifier()),
    ...     ('xgb', XGBClassifier()),
    ... ]
    >>> 
    >>> ensemble = WeightedVotingEnsemble(
    ...     models=models,
    ...     weights=[0.6, 0.4]  # LightGBM tem mais peso
    ... )
    >>> ensemble.fit(X_train, y_train)
    >>> y_pred = ensemble.predict_proba(X_test)[:, 1]
    """
    
    def __init__(
        self,
        models: List[Tuple[str, BaseEstimator]],
        weights: Optional[List[float]] = None,
        voting: str = 'soft'
    ):
        self.models = models
        self.weights = weights
        self.voting = voting
        
        # Validar weights
        if weights is not None:
            if len(weights) != len(models):
                raise ValueError(f"Número de weights ({len(weights)}) != número de models ({len(models)})")
            if not np.isclose(sum(weights), 1.0):
                logger.warning(f"Weights não somam 1.0 ({sum(weights)}). Normalizando...")
                self.weights = np.array(weights) / sum(weights)
        else:
            # Pesos iguais
            self.weights = np.ones(len(models)) / len(models)
    
    def fit(self, X: Union[np.ndarray, pd.DataFrame], y: Union[np.ndarray, pd.Series]):
        """Treina todos os modelos."""
        if isinstance(X, pd.DataFrame):
            X = X.values
        if isinstance(y, pd.Series):
            y = y.values
        
        logger.info(f"🗳️  Treinando Weighted Voting Ensemble ({len(self.models)} modelos)")
        
        self.fitted_models_ = []
        for name, model in self.models:
            logger.info(f"   Treinando {name}...")
            fitted_model = clone(model)
            fitted_model.fit(X, y)
            self.fitted_models_.append((name, fitted_model))
        
        logger.info(f"✅ Ensemble treinado | Weights: {self.weights}")
        return self
    
    def predict_proba(self, X: Union[np.ndarray, pd.DataFrame]) -> np.ndarray:
        """Prediz probabilidades com votação ponderada."""
        if isinstance(X, pd.DataFrame):
            X = X.values
        
        if self.voting == 'soft':
            # Média ponderada de probabilidades
            proba_sum = np.zeros((X.shape[0], 2))
            
            for (name, model), weight in zip(self.fitted_models_, self.weights):
                proba = model.predict_proba(X)
                proba_sum += proba * weight
            
            return proba_sum
        
        else:  # hard voting
            # Voto majoritário
            predictions = []
            for name, model in self.fitted_models_:
                pred = model.predict(X)
                predictions.append(pred)
            
            predictions = np.array(predictions).T
            
            # Contar votos
            votes_0 = (predictions == 0).sum(axis=1)
            votes_1 = (predictions == 1).sum(axis=1)
            
            proba = np.column_stack([votes_0, votes_1]) / len(self.fitted_models_)
            return proba
    
    def predict(self, X: Union[np.ndarray, pd.DataFrame]) -> np.ndarray:
        """Prediz classes."""
        return (self.predict_proba(X)[:, 1] > 0.5).astype(int)


def create_default_stacking_ensemble(use_gpu: bool = False, random_state: int = 42) -> StackingEnsemble:
    """
    Cria ensemble de stacking com configuração padrão para AML.
    
    Base Models:
    - LightGBM (rápido, eficiente)
    - XGBoost (robusto)
    - CatBoost (categóricas nativas)
    
    Meta-learner:
    - LogisticRegression (simples, interpretável)
    
    Parâmetros
    ----------
    use_gpu : bool, default=False
        Se True, usa GPU para CatBoost
    random_state : int, default=42
        Seed
        
    Returns
    -------
    ensemble : StackingEnsemble
        Ensemble configurado
    """
    try:
        from catboost import CatBoostClassifier
        has_catboost = True
    except ImportError:
        has_catboost = False
        logger.warning("CatBoost não instalado. Usando apenas LightGBM e XGBoost.")
    
    # Base models
    base_models = [
        ('lgbm', LGBMClassifier(
            n_estimators=500,
            learning_rate=0.05,
            max_depth=7,
            num_leaves=50,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=random_state,
            class_weight='balanced',
            verbose=-1
        )),
        ('xgb', XGBClassifier(
            n_estimators=500,
            learning_rate=0.05,
            max_depth=7,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=random_state,
            eval_metric='logloss',
            verbosity=0
        ))
    ]
    
    if has_catboost:
        base_models.append(
            ('catboost', CatBoostClassifier(
                iterations=500,
                learning_rate=0.05,
                depth=7,
                subsample=0.8,
                auto_class_weights='Balanced',
                random_state=random_state,
                verbose=False,
                task_type='GPU' if use_gpu else 'CPU'
            ))
        )
    
    # Meta-learner: LogisticRegression
    meta_learner = LogisticRegression(
        max_iter=1000,
        random_state=random_state,
        class_weight='balanced',
        penalty='l2',
        C=1.0
    )
    
    return StackingEnsemble(
        base_models=base_models,
        meta_learner=meta_learner,
        cv=5,
        use_probas=True,
        use_original_features=False,
        random_state=random_state
    )


def compare_ensemble_strategies(
    X_train: Union[np.ndarray, pd.DataFrame],
    y_train: Union[np.ndarray, pd.Series],
    X_test: Union[np.ndarray, pd.DataFrame],
    y_test: Union[np.ndarray, pd.Series],
    random_state: int = 42
) -> pd.DataFrame:
    """
    Compara diferentes estratégias de ensemble.
    
    Estratégias testadas:
    1. Stacking (LogisticRegression meta-learner)
    2. Stacking (LightGBM meta-learner)
    3. Weighted Voting (soft)
    4. Simple Averaging
    
    Parâmetros
    ----------
    X_train, y_train : Dados de treino
    X_test, y_test : Dados de teste
    random_state : Seed
    
    Returns
    -------
    comparison : pd.DataFrame
        Tabela comparativa com métricas
    """
    if isinstance(X_train, pd.DataFrame):
        X_train = X_train.values
    if isinstance(X_test, pd.DataFrame):
        X_test = X_test.values
    if isinstance(y_train, pd.Series):
        y_train = y_train.values
    if isinstance(y_test, pd.Series):
        y_test = y_test.values
    
    logger.info("🔬 Comparando estratégias de ensemble...")
    
    # Base models configuration
    base_models = [
        ('lgbm', LGBMClassifier(n_estimators=300, random_state=random_state, class_weight='balanced', verbose=-1)),
        ('xgb', XGBClassifier(n_estimators=300, random_state=random_state, eval_metric='logloss', verbosity=0))
    ]
    
    results = []
    
    # 1. Stacking com LogisticRegression
    logger.info("\n1️⃣ Stacking (LogisticRegression)")
    stacking_lr = StackingEnsemble(
        base_models=base_models,
        meta_learner=LogisticRegression(max_iter=1000, random_state=random_state),
        cv=3,
        random_state=random_state
    )
    start = time.time()
    stacking_lr.fit(X_train, y_train)
    train_time_lr = time.time() - start
    y_pred_lr = stacking_lr.predict_proba(X_test)[:, 1]
    
    results.append({
        'strategy': 'Stacking (LogReg)',
        'roc_auc': roc_auc_score(y_test, y_pred_lr),
        'pr_auc': average_precision_score(y_test, y_pred_lr),
        'f1': f1_score(y_test, (y_pred_lr > 0.5).astype(int)),
        'train_time_s': train_time_lr
    })
    
    # 2. Stacking com LightGBM
    logger.info("\n2️⃣ Stacking (LightGBM)")
    stacking_lgbm = StackingEnsemble(
        base_models=base_models,
        meta_learner=LGBMClassifier(n_estimators=100, random_state=random_state, verbose=-1),
        cv=3,
        random_state=random_state
    )
    start = time.time()
    stacking_lgbm.fit(X_train, y_train)
    train_time_lgbm = time.time() - start
    y_pred_lgbm = stacking_lgbm.predict_proba(X_test)[:, 1]
    
    results.append({
        'strategy': 'Stacking (LGBM)',
        'roc_auc': roc_auc_score(y_test, y_pred_lgbm),
        'pr_auc': average_precision_score(y_test, y_pred_lgbm),
        'f1': f1_score(y_test, (y_pred_lgbm > 0.5).astype(int)),
        'train_time_s': train_time_lgbm
    })
    
    # 3. Weighted Voting
    logger.info("\n3️⃣ Weighted Voting")
    voting = WeightedVotingEnsemble(
        models=base_models,
        weights=[0.6, 0.4],  # LightGBM ligeiramente melhor
        voting='soft'
    )
    start = time.time()
    voting.fit(X_train, y_train)
    train_time_voting = time.time() - start
    y_pred_voting = voting.predict_proba(X_test)[:, 1]
    
    results.append({
        'strategy': 'Weighted Voting',
        'roc_auc': roc_auc_score(y_test, y_pred_voting),
        'pr_auc': average_precision_score(y_test, y_pred_voting),
        'f1': f1_score(y_test, (y_pred_voting > 0.5).astype(int)),
        'train_time_s': train_time_voting
    })
    
    # 4. Simple Averaging (baseline)
    logger.info("\n4️⃣ Simple Averaging")
    # Treinar modelos individuais
    models_fitted = []
    start = time.time()
    for name, model in base_models:
        m = clone(model)
        m.fit(X_train, y_train)
        models_fitted.append(m)
    train_time_avg = time.time() - start
    
    # Média simples
    y_pred_avg = np.mean([m.predict_proba(X_test)[:, 1] for m in models_fitted], axis=0)
    
    results.append({
        'strategy': 'Simple Averaging',
        'roc_auc': roc_auc_score(y_test, y_pred_avg),
        'pr_auc': average_precision_score(y_test, y_pred_avg),
        'f1': f1_score(y_test, (y_pred_avg > 0.5).astype(int)),
        'train_time_s': train_time_avg
    })
    
    # Criar DataFrame
    comparison_df = pd.DataFrame(results).sort_values('roc_auc', ascending=False)
    
    logger.info("\n📊 Resultados:")
    print(comparison_df.to_string(index=False))
    
    return comparison_df


# Exemplo de uso
if __name__ == "__main__":
    from sklearn.datasets import make_classification
    
    # Gerar dados sintéticos
    X, y = make_classification(
        n_samples=5000,
        n_features=20,
        n_informative=15,
        n_redundant=5,
        n_classes=2,
        weights=[0.7, 0.3],  # Desbalanceado
        random_state=42
    )
    
    # Split
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)
    
    print("="*80)
    print("TESTE: Stacking Ensemble para AML Detection")
    print("="*80)
    
    # Criar ensemble padrão
    ensemble = create_default_stacking_ensemble(use_gpu=False)
    
    # Treinar
    ensemble.fit(X_train, y_train)
    
    # Predição
    y_pred = ensemble.predict_proba(X_test)[:, 1]
    
    # Métricas
    print("\n📊 Métricas no Test Set:")
    print(f"   ROC-AUC: {roc_auc_score(y_test, y_pred):.4f}")
    print(f"   PR-AUC:  {average_precision_score(y_test, y_pred):.4f}")
    print(f"   F1:      {f1_score(y_test, (y_pred > 0.5).astype(int)):.4f}")
    
    # Feature importance
    print("\n🎯 Contribuição dos Base Models:")
    print(ensemble.get_feature_importance())
    
    # Comparar estratégias
    print("\n" + "="*80)
    print("COMPARAÇÃO DE ESTRATÉGIAS")
    print("="*80)
    comparison = compare_ensemble_strategies(X_train, y_train, X_test, y_test)

"""
Enhanced Ensemble Module with Anomaly Integration

Advanced ensemble combining supervised models with anomaly detection
and graph-based features for comprehensive money laundering detection.
"""

# import json  # Already imported at top
# import pandas as pd  # Already imported at top
# import numpy as np  # Already imported at top
# from datetime import datetime  # Already imported at top
# from pathlib import Path  # Already imported at top
# from typing import Dict, Any, List, Tuple, Optional, Union  # Already imported at top
# DUPLICATE IMPORTS REMOVED - All imports consolidated at top of file
# Suppress warnings for cleaner output
warnings.filterwarnings('ignore', category=UserWarning)


class EnhancedEnsembleRiskScorer:
    """
    Advanced ensemble that integrates:
    - Supervised ML models (baseline + tuned)
    - Anomaly detection scores
    - Graph network features
    - Temporal patterns
    - Risk score calibration
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.models = {}
        self.scalers = {}
        self.anomaly_models = {}
        self.feature_importance = {}
        self.ensemble_weights = {}
        self.calibration_model = None
        self.is_fitted = False
        
    def load_base_models(self, models_dir: Path, artifacts_dir: Path) -> None:
        """Load pre-trained baseline and tuned models."""
        models_dir = Path(models_dir)
        artifacts_dir = Path(artifacts_dir)
        
        print("🔧 Loading base models for ensemble...")
        
        # Load baseline model
        baseline_path = models_dir / 'best_baseline.pkl'
        if baseline_path.exists():
            with open(baseline_path, 'rb') as f:
                self.models['baseline'] = pickle.load(f)
            print("   ✅ Loaded baseline model")
        else:
            print("   ⚠️ Baseline model not found")
        
        # Load tuned model
        tuned_path = models_dir / 'best_model_tuned.pkl'
        if tuned_path.exists():
            with open(tuned_path, 'rb') as f:
                self.models['tuned'] = pickle.load(f)
            print("   ✅ Loaded tuned model")
        else:
            print("   ⚠️ Tuned model not found")
        
        # Load scaler
        scaler_path = artifacts_dir / 'scaler.pkl'
        if scaler_path.exists():
            with open(scaler_path, 'rb') as f:
                self.scalers['main'] = pickle.load(f)
            print("   ✅ Loaded feature scaler")
        
        # Load anomaly models
        anomaly_path = artifacts_dir / 'anomaly_models.pkl'
        if anomaly_path.exists():
            with open(anomaly_path, 'rb') as f:
                self.anomaly_models = pickle.load(f)
            print(f"   ✅ Loaded {len(self.anomaly_models)} anomaly detection models")
        else:
            print("   ⚠️ Anomaly models not found")
    
    def load_graph_features(self, data_dir: Path) -> Tuple[Optional[pd.DataFrame], Optional[pd.DataFrame]]:
        """Load graph-based network features if available."""
        data_dir = Path(data_dir)
        
        train_graph_path = data_dir / 'X_train_graph.csv'
        test_graph_path = data_dir / 'X_test_graph.csv'
        
        if train_graph_path.exists() and test_graph_path.exists():
            train_graph = pd.read_csv(train_graph_path)
            test_graph = pd.read_csv(test_graph_path)
            print(f"   ✅ Loaded graph features: {len(train_graph.columns)} features")
            return train_graph, test_graph
        else:
            print("   ⚠️ Graph features not available")
            return None, None
    
    def compute_anomaly_scores(self, X: pd.DataFrame, dataset_type: str = 'train') -> pd.DataFrame:
        """Compute anomaly scores from multiple detectors."""
        if not self.anomaly_models:
            print("   ⚠️ No anomaly models available, using placeholder scores")
            return pd.DataFrame({
                'isolation_forest_score': np.random.random(len(X)) * 0.1,  # Low baseline
                'local_outlier_score': np.random.random(len(X)) * 0.1,
                'one_class_svm_score': np.random.random(len(X)) * 0.1
            })
        
        anomaly_scores = {}
        
        # Prepare features (use only numeric columns)
        X_numeric = X.select_dtypes(include=[np.number]).fillna(0)
        
        print(f"   Computing anomaly scores for {len(X)} {dataset_type} samples...")
        
        try:
            for model_name, model in self.anomaly_models.items():
                if hasattr(model, 'decision_function'):
                    # For Isolation Forest, One-Class SVM
                    scores = model.decision_function(X_numeric)
                    # Convert to anomaly probability (higher = more anomalous)
                    scores = 1 / (1 + np.exp(-scores))  # Sigmoid transformation
                elif hasattr(model, 'negative_outlier_factor_'):
                    # For Local Outlier Factor
                    scores = -model.negative_outlier_factor_
                    # Normalize to [0,1]
                    scores = (scores - scores.min()) / (scores.max() - scores.min())
                else:
                    # Fallback: use transform if available
                    scores = model.transform(X_numeric)
                    if scores.ndim > 1:
                        scores = scores.mean(axis=1)
                    scores = (scores - scores.min()) / (scores.max() - scores.min())
                
                anomaly_scores[f'{model_name}_score'] = scores
                
        except Exception as e:
            print(f"   ⚠️ Error computing anomaly scores: {str(e)}")
            # Fallback to placeholder scores
            anomaly_scores = {
                'isolation_forest_score': np.random.random(len(X)) * 0.1,
                'local_outlier_score': np.random.random(len(X)) * 0.1,
                'one_class_svm_score': np.random.random(len(X)) * 0.1
            }
        
        return pd.DataFrame(anomaly_scores)
    
    def create_enhanced_features(
        self,
        X_main: pd.DataFrame,
        X_graph: Optional[pd.DataFrame] = None,
        dataset_type: str = 'train'
    ) -> pd.DataFrame:
        """Create enhanced feature set combining multiple sources."""
        
        print(f"   Creating enhanced features for {dataset_type} set...")
        
        enhanced_features = []
        
        # 1. Main features (scaled)
        X_main_numeric = X_main.select_dtypes(include=[np.number])
        if 'main' in self.scalers:
            X_main_scaled = pd.DataFrame(
                self.scalers['main'].transform(X_main_numeric),
                columns=X_main_numeric.columns,
                index=X_main_numeric.index
            )
        else:
            X_main_scaled = X_main_numeric
        
        enhanced_features.append(X_main_scaled)
        
        # 2. Anomaly scores
        anomaly_scores = self.compute_anomaly_scores(X_main, dataset_type)
        enhanced_features.append(anomaly_scores)
        
        # 3. Graph features (if available)
        if X_graph is not None:
            X_graph_numeric = X_graph.select_dtypes(include=[np.number])
            enhanced_features.append(X_graph_numeric)
            print(f"      Added {len(X_graph_numeric.columns)} graph features")
        
        # 4. Risk interaction features
        risk_features = self._create_risk_interaction_features(X_main_scaled, anomaly_scores)
        enhanced_features.append(risk_features)
        
        # Combine all features
        X_enhanced = pd.concat(enhanced_features, axis=1)
        
        # Handle any remaining NaN values
        X_enhanced = X_enhanced.fillna(0)
        
        print(f"      ✅ Enhanced features: {len(X_enhanced.columns)} total features")
        
        return X_enhanced
    
    def _create_risk_interaction_features(
        self,
        X_main: pd.DataFrame,
        anomaly_scores: pd.DataFrame
    ) -> pd.DataFrame:
        """Create interaction features between main features and anomaly scores."""
        
        risk_features = {}
        
        # Get top risk-relevant features (heuristic selection)
        amount_cols = [col for col in X_main.columns if 'amount' in col.lower() or 'value' in col.lower()]
        frequency_cols = [col for col in X_main.columns if 'freq' in col.lower() or 'count' in col.lower()]
        
        # Amount-anomaly interactions
        for col in amount_cols[:3]:  # Limit to top 3 for performance
            if col in X_main.columns:
                for anomaly_col in anomaly_scores.columns:
                    risk_features[f'{col}_{anomaly_col}_interaction'] = X_main[col] * anomaly_scores[anomaly_col]
        
        # Frequency-anomaly interactions
        for col in frequency_cols[:3]:
            if col in X_main.columns:
                for anomaly_col in anomaly_scores.columns:
                    risk_features[f'{col}_{anomaly_col}_interaction'] = X_main[col] * anomaly_scores[anomaly_col]
        
        # Combined anomaly risk score
        risk_features['combined_anomaly_score'] = anomaly_scores.mean(axis=1)
        risk_features['max_anomaly_score'] = anomaly_scores.max(axis=1)
        risk_features['anomaly_score_variance'] = anomaly_scores.var(axis=1)
        
        return pd.DataFrame(risk_features)
    
    def fit_ensemble(
        self,
        X_train: pd.DataFrame,
        y_train: pd.Series,
        X_train_graph: Optional[pd.DataFrame] = None,
        cv_folds: int = 5
    ) -> None:
        """Fit the enhanced ensemble model."""
        
        print("🚀 Training enhanced ensemble model...")
        
        # Create enhanced training features
        X_train_enhanced = self.create_enhanced_features(
            X_train, X_train_graph, 'train'
        )
        
        print(f"   Training on {len(X_train_enhanced)} samples with {len(X_train_enhanced.columns)} features")
        
        # Define ensemble components
        ensemble_models = {}
        
        # 1. Use existing trained models if available
        if 'baseline' in self.models:
            # Extract probabilities from baseline model
            baseline_proba = self._get_model_probabilities(self.models['baseline'], X_train_enhanced)
            ensemble_models['baseline_proba'] = baseline_proba[:, 1] if baseline_proba.ndim > 1 else baseline_proba
        
        if 'tuned' in self.models:
            tuned_proba = self._get_model_probabilities(self.models['tuned'], X_train_enhanced)
            ensemble_models['tuned_proba'] = tuned_proba[:, 1] if tuned_proba.ndim > 1 else tuned_proba
        
        # 2. Train specialized ensemble components
        
        # Anomaly-focused model (focuses on anomaly scores + interactions)
        anomaly_cols = [col for col in X_train_enhanced.columns if 'anomaly' in col or 'interaction' in col]
        if anomaly_cols:
            anomaly_model = LogisticRegression(
                class_weight='balanced',
                random_state=42,
                max_iter=1000
            )
            anomaly_model.fit(X_train_enhanced[anomaly_cols], y_train)
            ensemble_models['anomaly_specialist'] = anomaly_model.predict_proba(X_train_enhanced[anomaly_cols])[:, 1]
        
        # Graph-focused model (if graph features available)
        graph_cols = [col for col in X_train_enhanced.columns if 'graph' in col or 'network' in col]
        if graph_cols:
            graph_model = RandomForestClassifier(
                n_estimators=100,
                class_weight='balanced',
                random_state=42
            )
            graph_model.fit(X_train_enhanced[graph_cols], y_train)
            ensemble_models['graph_specialist'] = graph_model.predict_proba(X_train_enhanced[graph_cols])[:, 1]
        
        # 3. Meta-learner ensemble
        if len(ensemble_models) > 1:
            # Create meta-features from component predictions
            meta_features = pd.DataFrame(ensemble_models)
            
            # Add some engineered meta-features
            meta_features['prediction_variance'] = meta_features.var(axis=1)
            meta_features['prediction_max'] = meta_features.max(axis=1)
            meta_features['prediction_mean'] = meta_features.mean(axis=1)
            
            # Train meta-learner
            meta_model = LogisticRegression(
                class_weight='balanced',
                random_state=42,
                max_iter=1000
            )
            meta_model.fit(meta_features, y_train)
            
            self.models['meta_learner'] = meta_model
            self.models['component_models'] = ensemble_models
            
            print(f"   ✅ Trained ensemble with {len(ensemble_models)} components")
        else:
            print("   ⚠️ Insufficient models for ensemble, using single model approach")
            if ensemble_models:
                model_name, predictions = list(ensemble_models.items())[0]
                self.models['single_model'] = {model_name: predictions}
        
        # 4. Calibration model
        self._fit_calibration_model(X_train_enhanced, y_train)
        
        self.is_fitted = True
        print("   ✅ Enhanced ensemble training completed")
    
    def _get_model_probabilities(self, model, X: pd.DataFrame) -> np.ndarray:
        """Get probability predictions from a model, handling different model types."""
        try:
            if hasattr(model, 'predict_proba'):
                return model.predict_proba(X)
            elif hasattr(model, 'decision_function'):
                scores = model.decision_function(X)
                # Convert to probabilities using sigmoid
                probabilities = 1 / (1 + np.exp(-scores))
                return np.column_stack([1 - probabilities, probabilities])
            else:
                # Fallback: assume model outputs predictions directly
                predictions = model.predict(X)
                return np.column_stack([1 - predictions, predictions])
        except Exception as e:
            print(f"   ⚠️ Error getting probabilities from model: {str(e)}")
            # Return random probabilities as fallback
            n_samples = len(X)
            return np.column_stack([
                np.random.random(n_samples) * 0.9 + 0.05,  # Negative class
                np.random.random(n_samples) * 0.1 + 0.05   # Positive class
            ])
    
    def _fit_calibration_model(self, X: pd.DataFrame, y: pd.Series) -> None:
        """Fit calibration model for probability calibration."""
        try:
            from sklearn.calibration import CalibratedClassifierCV
            
            # Use a simple model for calibration
            base_calibration_model = LogisticRegression(
                class_weight='balanced',
                random_state=42,
                max_iter=1000
            )
            
            # Fit with calibration
            self.calibration_model = CalibratedClassifierCV(
                base_calibration_model,
                method='isotonic',
                cv=3
            )
            
            # Use subset of features for calibration to avoid overfitting
            feature_subset = X.columns[:min(20, len(X.columns))]  # Top 20 features
            self.calibration_model.fit(X[feature_subset], y)
            self.calibration_features = feature_subset.tolist()
            
            print("   ✅ Calibration model fitted")
            
        except Exception as e:
            print(f"   ⚠️ Calibration model fitting failed: {str(e)}")
            self.calibration_model = None
    
    def predict_enhanced_risk_scores(
        self,
        X_test: pd.DataFrame,
        X_test_graph: Optional[pd.DataFrame] = None
    ) -> Dict[str, np.ndarray]:
        """Generate comprehensive risk scores using the enhanced ensemble."""
        
        if not self.is_fitted:
            raise ValueError("Ensemble must be fitted before prediction")
        
        print("🎯 Generating enhanced risk scores...")
        
        # Create enhanced test features
        X_test_enhanced = self.create_enhanced_features(
            X_test, X_test_graph, 'test'
        )
        
        risk_scores = {}
        
        # 1. Component model predictions
        if 'meta_learner' in self.models:
            # Get component predictions
            component_predictions = {}
            for model_name, predictions in self.models['component_models'].items():
                if model_name.endswith('_proba'):
                    # These are stored training predictions, need to predict on test
                    if model_name.startswith('baseline') and 'baseline' in self.models:
                        test_proba = self._get_model_probabilities(self.models['baseline'], X_test_enhanced)
                        component_predictions[model_name] = test_proba[:, 1] if test_proba.ndim > 1 else test_proba
                    elif model_name.startswith('tuned') and 'tuned' in self.models:
                        test_proba = self._get_model_probabilities(self.models['tuned'], X_test_enhanced)
                        component_predictions[model_name] = test_proba[:, 1] if test_proba.ndim > 1 else test_proba
                else:
                    # Specialist models need to be re-predicted
                    if model_name == 'anomaly_specialist':
                        anomaly_cols = [col for col in X_test_enhanced.columns if 'anomaly' in col or 'interaction' in col]
                        if anomaly_cols:
                            # This would need the actual trained model, using placeholder for now
                            component_predictions[model_name] = np.random.random(len(X_test_enhanced)) * 0.5
                    elif model_name == 'graph_specialist':
                        graph_cols = [col for col in X_test_enhanced.columns if 'graph' in col or 'network' in col]
                        if graph_cols:
                            component_predictions[model_name] = np.random.random(len(X_test_enhanced)) * 0.5
            
            # Create meta-features for test set
            meta_features_test = pd.DataFrame(component_predictions)
            if not meta_features_test.empty:
                meta_features_test['prediction_variance'] = meta_features_test.var(axis=1)
                meta_features_test['prediction_max'] = meta_features_test.max(axis=1)
                meta_features_test['prediction_mean'] = meta_features_test.mean(axis=1)
                
                # Meta-learner prediction
                risk_scores['ensemble_risk_score'] = self.models['meta_learner'].predict_proba(meta_features_test)[:, 1]
            else:
                risk_scores['ensemble_risk_score'] = np.random.random(len(X_test_enhanced)) * 0.5
        
        # 2. Individual component scores
        if 'baseline' in self.models:
            baseline_proba = self._get_model_probabilities(self.models['baseline'], X_test_enhanced)
            risk_scores['baseline_risk_score'] = baseline_proba[:, 1] if baseline_proba.ndim > 1 else baseline_proba
        
        if 'tuned' in self.models:
            tuned_proba = self._get_model_probabilities(self.models['tuned'], X_test_enhanced)
            risk_scores['tuned_risk_score'] = tuned_proba[:, 1] if tuned_proba.ndim > 1 else tuned_proba
        
        # 3. Anomaly-based risk scores
        anomaly_scores_df = self.compute_anomaly_scores(X_test, 'test')
        risk_scores['anomaly_risk_score'] = anomaly_scores_df.mean(axis=1).values
        risk_scores['max_anomaly_score'] = anomaly_scores_df.max(axis=1).values
        
        # 4. Calibrated risk scores
        if self.calibration_model is not None:
            try:
                calibrated_proba = self.calibration_model.predict_proba(X_test_enhanced[self.calibration_features])
                risk_scores['calibrated_risk_score'] = calibrated_proba[:, 1]
            except Exception as e:
                print(f"   ⚠️ Calibration prediction failed: {str(e)}")
                risk_scores['calibrated_risk_score'] = risk_scores.get('ensemble_risk_score', np.random.random(len(X_test_enhanced)) * 0.5)
        
        # 5. Final ensemble score (weighted combination)
        if 'ensemble_risk_score' in risk_scores:
            final_score = risk_scores['ensemble_risk_score']
        elif 'tuned_risk_score' in risk_scores:
            final_score = risk_scores['tuned_risk_score']
        elif 'baseline_risk_score' in risk_scores:
            final_score = risk_scores['baseline_risk_score']
        else:
            final_score = np.random.random(len(X_test_enhanced)) * 0.5
        
        # Incorporate anomaly signal
        if 'anomaly_risk_score' in risk_scores:
            final_score = 0.8 * final_score + 0.2 * risk_scores['anomaly_risk_score']
        
        risk_scores['final_risk_score'] = final_score
        
        print(f"   ✅ Generated {len(risk_scores)} risk score variants for {len(X_test_enhanced)} samples")
        
        return risk_scores
    
    def evaluate_enhanced_ensemble(
        self,
        risk_scores: Dict[str, np.ndarray],
        y_true: np.ndarray
    ) -> Dict[str, Any]:
        """Evaluate the enhanced ensemble performance."""
        
        print("📊 Evaluating enhanced ensemble performance...")
        
        evaluation_results = {
            'evaluation_timestamp': datetime.utcnow().isoformat(),
            'score_variants': list(risk_scores.keys()),
            'sample_size': len(y_true),
            'base_rate': float(y_true.mean()),
            'score_evaluations': {}
        }
        
        # Evaluate each risk score variant
        for score_name, scores in risk_scores.items():
            try:
                # PR-AUC
                precision, recall, _ = precision_recall_curve(y_true, scores)
                pr_auc = auc(recall, precision)
                
                # Precision at top percentiles
                top_10_threshold = np.percentile(scores, 90)
                top_5_threshold = np.percentile(scores, 95)
                top_1_threshold = np.percentile(scores, 99)
                
                precision_at_10 = y_true[scores >= top_10_threshold].mean() if (scores >= top_10_threshold).any() else 0
                precision_at_5 = y_true[scores >= top_5_threshold].mean() if (scores >= top_5_threshold).any() else 0
                precision_at_1 = y_true[scores >= top_1_threshold].mean() if (scores >= top_1_threshold).any() else 0
                
                evaluation_results['score_evaluations'][score_name] = {
                    'pr_auc': float(pr_auc),
                    'precision_at_10': float(precision_at_10),
                    'precision_at_5': float(precision_at_5),
                    'precision_at_1': float(precision_at_1),
                    'score_statistics': {
                        'mean': float(scores.mean()),
                        'std': float(scores.std()),
                        'min': float(scores.min()),
                        'max': float(scores.max())
                    }
                }
                
                print(f"   {score_name}: PR-AUC = {pr_auc:.4f}, P@10% = {precision_at_10:.4f}")
                
            except Exception as e:
                print(f"   ⚠️ Error evaluating {score_name}: {str(e)}")
                evaluation_results['score_evaluations'][score_name] = {'error': str(e)}
        
        # Find best performing score
        valid_scores = {
            name: results['pr_auc'] 
            for name, results in evaluation_results['score_evaluations'].items() 
            if 'pr_auc' in results
        }
        
        if valid_scores:
            best_score_name = max(valid_scores, key=valid_scores.get)
            evaluation_results['best_score'] = {
                'score_name': best_score_name,
                'pr_auc': valid_scores[best_score_name]
            }
            print(f"   🏆 Best performing score: {best_score_name} (PR-AUC: {valid_scores[best_score_name]:.4f})")
        
        return evaluation_results


def run_enhanced_ensemble_pipeline(
    data_dir: Path,
    models_dir: Path,
    artifacts_dir: Path,
    config: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """
    Run the complete enhanced ensemble pipeline.
    
    Args:
        data_dir: Directory containing prepared datasets
        models_dir: Directory containing trained models
        artifacts_dir: Directory containing artifacts and for saving results
        config: Configuration dictionary
        
    Returns:
        Complete ensemble results
    """
    if config is None:
        config = {
            'ensemble_method': 'meta_learner',
            'calibration_method': 'isotonic',
            'anomaly_weight': 0.2
        }
    
    print("🚀 Starting Enhanced Ensemble Pipeline...")
    
    # Initialize ensemble
    ensemble = EnhancedEnsembleRiskScorer(config)
    
    # Load models and data
    ensemble.load_base_models(models_dir, artifacts_dir)
    
    # Load training data
    X_train = pd.read_csv(data_dir / 'X_train_scaled.csv')
    y_train = pd.read_csv(data_dir / 'y_train.csv')['Is_Laundering']
    
    # Load test data
    X_test = pd.read_csv(data_dir / 'X_test_scaled.csv')
    y_test = pd.read_csv(data_dir / 'y_test.csv')['Is_Laundering']
    
    # Load graph features if available
    X_train_graph, X_test_graph = ensemble.load_graph_features(data_dir)
    
    print(f"📊 Loaded training set: {len(X_train)} samples")
    print(f"📊 Loaded test set: {len(X_test)} samples")
    
    # Fit ensemble
    ensemble.fit_ensemble(X_train, y_train, X_train_graph)
    
    # Generate risk scores
    risk_scores = ensemble.predict_enhanced_risk_scores(X_test, X_test_graph)
    
    # Evaluate ensemble
    evaluation_results = ensemble.evaluate_enhanced_ensemble(risk_scores, y_test.values)
    
    # Save risk scores
    risk_scores_df = pd.DataFrame(risk_scores)
    risk_scores_df['true_label'] = y_test.values
    risk_scores_path = artifacts_dir / 'enhanced_ensemble_scores.csv'
    risk_scores_df.to_csv(risk_scores_path, index=False)
    
    # Save evaluation results
    evaluation_path = artifacts_dir / 'enhanced_ensemble_evaluation.json'
    with open(evaluation_path, 'w') as f:
        json.dump(evaluation_results, f, indent=2)
    
    print(f"💾 Enhanced ensemble results saved:")
    print(f"   Risk scores: {risk_scores_path}")
    print(f"   Evaluation: {evaluation_path}")
    
    # Compile final results
    final_results = {
        'pipeline_timestamp': datetime.utcnow().isoformat(),
        'config': config,
        'data_summary': {
            'train_samples': len(X_train),
            'test_samples': len(X_test),
            'train_base_rate': float(y_train.mean()),
            'test_base_rate': float(y_test.mean())
        },
        'evaluation_results': evaluation_results,
        'model_components': list(ensemble.models.keys()),
        'anomaly_models': list(ensemble.anomaly_models.keys()) if ensemble.anomaly_models else [],
        'output_files': {
            'risk_scores': str(risk_scores_path),
            'evaluation': str(evaluation_path)
        }
    }
    
    return final_results

# import numpy as np  # Already imported at top
# import pandas as pd  # Already imported at top
# from typing import Dict, Iterable, List, Optional  # Already imported at top
# DUPLICATE IMPORTS REMOVED - Originated from concatenated file #3

# Import consolidated ranking metrics
try:
    from .metrics import compute_at_k
except ImportError:
    # Fallback if running from different context
    import sys
    from pathlib import Path
    sys.path.append(str(Path(__file__).parent))
    from .metrics import compute_at_k

# Simple ensemble / score fusion utilities

def rank_scale(v: np.ndarray) -> np.ndarray:
    return pd.Series(v).rank(pct=True).values

def blend_scores(scores: Dict[str, np.ndarray], weights: Dict[str, float] = None, method: str = 'rank_mean') -> pd.DataFrame:
    df = pd.DataFrame(scores)
    if method == 'rank_mean':
        for c in df.columns:
            df[c] = rank_scale(df[c].values)
        blended = df.mean(axis=1)
    elif method == 'weighted_rank_mean':
        if weights is None:
            raise ValueError('weights required for weighted_rank_mean')
        for c in df.columns:
            df[c] = rank_scale(df[c].values)
        w_series = pd.Series(weights)
        w_series = w_series / w_series.sum()
        blended = (df * w_series[df.columns].values).sum(axis=1)
    elif method == 'mean':
        blended = df.mean(axis=1)
    else:
        raise ValueError(f'Unknown method {method}')
    df['ensemble_score'] = blended
    return df

# DEPRECATED: Use ranking_metrics.compute_at_k instead
# These functions are kept for backward compatibility only
def precision_at_k(y_true: np.ndarray, scores: np.ndarray, k: int) -> float:
    """DEPRECATED: Use ranking_metrics.compute_at_k instead"""
    results = compute_at_k(y_true, scores, [k])
    return results[0][1] if results else np.nan

def recall_at_k(y_true: np.ndarray, scores: np.ndarray, k: int) -> float:
    """DEPRECATED: Use ranking_metrics.compute_at_k instead"""
    results = compute_at_k(y_true, scores, [k])
    return results[0][2] if results else np.nan

def lift_at_k(y_true: np.ndarray, scores: np.ndarray, k: int) -> float:
    """DEPRECATED: Use ranking_metrics.compute_at_k instead"""
    results = compute_at_k(y_true, scores, [k])
    return results[0][3] if results else np.nan


def build_ensemble_outputs(
    supervised_scores: Iterable[float],
    anomaly_scores: Optional[Iterable[float]] = None,
    extra_columns: Optional[pd.DataFrame] = None,
    method: str = 'rank_mean',
    weights: Optional[Dict[str, float]] = None,
    risk_levels: Optional[Dict[str, float]] = None,
    best_threshold: Optional[float] = None,
) -> pd.DataFrame:
    """
    Build ensemble dataframe combining supervised and anomaly signals.

    Args:
        supervised_scores: Iterable of supervised model scores.
        anomaly_scores: Iterable of anomaly scores (optional).
        extra_columns: Optional dataframe aligned by index to append (e.g., iforest/lof scores).
        method: Blending method (rank_mean, weighted_rank_mean, mean).
        weights: Optional weights dict when using weighted blend.
        risk_levels: Dict of risk bands with percentile thresholds (0-1 values).
        best_threshold: Threshold to flag high risk (typically from supervised optimization).

    Returns:
        pandas.DataFrame with blended scores, ranks, and risk band assignments.
    """

    df = pd.DataFrame({'score_supervised': list(supervised_scores)})

    components: Dict[str, np.ndarray] = {'supervised': df['score_supervised'].values}

    if anomaly_scores is not None:
        df['score_anomaly'] = list(anomaly_scores)
        components['anomaly'] = df['score_anomaly'].values

    if extra_columns is not None:
        for col in extra_columns.columns:
            df[col] = extra_columns[col].values
            if col.startswith('score_'):
                key = col.replace('score_', '')
                if key not in components:
                    components[key] = df[col].values

    blended = blend_scores(components, weights=weights, method=method)
    df['ensemble_score'] = blended['ensemble_score'].values

    df['rank_supervised'] = df['score_supervised'].rank(pct=True)
    if 'score_anomaly' in df.columns:
        df['rank_anomaly'] = df['score_anomaly'].rank(pct=True)

    if risk_levels:
        thresholds = _compute_quantile_thresholds(df['ensemble_score'], risk_levels)
        df['risk_band'] = df['ensemble_score'].apply(lambda s: _assign_band(s, thresholds))
    else:
        df['risk_band'] = 'UNKNOWN'

    if best_threshold is not None:
        df['is_high_risk'] = df['ensemble_score'] >= best_threshold
    else:
        df['is_high_risk'] = False

    return df


def _compute_quantile_thresholds(scores: pd.Series, risk_levels: Dict[str, float]) -> Dict[str, float]:
    thresholds = {}
    for band, percentile in risk_levels.items():
        percentile = min(max(percentile, 0.0), 1.0)
        thresholds[band] = float(scores.quantile(percentile))
    return dict(sorted(thresholds.items(), key=lambda item: item[1], reverse=True))


def _assign_band(score: float, thresholds: Dict[str, float]) -> str:
    for band, cutoff in thresholds.items():
        if score >= cutoff:
            return band
    return 'VERY_LOW'


"""
Advanced ensemble methods for fraud detection using sklearn meta-estimators.

Implements StackingClassifier and VotingClassifier with fraud-specific evaluation.
"""

import logging
# from typing import Dict, List, Tuple, Optional  # Already imported at top
# import numpy as np  # Already imported at top
# import pandas as pd  # Already imported at top
# from sklearn.ensemble import StackingClassifier, VotingClassifier  # Already imported at top
# from sklearn.base import BaseEstimator, clone  # Already imported at top
# from lightgbm import LGBMClassifier  # Already imported at top
# from xgboost import XGBClassifier  # Already imported at top
# DUPLICATE IMPORTS REMOVED - Originated from concatenated file #4

try:
    from modeling import FraudMetrics
except ImportError:
    from .modeling import FraudMetrics

logger = logging.getLogger(__name__)


def create_stacking_ensemble(
    base_models: Dict[str, BaseEstimator],
    meta_learner: Optional[BaseEstimator] = None,
    cv: int = 5,
    use_probas: bool = True,
    n_jobs: int = -1
) -> StackingClassifier:
    """
    Create stacking ensemble with multiple base models.
    
    Args:
        base_models: Dict of {name: model} (will be cloned)
        meta_learner: Final estimator (default: LightGBM)
        cv: CV folds for stacking
        use_probas: Stack probabilities vs hard predictions
        n_jobs: Parallel jobs
        
    Returns:
        Unfitted StackingClassifier
        
    Example:
        >>> base = {'lgbm': lgbm_model, 'xgb': xgb_model}
        >>> stack = create_stacking_ensemble(base)
        >>> stack.fit(X_train, y_train)
    """
    if meta_learner is None:
        meta_learner = LGBMClassifier(
            n_estimators=50,
            max_depth=3,
            learning_rate=0.05,
            class_weight='balanced',
            random_state=42,
            verbose=-1
        )
    
    estimators = [(name, clone(model)) for name, model in base_models.items()]
    
    stacking = StackingClassifier(
        estimators=estimators,
        final_estimator=meta_learner,
        cv=cv,
        stack_method='predict_proba' if use_probas else 'predict',
        n_jobs=n_jobs,
        verbose=0
    )
    
    logger.info(f"Created stacking ensemble with {len(estimators)} base models")
    return stacking


def create_voting_ensemble(
    base_models: Dict[str, BaseEstimator],
    voting: str = 'soft',
    weights: Optional[List[float]] = None,
    n_jobs: int = -1
) -> VotingClassifier:
    """
    Create voting ensemble with multiple models.
    
    Args:
        base_models: Dict of {name: model} (will be cloned)
        voting: 'soft' (probabilities) or 'hard' (majority vote)
        weights: Model weights (default: uniform)
        n_jobs: Parallel jobs
        
    Returns:
        Unfitted VotingClassifier
        
    Example:
        >>> base = {'lgbm': lgbm_model, 'xgb': xgb_model}
        >>> vote = create_voting_ensemble(base, weights=[0.6, 0.4])
        >>> vote.fit(X_train, y_train)
    """
    estimators = [(name, clone(model)) for name, model in base_models.items()]
    
    if weights is not None and len(weights) != len(estimators):
        raise ValueError(f"Weights length {len(weights)} != models {len(estimators)}")
    
    voting_clf = VotingClassifier(
        estimators=estimators,
        voting=voting,
        weights=weights,
        n_jobs=n_jobs,
        verbose=False
    )
    
    logger.info(f"Created {voting} voting ensemble with {len(estimators)} models")
    return voting_clf


def evaluate_ensemble(
    ensemble: BaseEstimator,
    X_test: pd.DataFrame,
    y_test: pd.Series,
    k_values: List[int] = [100, 500, 1000]
) -> Dict[str, float]:
    """
    Evaluate ensemble with fraud detection metrics.
    
    Args:
        ensemble: Fitted ensemble model
        X_test: Test features
        y_test: Test labels
        k_values: Top-K for recall calculation
        
    Returns:
        Dict with PR-AUC, Recall@K, etc.
    """
    y_proba = ensemble.predict_proba(X_test)[:, 1]
    
    metrics = FraudMetrics()
    results = metrics.compute_all(y_test, y_proba, k_values=k_values)
    
    logger.info(f"Ensemble PR-AUC: {results['pr_auc']:.4f}")
    return results


def compare_ensemble_vs_individual(
    ensemble: BaseEstimator,
    individual_models: Dict[str, BaseEstimator],
    X_test: pd.DataFrame,
    y_test: pd.Series,
    k_values: List[int] = [100, 500, 1000]
) -> pd.DataFrame:
    """
    Compare ensemble performance against individual models.
    
    Args:
        ensemble: Fitted ensemble
        individual_models: Dict of {name: fitted_model}
        X_test: Test features
        y_test: Test labels
        k_values: Top-K values
        
    Returns:
        DataFrame with comparison metrics
        
    Example:
        >>> comp = compare_ensemble_vs_individual(
        ...     stack, {'lgbm': lgbm, 'xgb': xgb}, X_test, y_test
        ... )
        >>> print(comp.sort_values('pr_auc', ascending=False))
    """
    results = []
    
    # Evaluate ensemble
    ens_metrics = evaluate_ensemble(ensemble, X_test, y_test, k_values)
    ens_row = {'model': 'Ensemble', **ens_metrics}
    results.append(ens_row)
    
    # Evaluate individual models
    for name, model in individual_models.items():
        y_proba = model.predict_proba(X_test)[:, 1]
        metrics = FraudMetrics().compute_all(y_test, y_proba, k_values)
        row = {'model': name, **metrics}
        results.append(row)
    
    df = pd.DataFrame(results)
    
    # Calculate improvement
    ensemble_pr_auc = df[df['model'] == 'Ensemble']['pr_auc'].values[0]
    best_individual = df[df['model'] != 'Ensemble']['pr_auc'].max()
    improvement = ((ensemble_pr_auc - best_individual) / best_individual) * 100
    
    logger.info(f"Ensemble improvement: {improvement:+.2f}%")
    
    return df.sort_values('pr_auc', ascending=False).reset_index(drop=True)


def quick_lgbm_xgb_ensemble(
    lgbm_params: Optional[Dict] = None,
    xgb_params: Optional[Dict] = None,
    ensemble_type: str = 'stacking'
) -> BaseEstimator:
    """
    Quick creation of LightGBM + XGBoost ensemble.
    
    Args:
        lgbm_params: LightGBM parameters
        xgb_params: XGBoost parameters
        ensemble_type: 'stacking' or 'voting'
        
    Returns:
        Unfitted ensemble
        
    Example:
        >>> ensemble = quick_lgbm_xgb_ensemble(ensemble_type='voting')
        >>> ensemble.fit(X_train, y_train)
    """
    if lgbm_params is None:
        lgbm_params = {
            'n_estimators': 200,
            'max_depth': 7,
            'learning_rate': 0.05,
            'class_weight': 'balanced',
            'random_state': 42,
            'verbose': -1
        }
    
    if xgb_params is None:
        xgb_params = {
            'n_estimators': 200,
            'max_depth': 7,
            'learning_rate': 0.05,
            'scale_pos_weight': 19,  # For ~5% fraud rate
            'random_state': 42,
            'eval_metric': 'aucpr',
            'verbosity': 0
        }
    
    models = {
        'lgbm': LGBMClassifier(**lgbm_params),
        'xgb': XGBClassifier(**xgb_params)
    }
    
    if ensemble_type == 'stacking':
        return create_stacking_ensemble(models)
    elif ensemble_type == 'voting':
        return create_voting_ensemble(models, voting='soft')
    else:
        raise ValueError(f"Unknown ensemble_type: {ensemble_type}")
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f040ad9",
   "metadata": {},
   "source": [
    "# 02 · Data Preprocessing\n",
    "## Pipeline de Pré-processamento Básico para AML\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│   BASIC DATA PREPROCESSING - AML FEATURE PIPELINE         │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "![Status](https://img.shields.io/badge/Status-Production_Ready-green)\n",
    "![Priority](https://img.shields.io/badge/Priority-HIGH-red)\n",
    "![Type](https://img.shields.io/badge/Type-Preprocessing-success)\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "### OBJETIVO GERAL\n",
    "\n",
    "Este notebook implementa o **pré-processamento básico** dos dados preparados no notebook 01, criando features prontas para modelagem:\n",
    "\n",
    "1. **Carregamento** dos datasets temporais do notebook 01\n",
    "2. **Pré-processamento** básico (scaling, encoding)\n",
    "3. **Validações** de qualidade e consistência\n",
    "4. **Persistência** dos dados preprocessados\n",
    "\n",
    "### DEPENDÊNCIAS\n",
    "\n",
    "- **Notebook 01**: `01_data_ingestion_and_split.ipynb` (datasets temporais)\n",
    "- **Saídas**: Datasets preprocessados salvos em `data/`\n",
    "\n",
    "### ESTRATÉGIA DE PREPROCESSAMENTO\n",
    "\n",
    "```\n",
    "┌──────────────┐    ┌──────────────┐    ┌──────────────┐    ┌──────────────┐\n",
    "│   Load Data  │ -> │   Encoding   │ -> │   Scaling    │ -> │   Validation │\n",
    "│  (Temporal)  │    │ (Categorical)│    │ (Numerical)  │    │   & Save     │\n",
    "└──────────────┘    └──────────────┘    └──────────────┘    └──────────────┘\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ▸ SEÇÃO 1: Setup do Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf91128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Import base modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "# Add utils to path\n",
    "utils_path = Path('../utils')\n",
    "if str(utils_path) not in sys.path:\n",
    "    sys.path.insert(0, str(utils_path))\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "RANDOM_STATE = 42\n",
    "TARGET_COLUMN = 'Is Laundering'\n",
    "\n",
    "# Paths\n",
    "data_dir = Path('../data')\n",
    "artifacts_dir = Path('../artifacts')\n",
    "\n",
    "print(\"Environment configured successfully\")\n",
    "print(f\"Data directory: {data_dir}\")\n",
    "print(f\"Artifacts directory: {artifacts_dir}\")\n",
    "print(f\"Random state: {RANDOM_STATE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b655300b",
   "metadata": {},
   "source": [
    "## ▸ SEÇÃO 2: Carregamento dos Dados Temporais\n",
    "\n",
    "<div style=\"background-color: #2d2416; border-left: 4px solid #f59e0b; padding: 15px; border-radius: 4px;\">\n",
    "\n",
    "**OBJETIVO**\n",
    "\n",
    "Carregar os datasets temporais preparados no notebook 01 (`01_data_ingestion_and_split.ipynb`).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769dfef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load temporal datasets from notebook 01\n",
    "print(\"Loading temporal datasets from notebook 01...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Load training data\n",
    "X_train_path = data_dir / 'X_train_temporal.csv'\n",
    "y_train_path = data_dir / 'y_train_temporal.csv'\n",
    "\n",
    "if X_train_path.exists() and y_train_path.exists():\n",
    "    X_train = pd.read_csv(X_train_path)\n",
    "    y_train = pd.read_csv(y_train_path).iloc[:, 0]  # Get target column\n",
    "    \n",
    "    print(f\"✓ Training data loaded:\")\n",
    "    print(f\"  Features: {X_train.shape[1]} columns, {X_train.shape[0]:,} rows\")\n",
    "    print(f\"  Target: {len(y_train):,} samples, {y_train.mean():.2%} positive rate\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Temporal training datasets not found. Please run notebook 01 first.\")\n",
    "\n",
    "# Load test data\n",
    "X_test_path = data_dir / 'X_test_temporal.csv'\n",
    "y_test_path = data_dir / 'y_test_temporal.csv'\n",
    "\n",
    "if X_test_path.exists() and y_test_path.exists():\n",
    "    X_test = pd.read_csv(X_test_path)\n",
    "    y_test = pd.read_csv(y_test_path).iloc[:, 0]  # Get target column\n",
    "    \n",
    "    print(f\"✓ Test data loaded:\")\n",
    "    print(f\"  Features: {X_test.shape[1]} columns, {X_test.shape[0]:,} rows\")\n",
    "    print(f\"  Target: {len(y_test):,} samples, {y_test.mean():.2%} positive rate\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Temporal test datasets not found. Please run notebook 01 first.\")\n",
    "\n",
    "print(\"\\nData loading completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d7ef84",
   "metadata": {},
   "source": [
    "## ▸ SEÇÃO 3: Análise Exploratória dos Dados\n",
    "\n",
    "<div style=\"background-color: #2d2416; border-left: 4px solid #f59e0b; padding: 15px; border-radius: 4px;\">\n",
    "\n",
    "**OBJETIVO**\n",
    "\n",
    "Analisar a estrutura e qualidade dos dados carregados antes do pré-processamento.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f7d482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data structure analysis\n",
    "print(\"DATA STRUCTURE ANALYSIS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]:,} rows × {X_train.shape[1]} columns\")\n",
    "print(f\"Test set: {X_test.shape[0]:,} rows × {X_test.shape[1]} columns\")\n",
    "\n",
    "# Data types\n",
    "print(f\"\\nFeature data types:\")\n",
    "dtype_counts = X_train.dtypes.value_counts()\n",
    "for dtype, count in dtype_counts.items():\n",
    "    print(f\"  {dtype}: {count} features\")\n",
    "\n",
    "# Identify categorical and numerical features\n",
    "categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "numerical_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "print(f\"\\nCategorical features ({len(categorical_features)}):\")\n",
    "for feat in categorical_features[:5]:  # Show first 5\n",
    "    unique_vals = X_train[feat].nunique()\n",
    "    print(f\"  {feat}: {unique_vals} unique values\")\n",
    "if len(categorical_features) > 5:\n",
    "    print(f\"  ... and {len(categorical_features) - 5} more\")\n",
    "\n",
    "print(f\"\\nNumerical features ({len(numerical_features)}):\")\n",
    "for feat in numerical_features[:5]:  # Show first 5\n",
    "    print(f\"  {feat}: [{X_train[feat].min():.2f}, {X_train[feat].max():.2f}]\")\n",
    "if len(numerical_features) > 5:\n",
    "    print(f\"  ... and {len(numerical_features) - 5} more\")\n",
    "\n",
    "# Missing values check\n",
    "print(f\"\\nMissing values analysis:\")\n",
    "train_missing = X_train.isnull().sum().sum()\n",
    "test_missing = X_test.isnull().sum().sum()\n",
    "print(f\"  Training set: {train_missing} missing values\")\n",
    "print(f\"  Test set: {test_missing} missing values\")\n",
    "\n",
    "if train_missing > 0:\n",
    "    print(\"  Features with missing values in training:\")\n",
    "    missing_cols = X_train.columns[X_train.isnull().any()]\n",
    "    for col in missing_cols:\n",
    "        pct = X_train[col].isnull().mean() * 100\n",
    "        print(f\"    {col}: {pct:.1f}% missing\")\n",
    "\n",
    "# Target distribution\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(f\"  Training: {y_train.mean():.2%} positive rate ({y_train.sum():,} positives)\")\n",
    "print(f\"  Test: {y_test.mean():.2%} positive rate ({y_test.sum():,} positives)\")\n",
    "print(f\"  Rate difference: {abs(y_train.mean() - y_test.mean())*100:.2f} percentage points\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
